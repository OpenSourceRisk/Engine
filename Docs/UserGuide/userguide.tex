\documentclass[12pt, a4paper]{article}

\usepackage[urlcolor=blue]{hyperref}

\usepackage[disable]{todonotes}
%\usepackage{todonotes}

\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}
\usepackage[miktex]{gnuplottex}
\ShellEscapetrue
\usepackage{epstopdf}
\usepackage{longtable}
\usepackage{floatrow}
\usepackage{minted}
\usepackage{textcomp}
\usepackage{color,soul}
\usepackage[font={small,it}]{caption}
\floatsetup[listing]{style=Plaintop}    

% Turn off indentation but allow \indent command to still work.
\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

\addtolength{\textwidth}{0.8in}
\addtolength{\oddsidemargin}{-.4in}
\addtolength{\evensidemargin}{-.4in}
\addtolength{\textheight}{1.6in}
\addtolength{\topmargin}{-.8in}

\usepackage{longtable,supertabular}
\usepackage{listings}
\lstset{
  frame=top,frame=bottom,
  basicstyle=\ttfamily,
  language=XML,
  tabsize=2,
  belowskip=2\medskipamount
}

%\usepackage{float}
\usepackage{tabu}
\tabulinesep=1.0mm
\restylefloat{table}

\usepackage{siunitx}

%\usepackage[colorlinks=true]{hyperref}

\renewcommand\P{\ensuremath{\mathbb{P}}}
\newcommand\E{\ensuremath{\mathbb{E}}}
\newcommand\Q{\ensuremath{\mathbb{Q}}}
\newcommand\I{\mathds{1}}
\newcommand\F{\ensuremath{\mathcal F}}
\newcommand\V{\ensuremath{\mathbb{V}}}
\newcommand\YOY{{\rm YOY}}
\newcommand\Prob{\ensuremath{\mathbb{P}}}
\newcommand{\D}[1]{\mbox{d}#1}
\newcommand{\NPV}{\mathit{NPV}}
\newcommand{\CVA}{\mathit{CVA}}
\newcommand{\DVA}{\mathit{DVA}}
\newcommand{\FVA}{\mathit{FVA}}
\newcommand{\COLVA}{\mathit{COLVA}}
\newcommand{\FCA}{\mathit{FCA}}
\newcommand{\FBA}{\mathit{FBA}}
\newcommand{\KVA}{\mathit{KVA}}
\newcommand{\MVA}{\mathit{MVA}}
\newcommand{\PFE}{\mathit{PFE}}
\newcommand{\EE}{\mathit{EE}}
\newcommand{\EPE}{\mathit{EPE}}
\newcommand{\ENE}{\mathit{ENE}}
\newcommand{\PD}{\mathit{PD}}
\newcommand{\LGD}{\mathit{LGD}}
\newcommand{\DIM}{\mathit{DIM}}
\newcommand{\bs}{\textbackslash}
 
\begin{document}

%\title{Open Source Risk Engine \\ User Guide  }
\title{ORE User Guide}
\author{Quaternion Risk Management}
\date{7 October 2016}
\maketitle

\newpage

%-------------------------------------------------------------------------------
\section*{Document History}

\begin{center}
\begin{supertabular}{|l|l|l|p{7cm}|}
\hline
Date & Author & Comment \\
\hline
7 October 2016 & Quaternion Risk Management & initial release\\
6 April 2017 & Quaternion Risk Management & updates for release v2\\
\hline
\end{supertabular}
\end{center}

\newpage

\tableofcontents
\newpage

\section{Introduction}

The {\em Open Source Risk Project} \cite{ORE} aims at providing a transparent platform for pricing and risk analysis
that serves as
%\medskip
\begin{itemize}
\item a benchmarking, validation, training, and teaching reference,
\item an extensible foundation for tailored risk solutions.
\end{itemize}

Its main software project is {\em Open Source Risk Engine} (ORE), an application that provides
\begin{itemize}
\item a Monte Carlo simulation framework for contemporary risk analytics and value adjustments
\item simple interfaces for trade data, market data and system configuration
\item simple launchers and result visualisation in Jupyter, Excel, LibreOffice
\item unit tests and various examples.  
\end{itemize}
ORE is open source software, provided under the Modified BSD License. It is based 
on QuantLib, the open source library for quantitative finance \cite{QL}.

%\medskip
\subsubsection*{Audience}
The project aims at reaching quantitative risk ma\-nage\-ment practitioners (be it in financial institutions, audit
firms, consulting companies or regulatory bodies) who are looking for accessible software solutions, and quant
developers in charge of the implementation of pricing and risk methods similar to those in ORE. Moreover, the project
aims at reaching academics and students who would like to teach or learn quantitative risk management using a freely
available, contemporary risk application.

\subsubsection*{Contributions}
Quaternion Risk Management \cite{QRM} is committed to sponsoring the Open Source Risk project through ongoing project
administration, through providing this initial release and a series of subsequent releases in order to achieve a wide
analytics, product and risk factor class coverage. The community is invited to contribute to ORE, for example through
feedback, discussions and suggested enhancement in the forum on the ORE site \cite{ORE}, as well as contributions of ORE
enhancements in the form of source code. See the FAQ section on the ORE site \cite{ORE} on how to get involved.

\subsubsection*{Scope and Roadmap}

ORE currently provides portfolio pricing, cash flow generation, and a range of contemporary derivative portfolio
analytics. The latter are based on a Monte Carlo simulation framework which yields the evolution of various {\bf credit
  exposure} and {\bf market risk measures}:
\begin{itemize}
\item EE aka EPE (Expected Exposure or Expected Positive Exposure)
\item ENE (Expected Negative Exposure, i.e. the counterparty's perspective)
\item 'Basel' exposure measures relevant for regulatory capital charges under internal model methods 
\item PFE (Potential Future Exposure at some user defined quantile)
\item Value at Risk and Expected Shortfall
\end{itemize}
and {\bf derivative value adjustments}
\begin{itemize}
\item CVA (Credit Value Adjustment)
\item DVA (Debit Value Adjustment)
\item FVA (Funding Value Adjustment)
\item COLVA (Collateral Value Adjustment)
\item MVA (Margin Value Adjustment)
\end{itemize}
for portfolios with netting, variation and initial margin agreements.  Subsequent ORE releases will also compute {\bf
  regulatory capital charges} for counterparty credit risk under the new standardised approach (SA-CCR), and the Monte
Carlo based market risk measures will be complemented by parametric methods, e.g. for benchmarking various initial
margin calculation models applied in cleared and non-cleared derivatives business.

\medskip The first release of ORE in October 2016 covers the simulation of interest rate and FX risk factors and
portfolios of Interest Rate Swaps, Caps/Floors, Swaptions, FX Forwards, Cross Currency Swaps and FX Options. Subsequent
releases from Q1 2017 onwards will extend the derivative product and the risk factor range to Inflation, Credit, Equity
and Commodity. With the introduction of credit risk factors, the scope will also be extended to cover cash products
(loans and bonds) and related portfolio analytics.

\medskip The simulation models applied in ORE's risk factor evolution implement the models discussed in detail in {\em
  Modern Derivatives Pricing and Credit Exposure Analysis} \cite{Lichters}: The IR/FX risk factor evolution is based on
a cross currency model consisting of an arbitrage free combination of Linear Gauss Markov models for all interest rates
and lognormal processes for the FX rates. The model components are calibrated to cross currency discounting and forward
curves, Swaptions and FX Options.

\subsubsection*{Further Resources}
\begin{itemize}
\item Open Source Risk Project site: \url{http://www.opensourcerisk.org}
\item Frequently Asked Questions: \url{http://www.opensourcerisk.org/faqs}
\item Forum: \url{http://www.opensourcerisk.org/forum}
\item Source code and releases: \url{https://github.com/opensourcerisk/engine}
\item Follow ORE on Twitter {\tt @OpenSourceRisk} for updates on releases and events
\end{itemize}
An ORE Book will follow in 2017 that will elaborate on the engine's design and make the connection between methodology
and implementation. It will be announced in due course on the channels mentioned above.
 
\subsubsection*{Organisation of this document}

This document focuses on instructions how to use ORE to cover basic workflows from individual deal analysis to portfolio
processing. After an overview over the core ORE data flow in section \ref{sec:process} and installation instructions in
section \ref{sec:installation} we start in section \ref{sec:examples} with a series of examples that illustrate how to
launch ORE using its command line application, and we discuss typical results and reports. We then illustrate in section
\ref{sec:visualisation} interactive analysis of resulting 'NPV cube' data. The final sections of this text document ORE
parametrisation and the structure of trade and market data input.

%========================================================
\section{ORE Data Flow}\label{sec:process}
%========================================================

The core processing steps followed in ORE to produce risk analytics results are sketched in Figure \ref{fig_process}.
All ORE calculations and output are generated in three fundamental process steps as indicated in the three boxes in the
upper part of the figure. In each of these steps appropriate data (described below) is loaded and results are generated,
either in form of a human readable report, or in an intermediate step as pure data files (e.g. NPV data, exposure data).
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.6]{process.pdf}
\end{center}
\caption{Sketch of the ORE process, inputs and outputs. }
\label{fig_process}
\end{figure}

The overall ORE process needs to be parametrised using a set of configuration XML files which is the subject of section
\ref{sec:configuration}. The portfolio is provided in XML format which is explained in detail in sections
\ref{sec:portfolio_data} and \ref{sec:nettingsetinput}. Note that ORE comes with 'Schema' files for all supported
products so that any portfolio xml file can be validated before running through ORE. Market data is provided in a simple
three-column text file with unique human-readable labelling of market data points, as explained in section
\ref{sec:market_data}.  \\

The first processing step (upper left box) then comprises 
\begin{itemize}
\item loading the portfolio to be analysed, 
\item building any yield curves or other 'term structures' needed for pricing, 
\item calibration of pricing and simulation models.
\end{itemize}

The second processing step (upper middle box) is then 
\begin{itemize}
\item portfolio valuation, cash flow generation,
\item going forward - conventional risk analysis such as sensitivity analysis and stress testing, standard-rule capital
  calculations such as SA-CCR, etc,
\item and in particular, more time-consuming, the market simulation and portfolio valuation through time under Monte
  Carlo scenarios.
\end{itemize}
This process step produces several reports (NPV, cashflows etc) and in particular an {\bf NPV cube}, i.e. NPVs per
trade, scenario and future evaluation date. The cube is written to a file in both condensed binary and human-readable
text format.  \\

The third processing step (upper right box) performs more 'sophisticated' risk ana\-ly\-sis by post-processing the NPV
cube data:
\begin{itemize}
\item aggregating over trades per netting set, 
\item applying collateral rules to compute simulated variation margin as well as simulated (dynamic) initial margin
  posting,
\item computing various XVAs including CVA, DVA, FVA, MVA for all netting sets, with and without taking collateral
  (variation and initial margin) into account, on demand with allocation to the trade level.
\end{itemize}
The output of this process step are XVA reports and the 'net' NPV cube, i.e. after aggregation, netting and collateral. \\

The example section \ref{sec:examples} demonstrates for representative product types how the described processing steps
can be combined in a simple batch process which produces the mentioned reports, output files and exposure evolution
graphs in one 'go'.

Moreover, both NPV cubes can be further analysed interactively using a visualisation tool introduced in section
\ref{sec:jupyter}. And finally, sections \ref{sec:calc} and \ref{sec:excel} demonstrate how ORE processes can be
launched in spreadsheets and key results presented automatically within the same sheet.

%========================================================
\section{Getting and Building ORE}\label{sec:installation}
%========================================================

You can get ORE in two ways, either by downloading a release bundle as described in section \ref{sec:release} or by
checking out the source code from the github repository as described in section \ref{sec:build_ore}.

\subsection{ORE Releases}\label{sec:release}

ORE releases are regularly provided in the form of source code archives, Windows exe\-cutables {\tt ore.exe}, example
cases and documentation. Release archives will be provided at \url{https://github.com/opensourcerisk/engine/releases}.

\medskip
The release consists of a single archive in zip format
\begin{itemize}
\item {\tt ORE-1.8.zip}
\end{itemize}

When unpacked, it creates a directory {\tt ORE-1.8} with the following files respectively subdirectories
\begin{enumerate}
%\item {\tt bin/win32/ore.exe}
%\item {\tt bin/x64/ore.exe}
\item {\tt App/}
\item {\tt Docs/}
\item {\tt Examples/}
\item {\tt OREAnalytics/}
\item {\tt OREData/}
\item {\tt QuantExt/}
\item {\tt ThirdPartyLibs/}
\item {\tt userguide.pdf}
\item {\tt xsd}
\end{enumerate} 

The first three items and {\tt userguide.pdf} are sufficient to run the compiled ORE application
on the list of examples described in the user guide (this works on Windows only). The Windows executables are located in {\tt App/bin/Win32/Release/} respectively {\tt App/bin/x64/Release/}. To continue with the compiled
executables:
\begin{itemize}
\item Ensure that the scripting language Python is installed on your computer, see also section \ref{sec:python}
  below;
\item Move on to the examples in section \ref{sec:examples}.
\end{itemize}

\medskip
To build ORE from sources:
\begin{itemize}
\item Set up Boost as described in section \ref{sec:boost}, unless already installed
\item Set up QuantLib 1.8 \cite{QL,quantlib-install} from its github or sourceforge download page, unless already
  installed; QuantLib needs to be located in this project directory {\tt ORE-1.8}. Alternatively, you can create a
  symbolic link named QuantLib here that points to the actual QuantLib directory
\item Build QuantExt, OREData, OREAnalytics, App (in this order) as described in section \ref{sec:build}
\item Note that ThirdPartyLibs does not need to be built, it contains RapdidXml, header only code for reading and
  writing XML files
\item Move on to section \ref{sec:python} and the examples in section \ref{sec:examples}.
\end{itemize}

Open {\tt Docs/html/index.html} to see the API documentation for QuantExt, OREData and OREAnalytics, generated by
doxygen.

\subsection{Building ORE}\label{sec:build_ore}

ORE's source code is hosted on github.com at \url{https://github.com/opensourcerisk/engine} using {\tt git}, a free and
open source distributed version control system.

\subsubsection{Git}

To access the current code base on GitHub, one needs to get {\tt git} installed first.
   
\begin{enumerate}
\item Install and setup Git on your machine following instructions at \cite{git-download}

\item Fetch ORE from github by running the following: 

{\tt\% git clone https://github.com/opensourcerisk/engine.git ore}      

This will create a folder 'ore' in your current directory that contains the codebase.

\item Initially, the QuantLib subdirectory under {\tt ore} is empty as it is a submodule pointing to the official
  QuantLib repository. To pull down locally, use the following commands:

{\tt
\% cd ore \\
\% git submodule init \\
\% git submodule update
}

\end{enumerate}

\subsubsection{Boost}\label{sec:boost}

QuantLib and ORE depend on the boost C++ libraries. Hence these need to be installed before building QuantLib and
ORE. With Unix (Linux, OS X), we recommend boost version 1\_55 or higher, with Windows we recommend boost version 1\_57
or higher. Older versions may work on some platforms and system configurations, but were not tested.

\subsubsection*{Windows}

\begin{enumerate}
\item Download the pre-compiled binaries for MSVC-14 (MSVC2015) from \cite{boost-binaries}
%, any recent version should work
\begin{itemize}
\item 32-bit: \cite{boost-binaries}{\bs}VERSION{\bs}boost\_VERSION-msvc-14.0-32.exe{\bs}download 
\item 64-bit: \cite{boost-binaries}{\bs}VERSION{\bs}boost\_VERSION-msvc-14.0-64.exe{\bs}download
\end{itemize}
\item Start the installation file and choose an installation folder. Take a note of that folder as it will be needed
  later on.
\item Finish the installation by clicking Next a couple of times.
\end{enumerate}
    
Alternatively, compile all Boost libraries directly from the source code:

\begin{enumerate}
\item Open a Visual Studio Tools Command Prompt
\begin{itemize}
\item 32-bit: VS2015/VS2013 x86 Native Tools Command Prompt
\item 64-bit: VS2015/VS2013 x64 Native Tools Command Prompt
\end{itemize}
\item Navigate to the boost root directory
\item Run bootstrap.bat
\item Build the libraries from the source code
\begin{itemize}
\item 32-bit: \\
  {\footnotesize\tt .{\bs}b2 --stagedir=.{\bs}lib{\bs}Win32{\bs}lib --build-type=complete toolset=msvc-14.0 \bs \\
    address-model=32 --with-test --with-system --with-filesystem  \bs \\
    --with-serialization --with-regex --with-date\_time stage}
\item 64-bit: \\
  {\footnotesize\tt .{\bs}b2 --stagedir=.{\bs}lib{\bs}x64{\bs}lib --build-type=complete toolset=msvc-14.0 \bs \\
    address-model=64 --with-test --with-system --with-filesystem \bs \\
    --with-serialization --with-regex --with-date\_time stage}
\end{itemize}
\end{enumerate}

\subsubsection*{Unix}

\begin{enumerate}
\item Download Boost from \cite{boost} and build following the instructions on the site
%, any recent version should work
\item Define the environment variable BOOST that points to the boost directory
(so includes should be in BOOST and libs should be in BOOST/stage/lib)
\end{enumerate}

\subsubsection{ORE Libraries and Application}\label{sec:build}

\subsubsection*{Windows}

\begin{enumerate}

\item Download and install Visual Studio Community Edition. During the installation, make sure you install the Visual
  C++ support under the Programming Languages features (disabled by default).

\item To configure the boost paths in Visual Studio open any of the Visual Studio solution files in item 3 below and
  select View $\rightarrow$ Other Windows $\rightarrow$ Property Manager. It does not matter which solution you open, if
  it is for example the Quant\-Ext solution you should see two Projects 'QuantExt' and 'quantexttestsuite' in the property
  manager. Expand any of them (e.g. QuantExt) and then one of the Win32 or x64 configurations. The settings will be
  specific for the Win32 or x64 configuration but otherwise it does not matter which of the projects or configurations
  you expand, they all contain the same configuration file. You should now see 'Microsoft.Cpp.Win32.user' respectively
  'Microsoft.Cpp.x64.user' depending on whether you chose a Win32 or a x64 configuration. Click on this file to open the
  property pages. Select VC++ Directories and then add your boost directory to the 'Include Directories' entry. Likewise
  add your boost library directory to the 'Library Directories' entry. If for example your boost installation is in {\tt
    C:{\bs}boost\_1\_57\_0} and the libraries reside in the {\tt stage{\bs}lib} subfolder, add {\tt
    C:{\bs}boost\_1\_57\_0} to the 'Include Directories' entry and {\tt C:{\bs}boost\_1\_57\_0{\bs}stage{\bs} lib} to the
  'Library Directories' entry. Press OK. If you want to configure the boost paths for Win32 resp. x64 as well, repeat
  the previous step for 'Microsoft.Cpp. Win32.user' respectively 'Microsoft.Cpp.x64.user'. To complete the configuration
  just close the property manager window.

  % \item Open any solution file and update path to Boost as described in sections 5 \& 6 in
  %   \cite{quantlib-install}. You only need to do this for a single project as this will update the path to boost
  %   across all projects and solutions on your machine. The paths should be set as follow (in case you compiled Boost
  %   on your own, use the path specified using the --stagedir cmd argument):
%
%\begin{itemize}
%\item Include Directories: [Boost Installation Folder]
%\item Library Directories: [Boost Installation Folder]{\bs}libs
%\end{itemize}
%
%\item Add the additional path to the Boost pre-compiled libraries to the linker setting:
%
%\begin{itemize}
%\item 32-bit: [Boost Installation Folder]{\bs}lib32-msvc-14.0
%\item 64-bit: [Boost Installation Folder]{\bs}lib64-msvc-14.0
%\end{itemize}

\item Open each of the sub-projects and compile them in the following order: QuantLib, QuantExt, OREData, OREAnalytics
  and App. For each project, do the following:

\begin{itemize}
\item Switch to the correct platform (i.e. Win32 or x64) from the Configuration Manager. The selection should match the
  pre-compiled version of Boost. Trying to compile using a mixed configuration (e.g. Boost 64-bit and 32-bit QuantLib)
  will fail.
\item Compile the project: Build $\rightarrow$ Build Solution
\item Once the compilation is complete, run the test suite.
\end{itemize}

\end{enumerate}

\subsubsection*{Unix}

\begin{enumerate}

\item Build QuantLib as usual.

{\tt\footnotesize
\% cd QuantLib \\
\% ./autogen.sh \\
\% ./configure --with-boost-include=\$BOOST --with-boost-lib=\$BOOST/stage/lib \\
\% make -j4 
}

\item Build QuantExt

{\tt\footnotesize
\% cd QuantExt \\
\% ./autogen.sh \\
\% ./configure \\
\% make -j4
}

This will build both the QuantExt library and test suite.

\item Run the test suite

{\tt\footnotesize
\% ./test/quantext-test-suite 
}

\item  Build OREData, OREAnalytics and their test suites. 

Follow the same steps as for QuantExt.
To run the unit test suites, do 

{\tt\footnotesize
\% ./test/ored-test-suite 
}

and 

{\tt\footnotesize
\% ./test/orea-test-suite 
}

in the respective library directories.

\item Build App/ore

{\tt\footnotesize
\% cd App \\
\% ./autogen.sh \\
\% ./configure \\
\% make -j4
}

Note: On Linux systems, the 'locale' settings can negatively affect the ORE process and output. To avoid this, we
recommend setting the environment variable {\tt LC\_NUMERIC} to {\tt C}, e.g. in a bash shell, do

{\tt\footnotesize
\% export LC\_NUMERIC=C
}

before running ORE or any of the examples below. This will suppress thousand separators in numbers when converted to
strings.

\item Run Examples (see section \ref{sec:examples})

{\tt\footnotesize
\% cd Examples/Example\_1 \\
\% python run.py 
}

\end{enumerate}

\subsection{Python and Jupyter}\label{sec:python}

Python (version 3.5 or higher) is required to run the examples in section \ref{sec:examples} and plot exposure
evolutions. Moreover, we use Jupyter \cite{jupyter} in section \ref{sec:visualisation} to visualise simulation
results. Both are part of the 'Anaconda Open Data Science Analytics Platform' \cite{Anaconda}. Anaconda installation
instructions for Windows, OS X and Linux are available on the Anaconda site, with graphical installers for
Windows\footnote{With Windows, after a fresh installation of Python the user may have to run the {\tt python} command
  once in a command shell so that the Python executable will be found subsequently when running the example scripts in
  section \ref{sec:examples}.}, Linux and OS X.

With Linux and OS X, the following environment variable settings are required
\begin{itemize}
\item set {\tt LANG} and {\tt LC\_ALL } to {\tt en\_US.UTF-8} or {\tt en\_GB.UTF-8}
\item set {\tt LC\_NUMERIC} to {\tt C}. 
\end{itemize}
The former is required for both running the Python scripts in the examples section, as well as successful installation
of the following packages. \\

The full functionality of the Jupyter notebook introduced in section \ref{sec:jupyter} requires furthermore installing
\begin{itemize}
\item jupyter\_dashboards: \url{https://github.com/jupyter-incubator/dashboards}
\item ipywidgets: \url{https://github.com/ipython/ipywidgets}
\item pythreejs: \url{https://github.com/jovyan/pythreejs}
\item bqplot: \url{https://github.com/bloomberg/bqplot}
\end{itemize}
With Python and Anaconda already installed, this can be done by running these commands
\begin{itemize}
\item {\tt conda install -c conda-forge ipywidgets}
\item {\tt pip install jupyter\_dashboards}
\item {\tt jupyter dashboards quick-setup --sys-prefix}
\item {\tt conda install -c conda-forge bqplot}
\item {\tt conda install -c conda-forge pythreejs}
\end{itemize}
Note that the bqplot installation requires the environment settings mentioned above.

%========================================================
\section{Examples}\label{sec:examples}
%========================================================

The examples shown in table \ref{tab_0} are intended to help with getting started with ORE, and to serve as plausibility
checks for the simulation results generated with ORE.

\begin{table}[hbt]
\scriptsize
\begin{center}
\begin{tabular}{|c|l|}
\hline
Example & Description \\
\hline
\hline
1 & Vanilla at-the-money Swap with flat yield curve \\
\hline
2 & Vanilla Swap with normal yield curve \\
\hline
3 & European Swaption \\
\hline
4 & Bermudan Swaption \\
\hline
5 & Callable Swap \\
\hline
6 & Cap/Floor \\
\hline
7 & FX Forward \\
  & European FX Option \\ 
\hline
8 & Cross Currency Swap without notional reset \\
\hline
9 & Cross Currency Swap with notional reset \\
\hline
10 & Three-Swap portfolio with netting and collateral \\
   & XVAs - CVA, DVA, FVA, MVA, COLVA \\
   & Exposure and XVA Allocation to trade level \\
\hline
11 & Basel exposure measures - EE, EPE, EEPE \\
\hline
12 & Long term simulation with horizon shift \\
\hline
13 & Dynamic Initial Margin and MVA \\
\hline
\end{tabular}
\caption{ORE examples.}
\label{tab_0}
\end{center}
\end{table}
All example results can be produced with the Python scripts {\tt run.py} in the ORE release's {\tt Examples/Example\_\#}
folders which work on both Windows and Unix platforms. In a nutshell, all scripts call ORE's command line application
with a single input XML file

\medskip
\centerline{\tt ore[.exe] ore.xml}
\medskip

They produce a number of standard reports and exposure graphs in PDF format. The structure of the input file and of the
portfolio, market and other configuration files referred to therein will be explained in section
\ref{sec:configuration}.

\medskip ORE is driven by a number of input files, listed in table \ref{tab_1} and explained in detail in sections
\ref{sec:configuration} to \ref{sec:fixings}. In all examples, these input files are either located in the example's sub
directory {\tt Examples/Example\_\#/Input} or the main input directory {\tt Examples/Input} if used across several
examples. The particular selection of input files is determined by the 'master' input file {\tt ore.xml}.

\begin{table}[h]
\scriptsize
\begin{center}
\begin{tabular}{|l|p{11cm}|}
  \hline
  File Name & Description \\
  \hline
  {\tt ore.xml}&   Master input file, selection of further inputs below and selection of analytics \\
  {\tt portfolio.xml} & Trade data \\
  {\tt netting.xml} &  Collateral (CSA) data \\
  {\tt simulation.xml} & Configuration of simulation model and market\\
  {\tt market.txt} &  Market data snapshot \\
  {\tt fixings.txt} &  Index fixing history \\
  {\tt curveconfig.xml} & Curve and term structure composition from individual market instruments\\
  {\tt conventions.xml} & Market conventions for all market data points\\
  {\tt todaysmarket.xml} &  Configuration of the market composition, relevant for the pricing of the given portfolio as
                           of today (yield curves, FX rates, volatility surfaces etc) \\
  {\tt pricingengines.xml} &  Configuration of pricing methods by product\\
  \hline
\end{tabular}
\end{center}
\caption{ORE input files}
\label{tab_1}
\end{table}

The typical list of output files and reports is shown in table \ref{tab_2}. The names of output files can be configured
through the master input file {\tt ore.xml}. Whether these reports are generated also depends on the setting in {\tt
  ore.xml}. For the examples, all output will be written to the directory {\tt Examples/Example\_\#/Output}.

\begin{table}[h]
\scriptsize
\begin{center}
\begin{tabular}{|l|p{11cm}|}
\hline
File Name & Description \\
\hline
{\tt npv.csv}&   NPV report \\
{\tt flows.csv} & Cashflow report \\
{\tt curves.csv} & Generated yield (discount) curves report \\
{\tt xva.csv} & XVA report, value adjustments at netting set and trade level \\
{\tt exposure\_trade\_*.csv} & Trade exposure evolution reports\\
{\tt exposure\_nettingset\_*.csv} &  Netting set exposure evolution reports\\
{\tt rawcube.csv} & NPV cube in readable text format \\
{\tt netcube.csv} & NPV cube after netting and colateral, in readable text format \\
{\tt *.dat} & Intermediate storage of NPV cube and scenario data in binary format \\
{\tt *.pdf} &  Exposure graphics produced by the python script {\tt run.py} after ORE completed\\
\hline
\end{tabular}
\end{center}
\caption{ORE output files}
\label{tab_2}
\end{table}

Note: When building ORE from sources on Windows platforms, make sure that you copy your {\tt ore.exe} to the binary
directory {\tt bin/win32/} respectively {\tt bin/x64/}. Otherwise the examples may be run using the pre-compiled
executables which come with the ORE release.

%--------------------------------------------------------
\subsection{Interest Rate Swap Exposure}\label{sec:example1}
%--------------------------------------------------------

We start with a vanilla single currency Swap (currency EUR, maturity 20y, notional 10m, receive fixed 2\% annual, pay
6M-Euribor flat). The market yield curves (for both discounting and forward projection) are set to be flat at 2\% for
all maturities, i.e. the Swap is at the money initially and remains at the money on average throughout its life. Running
ORE in directory {\tt Examples/Example\_1} with

\medskip
\centerline{\tt python run.py } 
\medskip

yields the exposure evolution in 

\medskip
\centerline{\tt Examples/Example\_1/Output/*.pdf } 
\medskip

and shown in figure \ref{fig_1}. 
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_swap_1_1m_sbb_100k.pdf}
\end{center}
\caption{Vanilla ATM Swap expected exposure in a flat market environment from both parties' perspectives. The symbols
  are European Swaption prices. The simulation was run with monthly time steps. To demonstrate the convergence of EPE
  and ENE profiles we have used an exceptionally large number of samples (100,000) to produce this graph. A similar
  outcome can be obtained more quickly with 5,000 samples on a quarterly time grid which is the default setting of
  Example\_1. }
\label{fig_1}
\end{figure}
Both Swap simulation and Swaption pricing are run with calls to the ORE executable, essentially 

\medskip
\centerline{\tt ore[.exe] ore.xml} 

\centerline{\tt ore[.exe] ore\_swaption.xml} 
\medskip

which are wrapped into the script {\tt Examples/Example\_1/run.py} provided with the ORE release.
It is instructive to look into the input folder in Examples/Example\_1, the content of the main input file {\tt
  ore.xml}, together with the explanations in section \ref{sec:configuration}. \\

This simple example is an important test case which is also run similarly in one of the unit test suites of ORE. The
expected exposure can be seen as a European option on the underlying netting set, see also appendix
\ref{sec:app_exposure}. In this examplÂ«e, the expected exposure at some future point in time, say 10 years, is equal to
the European Swaption price for an option with expiry in 10 years, underlying Swap start in 10 years and underlying Swap
maturity in 20 years. We can easily compute such standard European Swaption prices for all future points in time where
both Swap legs reset, i.e. annually in this case\footnote{Using closed form expressions for standard European Swaption
  prices.}. And if the simulation model has been calibrated to the points on the Swaption surface which are used for
European Swaption pricing, then we can expect to see that the simulated exposure matches Swaption prices at these annual
points, as in figure \ref{fig_1}.  In Example\_1 we used co-terminal ATM Swaptions for both model calibration and
Swaption pricing. Moreover, as the the yield curve is flat in this example, the exposures from both parties'
perspectives (EPE and ENE) match not only at the annual resets, but also for the period between annual reset of both
legs to the point in time when the floating leg resets. Thereafter, between floating leg (only) reset and next joint
fixed/floating leg reset, we see and expect a deviation of the two exposure profiles.

\medskip Moving to {\tt Examples/Example\_2}, we see what changes when using a realistic (non-flat) market
environment. Running the example with

\medskip
\centerline{\tt python run.py } 
\medskip

yields the exposure evolution in 

\medskip
\centerline{\tt Examples/Example\_2/Output/*.pdf } 
\medskip

shown in figure \ref{fig_2}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_swap_3.pdf}
\end{center}
\caption{Vanilla ATM Swap expected exposure in a realistic market environment as of 05/02/2016 from both parties'
  perspectives. The Swap is the same as in figure \ref{fig_1} but receiving fixed 1\%, roughly at the money. The symbols
  are the prices of European payer and receiver Swaptions. Simulation with 5000 paths and monthly time steps.}
\label{fig_2}
\end{figure}
In this case, where the curves (discount and forward) are upward sloping, the receiver Swap is at the money at inception
only and moves (on average) out of the money during its life. Similarly, the Swap moves into the money from the
counterparty's perspective. Hence the expected exposure evolutions from our perspective (EPE) and the counterparty's
perspective (ENE) 'detach' here, while both can still be be reconciled with payer or respectively receiver Swaption
prices.

%--------------------------------------------------------
\subsection{European Swaption Exposure}\label{sec:european_swaption}
%--------------------------------------------------------

This demo case in folder {\tt Examples/Example\_3} shows the exposure evolution of European Swaptions with cash and
physical delivery, respectively, see figure \ref{fig_3}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_swaption.pdf}
\end{center}
\caption{European Swaption exposure evolution, expiry in 10 years, final maturity in 20 years, for cash and physical
  delivery. Simulation with 1000 paths and quarterly time steps.}
\label{fig_3}
\end{figure}
The delivery type (cash vs physical) yields significantly different valuations as of today due to the steepness of the
relevant yield curves (EUR). The cash settled Swaption's exposure graph is truncated at the exercise date, whereas the
physically settled Swaption exposure turns into a Swap-like exposure after expiry. For comparison, the example also
provides the exposure evolution of the underlying forward starting Swap which yields a somewhat higher exposure after
the forward start date than the physically settled Swaption. This is due to scenarios with negative Swap NPV at expiry
(hence not exercised) and positive NPVs thereafter.

%--------------------------------------------------------
\subsection{Bermudan Swaption Exposure}
%--------------------------------------------------------

This demo case in folder {\tt Examples/Example\_4} shows the exposure evolution of Bermudan rather than European
Swaptions with cash and physical delivery, respectively, see figure \ref{fig_3b}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_bermudan_swaption.pdf}
\end{center}
\caption{Bermudan Swaption exposure evolution, 5 annual exercise dates starting in 10 years, final maturity in 20 years,
  for cash and physical delivery. Simulation with 1000 paths and quarterly time steps.}
\label{fig_3b}
\end{figure}
The underlying Swap is the same as in the European Swaption example in section \ref{sec:european_swaption}. Note in
particular the difference between the Bermudan and European Swaption exposures with cash settlement: The Bermudan shows
the typical step-wise decrease due to the series of exercise dates. Also note that we are using the same Bermudan option
pricing engines for both settlement types, in contrast to the European case, so that the Bermudan option cash and
physical exposures are identical up to the first exercise date. When running this example, you will notice the
significant difference in computation time compared to the European case (ballpark 30 minutes here for 2 Swaptions, 1000
samples, 90 time steps). The Bermudan example takes significantly more computation time because we use an LGM grid
engine for pricing under scenarios in this case. In a realistic context one would more likely resort to American Monte
Carlo simulation, feasible in ORE, but not provided in the first release. However, this implementation can be used to
benchmark any faster / more sophisticated approach to Bermudan Swaption exposure simulation.

%--------------------------------------------------------
\subsection{Callable Swap Exposure}
%--------------------------------------------------------

This demo case in folder {\tt Examples/Example\_5} shows the exposure evolution of a European callable Swap, represented
as two trades - the non-callable Swap and a Swaption with physical delivery. We have sold the call option, i.e. the
Swaption is a right for the counterparty to enter into an offsetting Swap which economically terminates all future flows
if exercised. The resulting exposure evolutions for the individual components (Swap, Swaption), as well as the callable
Swap are shown in figure \ref{fig_4}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_callable_swap.pdf}
\end{center}
\caption{European callable Swap represented as a package consisiting of non-callable Swap and Swaption. The Swaption has
  physical delivery and offsets all future Swap cash flows if exercised. The exposure evolution of the package is shown
  here as 'EPE NettingSet' (green line). This is covered by the pink line, the exposure evolution of the same Swap but
  with maturity on the exercise date. The graphs match perfectly here, because the example Swap is deep in the money and
  exercise probability is close to one. Simulation with 5000 paths and quarterly time steps.}
\label{fig_4}
\end{figure}
The example is an extreme case where the underlying Swap is deeply in the money (receiving fixed 5\%), and hence the
call exercise probability is close to one. Modify the Swap and Swaption fixed rates closer to the money ($\approx$ 1\%)
to see the deviation between net exposure of the callable Swap and the exposure of a 'short' Swap with maturity on
exercise.

%--------------------------------------------------------
\subsection{Cap/Floor Exposure}\label{sec:capfloor}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_6} generates exposure evolutions of several Swaps, caps and floors. The
example shown in figure \ref{fig_capfloor_1} ('portfolio 1') consists of a 20y Swap receiving 3\% fixed and paying
Euribor 6M plus a long 20y Collar
with both cap and floor at 4\% so that the net exposure corresponds to a Swap paying 1\% fixed. \\

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_capfloor_1.pdf}
\end{center}
\caption{Swap+Collar, portfolio 1. The Collar has identical cap and floor rates at 4\% so that it corresponds to a
  fixed leg which reduces the exposure of the Swap, which receives 3\% fixed. Simulation with 1000 paths and quarterly
  time steps.}
\label{fig_capfloor_1}
\end{figure}

The second example in this folder shown in figure \ref{fig_capfloor_2} ('portfolio 2') consists of a short Cap, long
Floor and a long Collar that exactly offsets the netted Cap and Floor.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_capfloor_2.pdf}
\end{center}
\caption{Short Cap and long Floor vs long Collar, portfolio 2. Simulation with 1000 paths and quarterly time steps.}
\label{fig_capfloor_2}
\end{figure}

Further three test portfolios are provided as part of this example. Run the example and inspect the respective output
directories {\tt Examples/Example\_7/Output/portfolio\_\#}. Note that these directories have to be present/created
before running the batch with {\tt python run.py}.

%--------------------------------------------------------
\subsection{FX Forward Exposure}\label{sec:fxfwd}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_7} generates the exposure evolution for a EUR / USD FX Forward transaction
with value date in 10Y. This is a particularly simple show case because of the single cash flow in 10Y. On the other
hand it checks the cross currency model implementation by means of comparison to analytic limits - EPE and ENE at the
trade's value date must match corresponding Vanilla FX Option prices, as shown in figure \ref{fig_5}.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.45]{mpl_fxforward.pdf}
\end{center}
\caption{EUR/USD FX Forward expected exposure in a realistic market environment as of 26/02/2016 from both parties'
  perspectives. Value date is obviously in 10Y. The flat lines are FX Option prices which coincide with EPE and ENE,
  respectively, on the value date. Simulation with 5000 paths and quarterly time steps.}
\label{fig_5}
\end{figure}

%--------------------------------------------------------
\subsection{FX Option Exposure}
%--------------------------------------------------------

This example (in folder {\tt Examples/Example\_7}, as the FX Forward example) illustrates the exposure evolution for an
FX Option, see figure \ref{fig_7}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_fxoption.pdf}
\end{center}
\caption{EUR/USD FX Call and Put Option exposure evolution, same underlying and market data as in section
  \ref{sec:fxfwd}, compared to the call and put option price as of today (flat line). Simulation with 5000 paths and
  quarterly time steps.}
\label{fig_7}
\end{figure}
Recall that the FX Option value $NPV(t)$ as of time $0 \leq t \leq T$ satisfies
\begin{align*}
\frac{NPV(t)}{N(t)} &= \mbox{Nominal}\times\E_t\left[\frac{(X(T) - K)^+}{N(T)}\right]\\
NPV(0) &= \E\left[\frac{NPV(t)}{N(t)}\right] = \E\left[\frac{NPV^+(t)}{N(t)} \right]= \EPE(t) 
\end{align*}
One would therefore expect a flat exposure evolution up to option expiry. The deviation from this in ORE's simulation is
due to the pricing approach chosen here under scenarios. A Black FX option pricer is used with deterministic Black
volatility derived from today's volatility structure (pushed or rolled forward, see section \ref{sec:sim_market}). The
deviation can be removed by extending the volatility modelling, e.g. implying model consistent Black volatilities in
each simulation step on each path.  \todo[inline]{Add exposure evolution graph with 'simulated' FX vol}

%--------------------------------------------------------
\subsection{Cross Currency Swap Exposure and FX Reset}
%--------------------------------------------------------

The case in {\tt Examples/Example\_8} is a vanilla cross currency Swap. It shows the typical blend of an Interest Rate
Swap's saw tooth exposure evolution with an FX Forward's exposure which increases monotonically to final maturity, see
figure \ref{fig_6}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_ccswap.pdf}
\end{center}
\caption{Cross Currency Swap exposure evolution without mark-to-market notional reset. Simulation with 1000 paths and
  quarterly time steps.}
\label{fig_6}
\end{figure}

The effect of the FX resetting feature, common in Cross Currency Swaps nowadays, is shown in {\tt Examples/Example\_9}.
The example shows the exposure evolution of a EUR/USD cross currency basis Swap with FX reset at each interest period
start, see figure \ref{fig_6b}. As expected, the notional reset causes an exposure collapse at each period start when
the EUR leg's notional is reset to match the USD notional.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_xccy_reset.pdf}
\end{center}
\caption{Cross Currency Basis Swap exposure evolution with and without mark-to-market notional reset. Simulation with
  1000 paths and quarterly time steps.}
\label{fig_6b}
\end{figure}
  
%--------------------------------------------------------
\subsection{Netting and Collateral}
%--------------------------------------------------------

In this example (see folder {\tt Examples/Example\_10}) we showcase a small netting set consisting of three Swaps in
different currencies, with different collateral choices
\begin{itemize}
\item no collateral - figure \ref{fig_8},
\item collateral with threshold (THR) 1m EUR, minimum transfer amount (MTA) 100k EUR, margin period of risk (MPOR) 2
  weeks - figure \ref{fig_9}
\item collateral with zero THR and MTA, and MPOR 2w - figure \ref{fig_10}
\end{itemize}
The exposure graphs with collateral and positive margin period of risk show typical spikes. What is causing these? As
sketched in appendix \ref{sec:app_collateral}, ORE uses a {\em classical collateral model} that applies collateral
amounts to offset exposure with a time delay that corresponds to the margin period of risk. The spikes are then caused
by instrument cash flows falling between exposure measurement dates $d_1$ and $d_2$ (an MPOR apart), so that a
collateral delivery amount determined at $d_1$ but settled at $d_2$ differs significantly from the closeout amount at
$d_2$ causing a significant residual exposure for a short period of time. See for example \cite{Andersen2016} for a
recent detailed discussion of collateral modelling. The approach currently implemented in ORE corresponds to {\em
  Classical+} in \cite{Andersen2016}, the more conservative approach of the classical methods. The less conservative
alternative, {\em Classical-}, would assume that both parties stop paying trade flows at the beginning of the MPOR, so
that the P\&L over the MPOR does not contain the cash flow effect, and exposure spikes are avoided. Note that the size
and position of the largest spike in figure \ref{fig_9} is consistent with a cash flow of the 40 million GBP Swap in the
example's portfolio that rolls over the 3rd of March and has a cash flow on 3 March 2020, a bit more than four years
from the evaluation date.
  
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_nocollateral_epe.pdf}
\end{center}
\caption{Three Swaps netting set, no collateral. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_8}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.45]{mpl_threshold_break_epe.pdf}
\end{center}
\caption{Three Swaps netting set, THR=1m EUR, MTA=100k EUR, MPOR=2w. The red evolution assumes that the each trade is
  terminated at the next break date. The blue evolution ignores break dates. Simulation with 5000 paths and bi-weekly
  time steps.}
\label{fig_9}
\end{figure}

%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=1.0]{example_mta_epe.pdf}
%\end{center}
%\caption{Three swaps, threshold = 0, mta > 0.}
%\label{fig_7}
%\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_mpor_epe.pdf}
\end{center}
\caption{Three Swaps, THR=MTA=0, MPOR=2w. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_10}
\end{figure}

%--------------------------------------------------------
\subsection{CVA, DVA, FVA, COLVA, MVA, Collateral Floor}
%--------------------------------------------------------

We use one of the cases in {\tt Examples/Example\_10} to demonstrate the
XVA outputs, see folder {\tt Examples/Example\_10/Output/collateral\_threshold\_dim}.

\medskip The summary of all value adjustments (CVA, DVA, FVA, COLVA, MVA, as well as the Collateral Floor) is provided
in file {\tt xva.csv}.  The file includes the allocated CVA and DVA numbers to individual trades as introduced in the
next section. The following table illustrates the file's layout, omitting the three columns containing allocated data.

\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|r|r|r|r|r|r|r|r|r|}
\hline
TradeId & NettingSetId & CVA & DVA & FBA & FCA & COLVA & MVA & CollateralFloor & BaselEPE & BaselEEPE \\
\hline
 & CPTY\_A &  6,521  &  151,193  & -946  &  72,103  &  2,769  & -14,203  &  189,936  &  113,260  &  1,211,770 \\
Swap\_1 & CPTY\_A &  127,688  &  211,936  & -19,624  &  100,584  &  n/a  &  n/a  &  n/a   &  2,022,590  &  2,727,010 \\
Swap\_3 & CPTY\_A &  71,315  &  91,222  & -11,270  &  43,370  &  n/a  &  n/a  &  n/a   &  1,403,320  &  2,183,860 \\
Swap\_2 & CPTY\_A &  68,763  &  100,347  & -10,755  &  47,311  &  n/a  &  n/a  &  n/a   &  1,126,520  &  1,839,590 \\
\hline
\end{tabular}
}
\end{center}

The line(s) with empty TradeId column contain values at netting set level, the others contain uncollateralised
single-trade VAs.  Note that COLVA, MVA and Collateral Floor are only available at netting set level at which collateral
is posted.

\medskip
Detailed output is written for COLVA and Collateral Floor to file {\tt colva\_nettingset\_*.csv} which shows the 
incremental contributions to these two VAs through time.


%--------------------------------------------------------
\subsection{Exposure Reports \& XVA Allocation to Trades}
%--------------------------------------------------------
Using the example in folder {\tt Examples/Example\_10} we illustrate here the layout of an exposure report produced by
ORE. The report shows the exposure evolution of Swap\_1 without collateral which - after running Example\_10 - is found
in folder \\
{\tt Examples/Example\_10/Output/collateral\_none/exposure\_trade\_Swap\_1.csv}:

\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|r|r|r|r|r|r|r|r|}
\hline
TradeId & Date & Time & EPE & ENE & AllocEPE & AllocENE & PFE & BaselEE & BaselEEE \\
\hline
Swap\_1 & 05/02/16 & 0.0000 & 0  & 1,711,850  & 0  & 0  & 0  & 0  & 0 \\
Swap\_1 & 19/02/16 & 0.0383 & 40,219  & 1,744,080  & -1,190,830  & 513,032  & 263,973  & 40,217  & 40,217 \\
Swap\_1 & 04/03/16 & 0.0765 & 137,552  & 1,840,760  & -914,468  & 788,739  & 1,053,940  & 137,535  & 137,535 \\
Swap\_1 & 18/03/16 & 0.1148 & 299,155  & 1,742,450  & -650,225  & 793,067  & 1,914,150  & 299,091  & 299,091 \\
Swap\_1 & 01/04/16 & 0.1530 & 390,178  & 1,834,810  & -552,029  & 892,604  & 2,373,560  & 390,058  & 390,058 \\
Swap\_1 & 15/04/16 & 0.1913 & 471,849  & 1,918,600  & -465,580  & 981,171  & 2,765,710  & 471,659  & 471,659 \\
Swap\_1 & 29/04/16 & 0.2295 & 550,301  & 2,000,640  & -330,578  & 1,119,760  & 3,106,810  & 550,016  & 550,016 \\
Swap\_1 & 13/05/16 & 0.2678 & 620,279  & 2,074,880  & -266,042  & 1,188,560  & 3,427,080  & 619,888  & 619,888 \\
Swap\_1 & 27/05/16 & 0.3060 & 690,018  & 2,140,320  & -190,419  & 1,259,880  & 3,778,570  & 689,509  & 689,509 \\
Swap\_1 & 10/06/16 & 0.3443 & 763,207  & 2,206,020  & -137,681  & 1,305,130  & 4,052,870  & 762,560  & 762,560 \\
Swap\_1 & ... & ...& ... & ... & ... & ... & ... & ... & ... \\
\hline
\end{tabular}
}
\end{center}

The exposure measures EPE, ENE and PFE, and the Basel exposure measures $EE_B$ and $EEE_B$, are defined in appendix
\ref{sec:app_exposure}. Allocated exposures are defined in appendix \ref{sec:app_allocation}. The PFE quantile and
allocation method are chosen as described in section \ref{sec:analytics}. \\

In addition to single trade exposure files, ORE produces an exposure file per netting set. The example from the same
folder as above is:

\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|r|r|r|r|r|r|r|}
\hline
NettingSet & Date & Time & EPE & ENE & PFE & ExpectedCollateral & BaselEE & BaselEEE \\
\hline
CPTY\_A & 05/02/16 & 0.0000 & 1,211,770 & 0 & 1,211,770 & 0 & 1,211,770 & 1,211,770\\
CPTY\_A & 19/02/16 & 0.0383 & 1,344,220 & 137,776 & 3,414,000 & 0 & 1,344,160 & 1,344,160\\
CPTY\_A & 04/03/16 & 0.0765 & 1,518,610 & 308,381 & 4,354,060 & 0 & 1,518,410 & 1,518,410\\
CPTY\_A & 18/03/16 & 0.1148 & 1,846,900 & 382,068 & 5,200,730 & 0 & 1,846,500 & 1,846,500\\
CPTY\_A & 01/04/16 & 0.1530 & 1,961,290 & 494,416 & 5,869,470 & 0 & 1,960,690 & 1,960,690\\
CPTY\_A & 15/04/16 & 0.1913 & 2,067,240 & 598,283 & 6,384,140 & 0 & 2,066,400 & 2,066,400\\
CPTY\_A & 29/04/16 & 0.2295 & 2,053,670 & 745,960 & 6,740,070 & 0 & 2,052,610 & 2,066,400\\
CPTY\_A & 13/05/16 & 0.2678 & 2,149,190 & 845,507 & 6,930,230 & 0 & 2,147,840 & 2,147,840\\
CPTY\_A & 27/05/16 & 0.3060 & 2,235,630 & 930,218 & 7,295,440 & 0 & 2,233,980 & 2,233,980\\
CPTY\_A & 10/06/16 & 0.3443 & 2,314,470 & 1,014,690 & 7,753,190 & 0 & 2,312,510 & 2,312,510\\
CPTY\_A & ... & ...& ... & ... & ... & ... & ... & ...\\
CPTY\_A & 07/07/17 & 1.4167 & 3,320,430 & 2,423,890 & 12,787,900 & 0 & 3,304,650 & 3,304,650\\
CPTY\_A & 21/07/17 & 1.4551 & 3,351,780 & 2,452,640 & 12,964,200 & 0 & 3,335,420 & 3,335,420\\
CPTY\_A & 04/08/17 & 1.4934 & 3,302,820 & 2,511,500 & 12,796,100 & 0 & 3,286,260 & 3,335,420\\
CPTY\_A & 18/08/17 & 1.5318 & 3,339,840 & 2,545,850 & 13,120,000 & 0 & 3,322,640 & 3,335,420\\
CPTY\_A & 01/09/17 & 1.5701 & 3,371,300 & 2,576,100 & 13,238,700 & 0 & 3,353,480 & 3,353,480\\
CPTY\_A & 15/09/17 & 1.6085 & 3,279,670 & 2,555,370 & 13,041,300 & 0 & 3,261,880 & 3,353,480\\
CPTY\_A & 29/09/17 & 1.6468 & 3,305,060 & 2,579,200 & 13,072,800 & 0 & 3,286,680 & 3,353,480\\
CPTY\_A & 13/10/17 & 1.6852 & 3,332,830 & 2,604,200 & 13,225,600 & 0 & 3,313,850 & 3,353,480\\
CPTY\_A & 27/10/17 & 1.7236 & 3,280,280 & 2,661,770 & 13,034,600 & 0 & 3,261,150 & 3,353,480\\
CPTY\_A & 13/11/17 & 1.7701 & 3,316,800 & 2,701,060 & 13,331,600 & 0 & 3,296,880 & 3,353,480\\
CPTY\_A & 24/11/17 & 1.8003 & 3,337,760 & 2,720,870 & 13,402,400 & 0 & 3,317,280 & 3,353,480\\
CPTY\_A & ... & ...& ... & ... & ... & ... & ... & ...\\
\hline
\end{tabular}
}
\end{center}

Allocated exposures are missing here, as they make sense at the trade level only, and the expected collateral balance is
added for information (in this case zero as collateralisation is deactivated in this example).

\medskip The allocation of netting set exposure and XVA to the trade level is frequently required by finance
departments. This allocation is also featured in {\tt Examples/Example\_10}. We start again with the uncollateralised
case in figure \ref{fig_12}, followed by the case with threshold 1m EUR in figure \ref{fig_13}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_nocollateral_allocated_epe.pdf}
\end{center}
\caption{Exposure allocation without collateral. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_12}
\end{figure}
In both cases we apply the {\em marginal} (Euler) allocation method as published by Pykhtin and Rosen in 2010, hence we
see the typical negative EPE for one of the trades at times when it reduces the netting set exposure. The case with
collateral moreover shows the typical spikes in the allocated exposures.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_threshold_allocated_epe.pdf}
\end{center}
\caption{Exposure allocation with collateral and threshold 1m EUR. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_13}
\end{figure}
The analytics results also feature allocated XVAs in file {\tt xva.csv} which are derived from the allocated exposure
profiles. Note that ORE also offers alternative allocation methods to the marginal method by Pykhtin/Rosen, which can be
explored with {\tt Examples/Example\_10}.

%--------------------------------------------------------
\subsection{Basel Exposure Measures}\label{sec:basel}
%--------------------------------------------------------

Example {\tt Example\_11} demonstrates the relation between the evolution of the expected exposure (EPE in our notation)
to the `Basel' exposure measures EE\_B, EEE\_B, EPE\_B and EEPE\_B as defined in appendix \ref{sec:app_exposure}. In
particular the latter is used in internal model methods for counterparty credit risk as a measure for the exposure at
default. It is a `derivative' of the expected exposure evolution and defined as a time average over the running maximum
of EE\_B up to the horizon of one year.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_basel_exposures.pdf}
\end{center}
\caption{Evolution of the expected exposure of Vanilla Swap, comparison to the `Basel' exposure measures EEE\_B, EPE\_B and EEPE\_B.}
\label{fig_14}
\end{figure}

%--------------------------------------------------------
\subsection{Long Term Simulation with Horizon Shift}\label{sec:longterm}
%--------------------------------------------------------

The example in folder {\tt Example\_12} finally demonstrates an effect that, at first glance, seems to cause a serious
issue with long term simulations. Fortunately this can be avoided quite easily in the Linear Gauss Markov model setting
that is used here. \\

In the example we consider a Swap with maturity in 50 years in a flat yield curve environment. If we simulate this
naively as in all previous cases, we obtain a particularly noisy EPE profile that does not nearly reconcile with the
known exposure (analytical Swaption prices). This is shown in figure \ref{fig_15} (`no horizon shift'). The origin of
this issue is the width of the risk-neutral NPV distribution at long time horizons which can turn out to be quite small
so that the Monte Carlo simulation with finite number of samples does not reach far enough into the positive or negative
NPV range to adequately sample the distribution, and estimate both EPE and ENE in a single run.  Increasing the number
of samples may not solve the problem, and may not even be feasible in a realistic setting. \\

The way out is applying a `shift transformation' to the Linear Gauss Markov model, see {\tt
  Example\_12/Input/simulation2.xml} in lines 92-95:
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
        <ParameterTransformation>
          <ShiftHorizon>30.0</ShiftHorizon>
          <Scaling>1.0</Scaling>
        </ParameterTransformation>
\end{minted}
%\hrule
%\caption{LGM Shift transformation}
%\label{lst:shift_transformation}
\end{listing}

The effect of the 'ShiftHorizon' parameter $T$ is to apply a shift to the Linear Gauss Markov model's $H(t)$ parameter
(see appendix \ref{sec:app_rfe}) {\em after} the model has been calibrated, i.e. to replace:
$$ 
H(t) \rightarrow H(t) - H(T) 
$$ 
It can be shown that this leaves all expectations computed in the model (such as EPE and ENE) invariant. As explained in
\cite{Lichters}, subtracting a $H$ shift effectively means performing a change of measure from the `native' LGM measure
to a T-Forward measure with horizon $T$, here 30 years. Both negative and positive shifts are permissible, but only
negative shifts are connected with a T-Forward measure and improve numerical stability. \\

In our experience it is helpful to place the horizon in the middle of the portfolio duration to significantly improve
the quality of long term expectations. The effect of this change (only) is shown in the same figure \ref{fig_15}
(`shifted horizon').
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_longterm.pdf}
\end{center}
\caption{Long term Swap exposure simulation with and without horizon shift.}
\label{fig_15}
\end{figure}
Figure \ref{fig_15b} further illustrates the origin of the problem and its resolution: The rate distribution's mean
(without horizon shift or change of measure) drifts upwards due to convexity effects (note that the yield curve is flat
in this example), and the distribution's width is then too narrow at long horizons to yield a sufficient number of low
rate scenarios with contributions to the Swap's $\EPE$ (it is a floating rate payer). With the horizon shift (change of
measure), the distribution's mean is pulled 'back' at long horizons, because the convexity effect is effectively wiped
out at the chosen horizon, and the expected rate matches the forward rate.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_rates.pdf}
\end{center}
\caption{Evolution of rate distributions with and without horizon shift (change of measure). Thick lines indicate mean
  values, thin lines are contours of the rate distribution at $\pm$ one standard devation.}
\label{fig_15b}
\end{figure}

%--------------------------------------------------------
\subsection{Dynamic Initial Margin and MVA}\label{sec:dim}
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_13} demonstrates Dynamic Initial Margin calculations (see also appendix
\ref{sec:app_dim}) for a number of elementary products:
\begin{itemize}
\item A single currency Swap in EUR (case A), 
\item a European Swaption in EUR with physical delivery (case B), 
\item a single currency Swap in USD (case C), and 
\item a EUR/USD cross currency Swap (case D).
\end{itemize}

The examples can be run as before with 

\medskip
\centerline{\tt python run\_A.py} 

\medskip
and likewise for cases B, C and D. The essential results of each run are are visualised in the form of 
\begin{itemize}
\item evolution of expected DIM
\item regression plots at selected future times 
\end{itemize}
illustrated for cases A and B in figures \ref{fig_ex13a_evolution} - \ref{fig_ex13b_regression}. In all cases the zero
order regression estimate of DIM differs noticeably from the higher orders (one and two). In the three swap cases we
moreover see that first and second order polynomial choice makes hardly any difference; note that case C and D use up to
three-dimensional regressors (simulated USD and EUR rate fixings and the EUR/USD FX rate). In the Swaption case B, first
and second order polynomial choice makes a difference before option expiry.
 
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_evolution_A_swap_eur.pdf}
\end{center}
\caption{Evolution of expected Dynamic Initial Margin (DIM) for the EUR Swap of Example 13 A. DIM is evaluated using
  regression of NPV change variances versus the simulated 3M Euribor fixing; regression polynomials are zero, first and
  second order (first and second order curves are not distinguishable here). The simulation uses 1000 samples and a time
  grid with bi-weekly steps in line with the Margin Period of Risk.}
\label{fig_ex13a_evolution}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_regression_A_swap_eur.pdf}
\end{center}
\caption{Regression snapshot at time step 100 for the EUR Swap of Example 13 A.}
\label{fig_ex13a_regression}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_evolution_B_swaption_eur.pdf}
\end{center}
\caption{Evolution of expected Dynamic Initial Margin (DIM) for the EUR Swaption of Example 13 B with expiry in 10Y
  around time step 100. DIM is evaluated using regression of NPV change variances versus the simulated 3M Euribor
  fixing; regression polynomials are zero, first and second order. The simulation uses 1000 samples and a time grid with
  bi-weekly steps in line with the Margin Period of Risk.}
\label{fig_ex13b_evolution}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_regression_B_swaption_eur_t100.pdf}
\end{center}
\caption{Regression snapshot at time step 100 (before expiry) for the EUR Swaption of Example 13 B.}
\label{fig_ex13b_regression}
\end{figure}

\clearpage
%========================================================
\section{Launchers and Visualisation}\label{sec:visualisation}
%========================================================

\subsection{Jupyter}\label{sec:jupyter}

ORE comes with an experimental Jupyter notebook for launching ORE batches and in particular for drilling into NPV cube
data.  The notebook is located in directory {\tt FrontEnd/Python/Visualization/npvcube}. To launch the notebook, change
to this directory and follow instructions in the {\tt Readme.txt}. In a nutshell, type\footnote{With Mac OS X, you may
  need to set the environment variable {\tt LANG} to {\tt en\_US.UTF-8} before running jupyter, as mentioned in the
  installation section \ref{sec:python}.}

\medskip
\centerline{\tt jupyter notebook}
\medskip

to start the ipyton console and open a browser window. From the list of files displayed in the browser then click

\medskip
\centerline{\tt ore\_jupyter\_dashboard.ipynb} 
\medskip

to open the ORE notebook. The notebook offers
\begin{itemize}
\item launching an ORE job
\item selecting an NPV cube file and netting sets or trades therein
\item plotting a 3d exposure probability density surface
\item viewing exposure probability density function at a selected future time
\item viewing expected exposure evolution through time  
\end{itemize}

The cube file loaded here by default when processing all cells of the notebook (without changing it or launching a ORE
batch) is taken from {\tt Example\_7} (FX Forwards and FX Options).

\todo[inline]{Add Jupyter section}

\subsection{Calc}\label{sec:calc}

ORE comes with a simple LibreOffice Calc \cite{LO} sheet as an ORE launcher and basic result viewer. This is
demonstrated on the example in section \ref{sec:example1}. It is currently based on the stable LibreOffice version 5.0.6
and tested on OS X. \\

To launch Calc, open a terminal, change to directory {\tt Examples/Example\_1}, and run

\medskip
{\centerline{\tt ./launchCalc.sh} }
\medskip

%This will show the blank sheet in figure \ref{fig_14}.
%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=0.4]{demo_calc_1}
%\end{center}
%\caption{Calc sheet after launching.}
%\label{fig_14}
%\end{figure}
The user can choose a configuration (one of the {\tt ore*.xml} files in Example\_1's subfolder Input) by hitting the
'Select' button. Initially Input/ore.xml is pre-selected. The ORE process is then kicked off by hitting 'Run'. Once
completed, standard ORE reports (NPV, Cashflow, XVA) are loaded into several sheets. Moreover, exposure evolutions can
then be viewed by hitting 'View' which shows the result in figure \ref{fig_16}.  \\
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{demo_calc_2}
\end{center}
\caption{Calc sheet after hitting 'Run'.}
\label{fig_16}
\end{figure}

This demo uses simple Libre Office Basic macros which call Python scripts to execute ORE. The Libre Office Python uno
module (which comes with Libre Office) is used to communicate between Python and Calc to upload results into the sheets.

\todo[inline]{Remove hard-coded file names from Python scripts}
\todo[inline]{Calc example on Windows and Linux} 
\todo[inline]{Harmonise layout with Excel launcher} 

\subsection{Excel}\label{sec:excel}

ORE also comes with a basic Excel sheet to demonstrate launching ORE and presenting results in Excel. This demo is more
self-contained than the Calc demo in the previous section, as it uses VBA only rather than calls to external Python
scripts. The Excel demo is available in Example\_1. Launch {\tt Example\_1.xlsm}. Then modify the paths on the first
sheet, and kick off the ORE process.

%========================================================
\section{Parametrisation}\label{sec:configuration}
%========================================================

ORE's batch version is kicked off with a single command line parameter 

\medskip
\centerline{\tt ore[.exe] ore.xml}
\medskip

which points to the 'master input file' referred to  as {\tt ore.xml} subsequently. 
This file is the starting point of the engine's configuration explained in the following sub section.

\subsection{Master Input File: {\tt ore.xml}}\label{sec:master_input}

The master input file contains general setup information (paths to configuration, trade data and market data), as well
as the selection and configuration of analytics. The file has an opening and closing root element {\tt <ORE>}, {\tt
  </ORE>} with three sections
\begin{itemize}
\item Setup
\item Markets
\item Analytics
\end{itemize}
which we will explain in the following.

\subsubsection{Setup}

This subset of data is easiest explained using an example, see listing \ref{lst:ore_setup}.
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Setup>
  <Parameter name="asofDate">2016-02-05</Parameter>
  <Parameter name="inputPath">Input</Parameter>
  <Parameter name="outputPath">Output</Parameter>
  <Parameter name="logFile">log.txt</Parameter>
  <Parameter name="marketDataFile">../../Input/market_20160205.txt</Parameter>
  <Parameter name="fixingDataFile">../../Input/fixings_20160205.txt</Parameter>
  <Parameter name="implyTodaysFixings">Y</Parameter>
  <Parameter name="curveConfigFile">../../Input/curveconfig.xml</Parameter>
  <Parameter name="conventionsFile">../../Input/conventions.xml</Parameter>
  <Parameter name="marketConfigFile">../../Input/todaysmarket.xml</Parameter>
  <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
  <Parameter name="portfolioFile">portfolio.xml</Parameter>
  <!-- None, Unregister, Defer or Disable -->
  <Parameter name="observationModel">Disable</Parameter>
</Setup>
\end{minted}
%\hrule
\caption{ORE setup example}
\label{lst:ore_setup}
\end{listing}

Parameter names are self explanatory: Input and output path are interpreted relative from the directory where the ORE
script is started, but can also be specified using absolute paths. All file names are then interpreted relative to the
'inputPath' and 'outputPath', respectively. The files starting with {\tt ../../Input/} then point to files in the global
Example input directory {\tt Example/Input/*}, whereas files such as {\tt portfolio.xml} are local inputs in {\tt
  Example/Example\_\#/Input/}. \\

When ORE starts, it will initialise today's market, i.e. load market data and fixings, and build all term structures as
specified in {\tt todaysmarket.xml}.  Moreover, ORE will load the trades in {\tt portfolio.xml} and link them with
pricing engines as specified in {\tt pricingengine.xml}. When parameter {\tt implyTodaysFixings} is set to Y, today's
fixings would not be loaded but implied, relevant when pricing/bootstrapping off hypothetical market data as e.g. in
scenario analysis and stress testing.

\medskip The last parameter {\tt observationModel} can be used to control ORE performance during simulation. The choices
{\em Disable } and {\em Unregister } yield similarly improved performance relative to choice {\em None}. For users
familiar with the QuantLib design - the parameter controls to which extent {\em QuantLib observer notifications} are
used when market and fixing data is updated and the evaluation date is shifted:
\begin{itemize}
\item The 'Unregister' option limits the amount of notifications by unregistering floating rate coupons from indices;
\item Option 'Defer' disables all notifications during market data and fixing updates with
{\tt ObservableSettings::instance().disableUpdates(true)}
and kicks off updates afterwards when enabled again
\item The 'Disable' option goes one step further and disables all notifications during market data and fixing updates,
  and in particular when the evaluation date is changed along a path, with \\
  {\tt ObservableSettings::instance().disableUpdates(false)} \\
  Updates are not deferred here. Required term structure and instrument recalculations are triggered explicitly.
\end{itemize}
\todo[inline]{Expand the technical description of observationModel}

\subsubsection{Markets}\label{sec:master_input_markets}

The {\tt Markets} section (see listing \ref{lst:ore_markets}) is used to choose market configurations for calibrating
the IR and FX simulation model components, pricing and simulation, respectively. These configurations have to be defined
in {\tt todaysmarket.xml} (see section \ref{sec:market}).

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Markets>
  <Parameter name="lgmcalibration">collateral_inccy</Parameter>
  <Parameter name="fxcalibration">collateral_eur</Parameter>
  <Parameter name="pricing">collateral_eur</Parameter>
  <Parameter name="simulation">collateral_eur</Parameter>
</Markets>
\end{minted}
%\hrule
\caption{ORE markets}
\label{lst:ore_markets}
\end{listing}

For example, the calibration of the simulation model's interest rate components requires local OIS discounting whereas
the simulation phase requires cross currency adjusted discount curves to get FX product pricing right. So far, the
market configurations are used only to distinguish discount curve sets, but the market configuration concept in ORE
applies to all term structure types.

\subsubsection{Analytics}\label{sec:analytics}

The {\tt Analytics} section lists all permissible analytics using tags {\tt <Analytic type="..."> ... </Analytic>} where
type can be (so far) in
\begin{itemize}
\item npv
\item cashflow
\item curves
\item simulation
\item xva
\item initialMargin
\end{itemize}

Each {\tt Analytic} section contains a list of key/value pairs to parameterise the analysis of the form {\tt <Parameter
  name="key">value</Parameter>}. Each analysis must have one key {\tt active} set to Y or N to activate/deactivate this
analysis.  The following listing \ref{lst:ore_analytics} shows the parametrisation of the first three basic analytics in
the list above.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>    
  <Analytic type="npv">
    <Parameter name="active">Y</Parameter>
    <Parameter name="baseCurrency">EUR</Parameter>
    <Parameter name="outputFileName">npv.csv</Parameter>
  </Analytic>      
  <Analytic type="cashflow">
    <Parameter name="active">Y</Parameter>
    <Parameter name="outputFileName">flows.csv</Parameter>
  </Analytic>      
  <Analytic type="curves">
    <Parameter name="active">Y</Parameter>
    <Parameter name="configuration">default</Parameter>
    <Parameter name="grid">240,1M</Parameter>
    <Parameter name="outputFileName">curves.csv</Parameter>
  </Analytic>
  <Analytic type="...">
    <!-- ... -->
  </Analytic>      
</Analytics>      
\end{minted}
\caption{ORE analytics: npv, cashflow and curves}
\label{lst:ore_analytics}
\end{listing}

The cashflow analytic writes a report containing all future cashflows of the portfolio. Table \ref{cashflowreport} shows
a typical output for a vanilla swap.

\begin{table}[hbt]
\scriptsize
\begin{center}
  \begin{tabular}{l|l|l|l|r|l|r|r|l|r}
\hline
\#ID & Type & LegNo & PayDate & Amount & Currency & Coupon & Accrual & fixingDate & fixingValue \\
\hline
\hline
tr123 & Swap & 0 & 13/03/17 & -76212.5 & EUR & -0.00201 & 0.50556 & 08/09/16 & -0.00201 \\
tr123 & Swap & 0 & 12/09/17 & -90683.9212 & EUR & -0.002379 & 0.50833 & 09/03/17 & -0.002381 \\
\ldots
\end{tabular}
\caption{Cashflow Report}
\label{cashflowreport}
\end{center}
\end{table}

The amount column contains the projected amount including embedded caps and floors and convexity (if applicable), the
coupon column displays the corresponding rate estimation. The fixing value on the other hand is the plain fixing
projection as given by the forward value, i.e. without embedded caps and floors or convexity.

Note that the fixing value might deviate from the coupon value even for a vanilla coupon, if the QuantLib library was
compiled {\em without} the flag \verb+QL_USE_INDEXED_COUPON+ (which is the default case). In this case the coupon value
uses a par approximation for the forward rate assuming the index estimation period to be identical to the accrual
period, while the fixing value is the actual forward rate for the index estimation period, i.e. whenever the index estimation
period differs from the accrual period the values will be slightly different.

The curves analytic exports all yield curves that have been built according to the specification in {\tt
  todaysmarket.xml}. Key {\tt configuration} selects the curve set to be used (see explanation in the previous Markets
section).  Key {\tt grid} defines the time grid on which the yield curves are evaluated, in the example above a grid of
240 monthly time steps from today. The discount factors for all curves with configuration default will be exported on
this monthly grid into the csv file specified by key {\tt outputFileName}. The grid can also be specified explicitly by
a comma separated list of tenor points such as {\tt 1W, 1M, 2M, 3M, \dots}.

\medskip The purpose of the {\tt simulation} 'analytics' is to run a Monte Carlo simulation which evolves the market as
specified in the simulation config file. The primary result is an NPV cube file, i.e. valuations of all trades in the
portfolio file (see section Setup), for all future points in time on the simulation grid and for all paths. Apart from
the NPV cube, additional scenario data (such as simulated overnight rates etc) are stored in this process which are
needed for subsequent XVA analytics.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
  <Analytic type="simulation">
    <Parameter name="active">Y</Parameter>
    <Parameter name="simulationConfigFile">simulation.xml</Parameter>
    <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
    <Parameter name="baseCurrency">EUR</Parameter>
    <Parameter name="storeFlows">Y</Parameter>
    <Parameter name="cubeFile">cube_A.dat</Parameter>
    <Parameter name="additionalScenarioDataFileName">scenariodata.dat</Parameter>
    <Parameter name="scenariodump">scenariodump.csv</Parameter>
  </Analytic>
</Analytics>      
\end{minted}
\caption{ORE analytic: simulation}
\label{lst:ore_simulation}
\end{listing}

The pricing engines file specifies how trades are priced under future scenarios which can differ from pricing as of
today (specified in section Setup).  Key base currency determines into which currency all NPVs will be converted. Key
store scenarios (Y or N) determines whether the market scenarios are written to a file for later reuse. And finally, the
key `store flows' (Y or N) controls whether cumulative cash flows between simulation dates are stored in the (hyper-)
cube for post processing in the context of Dynamic Initial Margin and Variation Margin calculations. The additional
scenario data (written to the specified file here) is likewise required in the post processor step. These data comprise
simulated index fixing e.g. for collateral compounding and simulated FX rates for cash collateral conversion into base
currency. The scenario dump file, if specified here, causes ORE to write simulated market data to a human-readable csv
file.
 
\medskip The XVA analytic section offers CVA, DVA, FVA and COLVA calculations which can be selected/deselected here
individually. All XVA calculations depend on a previously generated NPV cube (see above) which is referenced here via
the {\tt cubeFile} parameter. This means one can re-run the XVA analytics without regenerating the cube each time. The
XVA reports depend in particular on the settings in the {\tt csaFile} which determines CSA details such as margining
frequency, collateral thresholds, minimum transfer amounts, margin period of risk. By splitting the processing into
pre-processing (cube generation) and post-processing (aggregation and XVA analysis) it is possible to vary these CSA
details and analyse their impact on XVAs quickly without re-generating the NPV cube.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
  <Analytic type="xva">
    <Parameter name="active">Y</Parameter>
    <Parameter name="csaFile">netting.xml</Parameter>
    <Parameter name="cubeFile">cube.dat</Parameter>
    <Parameter name="hyperCube">Y</Parameter>
    <Parameter name="scenarioFile">scenariodata.dat</Parameter>
    <Parameter name="baseCurrency">EUR</Parameter>
    <Parameter name="exposureProfiles">Y</Parameter>
    <Parameter name="quantile">0.95</Parameter>
    <Parameter name="calculationType">Symmetric</Parameter>      
    <Parameter name="allocationMethod">None</Parameter>    
    <Parameter name="marginalAllocationLimit">1.0</Parameter>
    <Parameter name="exerciseNextBreak">N</Parameter>
    <Parameter name="cva">Y</Parameter>
    <Parameter name="dva">N</Parameter>
    <Parameter name="dvaName">BANK</Parameter>
    <Parameter name="fva">N</Parameter>
    <Parameter name="fvaBorrowingCurve">BANK_EUR_BORROW</Parameter>
    <Parameter name="fvaLendingCurve">BANK_EUR_LEND</Parameter>
    <Parameter name="colva">Y</Parameter>
    <Parameter name="collateralSpread">0.0010</Parameter>
    <Parameter name="collateralFloor">Y</Parameter>
    <Parameter name="dim">Y</Parameter>
    <Parameter name="mva">Y</Parameter>
    <Parameter name="dimQuantile">0.99</Parameter>
    <Parameter name="dimHorizonCalendarDays">14</Parameter>
    <Parameter name="dimRegressionOrder">1</Parameter>
    <Parameter name="dimRegressors">EUR-EURIBOR-3M,USD-LIBOR-3M,USD</Parameter>
    <Parameter name="dimLocalRegressionEvaluations">100</Parameter>
    <Parameter name="dimLocalRegressionBandwidth">0.25</Parameter>
    <Parameter name="dimScaling">1.0</Parameter>
    <Parameter name="dimEvolutionFile">dim_evolution.txt</Parameter>
    <Parameter name="dimRegressionFiles">dim_regression.txt</Parameter>
    <Parameter name="dimOutputNettingSet">CPTY_A</Parameter>      
    <Parameter name="dimOutputGridPoints">0</Parameter>
    <Parameter name="rawCubeOutputFile">rawcube.csv</Parameter>
    <Parameter name="netCubeOutputFile">netcube.csv</Parameter>     
  </Analytic>
</Analytics>
\end{minted}
\caption{ORE analytic: xva}
\label{lst:ore_xva}
\end{listing}

Parameters:
\begin{itemize}
\item {\tt csaFile:} Netting set definitions file covering CSA details such as margining frequency, thresholds, minimum
transfer amounts, margin period of risk
\item {\tt cubeFile:} NPV cube file previously generated and to be post-processed here
\item {\tt hyperCube:} If set to N, the cube file is expected to have depth 1 (storing NPV data only), if set to Y it is
expected to have depth > 1 (e.g. storing NPVs and cumulative flows)
\item {\tt scenarioFile:} Scenario data previously generated and used in the post-processor (simulated index fixings and
FX rates)
\item {\tt baseCurrency:} Expression currency for all NPVs, value adjustments, exposures
\item {\tt exposureProfiles:} Flag to enable/disable exposure output
\item {\tt quantile} Confidence level for Potential Future Exposure (PFE) reporting
\item {\tt calculationType} Determines the settlement of margin calls: Symmetric - margin for both counterparties
settled after the margin period of risk; AsymmetricCVA - margin requested from the counterparty settles with delay,
margin requested from us settles immediately; AsymmetricDVA - vice versa). \todo[inline]{Move calculationType into the
{\tt csaFile}?}
\item {\tt allocationMethod:} XVA allocation method, choices are {\em None, Marginal, RelativeXVA}
\item {\tt marginalAllocationLimit:} The marginal allocation method a la Pykhtin/Rosen breaks down when the netting set
value vanishes while the exposure does not. This parameter acts as a cutoff for the marginal allocation when the
absolute netting set value falls below this limit and switches to equal distribution of the exposure in this case.
\item {\tt exerciseNextBreak:} Flag to terminate all trades at their next break date before aggregation and the
subsequent analytics
\item {\tt cva, dva, fva, colva, collateralFloor, dim, mva:} Flags to enable/disable these analytics. \todo[inline]{Add
collateral rates floor to the collateral model file (netting.xml)}
\item {\tt dvaName:} Credit name to look up the own default probability curve and recovery rate for DVA calculation
\item {\tt fvaBorrowingCurve:} Identifier of the borrowing yield curve
\item {\tt fvaLendingCurve:} Identifier of the lending yield curve
\item {\tt collateralSpread:} Deviation between collateral rate and overnight rate, expressed in absolute terms (one
basis point is 0.0001) assuming the day count convention of the collateral rate. \todo[inline]{Move collateralSpread to
the collateral model file (netting.xml)}
\item {\tt dimQuantile:} Quantile for Dynamic Initial Margin (DIM) calculation
\item {\tt dimHorizonCalendarDays:} Horizon for DIM calculation, 14 calendar days for 2 weeks, etc.
\item {\tt dimRegressionOrder:} Order of the regression polynomial (netting set clean NPV move over the simulation
period versus netting set NPV at period start)
\item {\tt dimRegressors:} Variables used as regressors in a single- or multi-dimensional regression; these variable
  names need to match entries in the {\tt simulation.xml}'s AggregationScenarioDataCurrencies and
  AggregationScenarioDataIndices sections (only these scenario data are passed on to the post processor); if the list is
  empty, the NPV will be used as a single regressor
\item {\tt dimLocalRegressionEvaluations:} Nadaraya-Watson local regression evaluated at the given number of points to
validate polynomial regression. Note that Nadaraya-Watson needs a large number of samples for meaningful
results. Evaluating the local regression at many points (samples) has a significant performance impact, hence the option
here to limit the number of evaluations.
\item {\tt dimLocalRegressionBandwidth:} Nadaraya-Watson local regression bandwidth in standard deviations of the
independent variable (NPV)
\item {\tt dimScaling:} Scaling factor applied to all DIM values used, e.g. to reconcile simulated DIM with actual IM at
$t_0$
\item {\tt dimEvolutionFile:} Output file name to store the evolution of zero order DIM and average of nth order DIM
through time
\item {\tt dimRegressionFiles:} Output file name(s) for a DIM regression snapshot, comma separated list
\item {\tt dimOutputNettingSet:} Netting set for the DIM regression snapshot
\item {\tt dimOutputGridPoints:} Grid point(s) (in time) for the DIM regression snapshot, comma separated list
\item {\tt rawCubeOutputFile:} File name for the trade NPV cube in human readable csv file format (per trade, date,
sample)
\item {\tt netCubeOutputFile:} File name for the aggregated NPV cube in human readable csv file format (per netting set,
date, sample) {\em after} taking collateral into account
\end{itemize}

The latter two cube file outputs are provided for interactive analysis and visualisation purposes, see section
\ref{sec:visualisation}.

%--------------------------------------------------------
\subsection{Market: {\tt todaysmarket.xml}}\label{sec:market}
%--------------------------------------------------------

This configuration file determines the subset of the 'market' universe which is going to be built by ORE. It is the
user's responsibility to make sure that this subset is sufficient to cover the portfolio to be analysed. If it is not,
the application will complain at run time and exit.

\medskip We assume that the market configuration is provided in file {\tt todaysmarket.xml}, however, the file name can
be chosen by the user. The file name needs to be entered into the master configuration file {\tt ore.xml}, see section
\ref{sec:master_input}.

\medskip The file starts and ends with the opening and closing tags {\tt <TodaysMarket>} and {\tt </TodaysMarket>}. The
file then contains configuration blocks for
\begin{itemize}
\item Discounting curves
\item Index curves (to project index fixings)
\item Swap index curves (to project Swap rates)
\item FX spot rates
\item FX Volatility structures
\item Swaption volatility structures
\item Cap/Floor volatility structures
\item Default curves
\end{itemize}

There can be alternative versions of each block each labeled with a unique identifier (e.g. Discount curve block with ID
'default', discount curve block with ID 'ois', another one with ID 'xois', etc). The purpose of these IDs will be
explained at the end of this section. We now discuss each block's layout.

\subsubsection{Discounting Curves} 

We pick one discounting curve block as an example here (see {\tt Examples/Input/todaysmarket.xml}), the one with ID 'ois' 

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <DiscountingCurves Id="ois">
    <DiscountingCurve Currency="EUR">Yield/EUR/EUR1D</DiscountingCurve>
    <DiscountingCurve Currency="USD">Yield/USD/USD1D</DiscountingCurve>
    <DiscountingCurve Currency="GBP">Yield/GBP/GBP1D</DiscountingCurve>
    <DiscountingCurve Currency="CHF">Yield/CHF/CHF6M</DiscountingCurve>
    <DiscountingCurve Currency="JPY">Yield/JPY/JPY6M</DiscountingCurve>
    <!-- ... -->
  </DiscountingCurves>
\end{minted}
\caption{Discount curve block with ID 'ois'}
\label{lst:discountcurve_spec}
\end{listing}

This block instructs ORE to build five discount curves for the indicated currencies. The string within the tags,
e.g. Yield/EUR/EUR1D, uniquely identifies the curve to be built.  Curve Yield/EUR/EUR1D is defined in the curve
configuration file explained in section \ref{sec:curveconfig} below. In this case ORE is instructed to build an Eonia
Swap curve made of Overnight Deposit and Eonia Swap quotes. The right most token of the string Yield/EUR/EUR1D (EUR1D)
is user defined, the first two tokens Yield/EUR have to be used to point to a yield curve in currency EUR.
 
\subsubsection{Index Curves} 

See an excerpt of the index curve block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<IndexForwardingCurves Id="default">
  <Index Name="EUR-EURIBOR-3M">Yield/EUR/EUR3M</Index>
  <Index Name="EUR-EURIBOR-6M">Yield/EUR/EUR6M</Index>
  <Index Name="EUR-EURIBOR-12M">Yield/EUR/EUR6M</Index>
  <Index Name="EUR-EONIA">Yield/EUR/EUR1D</Index>
  <Index Name="USD-LIBOR-3M">Yield/USD/USD3M</Index>
  <!-- ... -->
</IndexForwardingCurves>
\end{minted}
\caption{Index curve block with ID 'default'}
\label{lst:indexcurve_spec}
\end{listing}

This block of curve specifications instructs ORE to build another set of yield curves, unique strings
(e.g. Yield/EUR/EUR6M etc.) point to the {\tt curveconfig.xml} file where these curves are defined. Each curve is then
associated with an index name (of format Ccy-IndexName-Tenor, e.g. EUR-EURIBOR-6M) so that ORE will project the
respective index using the selected curve (e.g. Yield/EUR/EUR6M).

\subsubsection{Swap Index Curves}

The following is an excerpt of the swap index curve block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<SwapIndexCurves Id="default">
  <SwapIndex Name="EUR-CMS-1Y">
    <Index>EUR-EURIBOR-6M</Index>
    <Discounting>EUR-EONIA</Discounting>
  </SwapIndex>
  <SwapIndex Name="EUR-CMS-30Y">
    <Index>EUR-EURIBOR-6M</Index>
    <Discounting>EUR-EONIA</Discounting>
  </SwapIndex>
  <!-- ... -->
</SwapIndexCurves>
\end{minted}
\caption{Swap index curve block with ID 'default'}
\label{lst:swapindexcurve_spec}
\end{listing}

These instructions do not build any additional curves. They only build the respective swap index objects and associate
them with the required index forwarding and discounting curves already built above. This enables a swap index to project
the fair rate of forward starting Swaps. Swap indices are also containers for conventions. Swaption volatility surfaces
require two swap indices each available in the market object, a long term and a short term swap index. The curve
configuration file below will show that in particular the required short term index has term 1Y, and the required long
term index has 30Y term. This is why we build these two indices at this point.

\subsubsection{FX Spot}

The following is an excerpt of the FX spot block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<FxSpots Id="default">
  <FxSpot Pair="EURUSD">FX/EUR/USD</FxSpot>
  <FxSpot Pair="EURGBP">FX/EUR/GBP</FxSpot>
  <FxSpot Pair="EURCHF">FX/EUR/CHF</FxSpot>
  <FxSpot Pair="EURJPY">FX/EUR/JPY</FxSpot>
  <!-- ... -->
</FxSpots>
\end{minted}
\caption{FX spot block with ID 'default'}
\label{lst:fxspot_spec}
\end{listing}

This block instructs ORE to provide four FX quote objects in the market object, all quoted with target currency EUR so
that foreign currency amounts can be converted into EUR via multiplication with that rate.
 
\subsubsection{FX Volatilities}

The following is an excerpt of the FX Volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<FxVolatilities Id="default">
  <FxVolatility Pair="EURUSD">FXVolatility/EUR/USD/EURUSD</FxVolatility>
  <FxVolatility Pair="EURGBP">FXVolatility/EUR/GBP/EURGBP</FxVolatility>
  <FxVolatility Pair="EURCHF">FXVolatility/EUR/CHF/EURCHF</FxVolatility>
  <FxVolatility Pair="EURJPY">FXVolatility/EUR/JPY/EURJPY</FxVolatility>
  <!-- ... -->
</FxVolatilities>
\end{minted}
\caption{FX volatility block with ID 'default'}
\label{lst:fxvol_spec}
\end{listing}

This instructs ORE to build four FX volatility structures for all FX pairs with target currency EUR, see curve
configuration file for the definition of the volatility structure.

\subsubsection{Swaption Volatilities}

The following is an excerpt of the Swaption Volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<SwaptionVolatilities Id="default">
  <SwaptionVolatility Currency="EUR">SwaptionVolatility/EUR/EUR_SW_N</SwaptionVolatility>
  <SwaptionVolatility Currency="USD">SwaptionVolatility/USD/USD_SW_N</SwaptionVolatility>
  <SwaptionVolatility Currency="GBP">SwaptionVolatility/GBP/GBP_SW_N</SwaptionVolatility>
  <SwaptionVolatility Currency="CHF">SwaptionVolatility/CHF/CHF_SW_N</SwaptionVolatility>
  <SwaptionVolatility Currency="JPY">SwaptionVolatility/CHF/JPY_SW_N</SwaptionVolatility>
</SwaptionVolatilities>
\end{minted}
\caption{Swaption volatility block with ID 'default'}
\label{lst:swaptionvol_spec}
\end{listing}

This instructs ORE to build five Swaption volatility structures, see the curve configuration file for the definition of
the volatility structure. The latter token (e.g. EUR\_SW\_N) is user defined and will be found in the curve
configuration's CurveId tag.

\subsubsection{Cap/Floor Volatilities}

The following is an excerpt of the Cap/Floor Volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CapFloorVolatilities id="default">
  <CapFloorVolatility currency="EUR">CapFloorVolatility/EUR/EUR_CF_N</CapFloorVolatility>
  <CapFloorVolatility currency="USD">CapFloorVolatility/USD/USD_CF_N</CapFloorVolatility>
  <CapFloorVolatility currency="GBP">CapFloorVolatility/GBP/GBP_CF_N</CapFloorVolatility>
</CapFloorVolatilities>
\end{minted}
\caption{Cap/Floor volatility block with ID 'default'}
\label{lst:capfloorvol_spec}
\end{listing}

This instructs ORE to build three Cap/Floor volatility structures, see the curve configuration file for the definition
of the volatility structure. The latter token (e.g. EUR\_CF\_N) is user defined and will be found in the curve
configuration's CurveId tag.

\subsubsection{Default Curves}

The following is an excerpt of the Default Curves block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<DefaultCurves Id="default">
  <DefaultCurve Name="BANK">Default/USD/BANK_SR_USD</DefaultCurve>
  <DefaultCurve Name="CPTY_A">Default/USD/CUST_A_SR_USD</DefaultCurve>
  <DefaultCurve Name="CPTY_B">Default/USD/CUST_A_SR_USD</DefaultCurve>
  <!-- ... -->
</DefaultCurves>
\end{minted}
\caption{Default curves block with ID 'default'}
\label{lst:defaultcurve_spec}
\end{listing}


This instructs ORE to build a set of default probability curves, again defined in the curve configuration file. Each
curve is then associated with a name (BANK, CUST\_A) for subsequent lookup.  As before, the last token
(e.g. BANK\_SR\_USD) is user defined and will be found in the curve configuration's CurveId tag.

\subsubsection{Market Configurations}

Finally, representatives of each type of block (Discount Curves, Index Curves, Volatility structures, etc, up to Default
curves) can be bundled into a market configuration. This is done by adding the following to the {\tt todaysmarket.xml}
file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Configuration Id="default">
  <DiscountingCurvesId> xois_eur </DiscountingCurvesId>
</Configuration>
<Configuration Id="collateral_inccy">
  <DiscountingCurvesId>ois</DiscountingCurvesId>
</Configuration>
<Configuration Id="collateral_eur">
  <DiscountingCurvesId>xois_eur</DiscountingCurvesId>
</Configuration>
<Configuration Id="libor">
  <DiscountingCurvesId>inccy_swap</DiscountingCurvesId>
</Configuration>
\end{minted}
\caption{Market configurations}
\label{lst:config_spec}
\end{listing}

When ORE constructs the market object, all market configurations will be build and labelled using the 'Configuration
Id'.  This allows configuring a market setup for different alternative purposes side by side in the same {\tt
  todaysmarket.xml} file. Typical use cases are
\begin{itemize}
\item different discount curves needed for model calibration and risk factor evolution, respectively
\item different discount curves needed for collateralised and uncollateralised derivatives pricing.
\end{itemize}
The former is actually used throughout the {\tt Examples} section. Each master input file {\tt ore.xml} has a Markets
section (see \ref{sec:master_input}) where four market configuration IDs have to be provided - the ones used for
'lgmcalibration', 'fxcalibration', 'pricing' and 'simulation' (i.e. risk factor evolution).

\medskip The configuration ID concept extends across all curve and volatility objects though currently used only to
distinguish discounting.
 
%--------------------------------------------------------
\subsection{Pricing Engines: {\tt pricingengine.xml}}
%--------------------------------------------------------

The pricing engine configuration file is provided to select pricing models and pricing engines by product type. The
following excerpt of the Example section's {\tt pricingengine.xml} shows the selection for Bermudan Swaption pricing:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<PricingEngines>
  <Product type="Swap">
    <Model>DiscountedCashflows</Model>
    <ModelParameters/>
    <Engine>DiscountingSwapEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="CrossCurrencySwap">
    <Model>DiscountedCashflows</Model>
    <ModelParameters/>
    <Engine>DiscountingCrossCurrencySwapEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="FxForward">
    <Model>DiscountedCashflows</Model>
    <ModelParameters/>
    <Engine>DiscountingFxForwardEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="FxOption">
    <Model>GarmanKohlhagen</Model>
    <ModelParameters/>
    <Engine>AnalyticEuropeanEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="EuropeanSwaption">
    <Model>BlackBachelier</Model> <!-- depends on input vol -->
    <ModelParameters/>
    <Engine>BlackBachelierSwaptionEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="BermudanSwaption">
    <Model>LGM</Model>
    <ModelParameters>
      <Parameter name="Calibration">Bootstrap</Parameter>
      <Parameter name="BermudanStrategy">CoterminalATM</Parameter>
      <Parameter name="Reversion">0.03</Parameter>
      <Parameter name="ReversionType">HullWhite</Parameter>
      <Parameter name="Volatility">0.01</Parameter>
      <Parameter name="VolatilityType">Hagan</Parameter>
      <Parameter name="Tolerance">0.0001</Parameter>
    </ModelParameters>
    <Engine>Grid</Engine>
    <EngineParameters>
      <Parameter name="sy">3.0</Parameter>
      <Parameter name="ny">10</Parameter>
      <Parameter name="sx">3.0</Parameter>
      <Parameter name="nx">10</Parameter>
    </EngineParameters>
  </Product>
</PricingEngines>
\end{minted}
\caption{Pricing engine configuration}
\label{lst:pricingengine_config}
\end{listing}

These settings will be taken into account when the engine factory is asked to build a Bermudan Swaption
pricing model, calibrate it and construct the pricing engine for it:

\begin{itemize}
\item The only model currently supported for Bermudan Swaption pricing is the LGM selected here. 

\item The first block of model parameters then provides initial values for the model (Reversion, Volatility) and chooses
  the parametrisation of the LGM model with ReversionType and VolatilityType choices {\em HullWhite} and {\em
    Hagan}. Calibration and BermudanStrategy can be set to {\em None} in order to skip model calibration. Alternatively,
  Calibration is set to {\em Bootstrap} and BermudanStrategy to {\em CoterminalATM} in order to calibrate to
  instrument-specific co-terminal ATM Swaptions, i.e. chosen to match the instruments first expiry and final maturity.

\item The second block of engine parameters specifies the Numerical Swaption engine parameters which determine the
  number of standard deviations covered in the probability density integrals (sy and sx), and the number of grid points
  used per standard deviation (ny and nx).
\end{itemize}

This file is relevant in particular for structured products which are in scope of future ORE releases. But it is also
intended to allow the selection of optimised pricing engines for vanilla products such as Interest Rate Swaps.
 
%--------------------------------------------------------
\subsection{Simulation: {\tt simulation.xml}}
%--------------------------------------------------------

This file determines the behaviour of the risk factor simulation (scenario generation) module.
It is structured in three blocks of data.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Simulation>
  <Parameters> ... </Parameters>
  <CrossAssetModel> ... </CrossAssetModel>
  <Market> ... </Market>
</Simulation>
\end{minted}
\caption{Simulation configuration}
\label{lst:simulation_configuration}
\end{listing}

Each of the three blocks is sketched in the following.

\subsubsection{Parameters}\label{sec:sim_params}

Let us discuss this section using the following example

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Parameters>
  <Discretization>Exact</Discretization>
  <Grid>80,3M</Grid>
  <Calendar>EUR,USD,GBP,CHF</Calendar>
  <Sequence>SobolBrownianBridge</Sequence>
  <Seed>42</Seed>
  <Samples>1000</Samples>
</Parameters>
\end{minted}
\caption{Simulation configuration}
\label{lst:simulation_params_configuration}
\end{listing}

\begin{itemize}
\item {\tt Discretization:} Chooses between time discretization schemes for the risk factor evolution. {\em Exact} means
exploiting the analytcal tractability of the model to avoid any time discretization error. {\em Euler} uses a naive time
discretization scheme which has numerical error and requires small time steps for accurate results (useful for testing
purposes)
\item {\tt Grid: }Specifies the simulation time grid, here 80 quarterly steps.
\item {\tt Calendar:} Calendar or combination of calendars used to adjust the dates of the grid. Date adjustment is
required because the simulation must step over 'good' dates on which index fixings can be stored.
%\item {\tt Scenario: } Choose between {\em Simple } and {\em Complex } implementations, the latter optimized for
more efficient memory usage. \todo[inline]{Remove Scenario choice}
\item {\tt Sequence:} Choose random sequence generator ({\em MersenneTwister, MersenneTwisterAntithetic, Sobol,
SobolBrownianBridge}).
\item {\tt Seed:} Random number generator seed
\item {\tt Samples:} Number of Monte Carlo paths to be produced
%\item {\tt Fixings: } Choose whether fixings should be simulated or not, and if so which fixing simulation method to
use ({\em Backward, Forward, BestOfForwardBackward, InterpolatedForwardBackward}), which number of forward horizon days
to use if one of the {\em Forward } related methods is chosen.
\end{itemize}

\subsubsection{Model}\label{sec:sim_model}

The {\tt CrossAssetModel} section determines the cross asset model's number of currencies covered, composition, and each
component's calibration. It is currently made of a sequence of LGM models for each currency (say $n$ currencies), $n-1$
FX models for each exchange rate to the base currency, and finally, a specification of the correlation structure between
all components.

\medskip The simulated currencies are specified as follows, with clearly identifying the domestic currency which is also
the target currency for all FX models listed subsequently. If the portfolio requires more currencies to be simulated,
this will lead to an exception at run time, so that it is the user's responsibility to make sure that the list of
currencies here is sufficient. The list can be larger than actually required by the portfolio. This will not lead to any
exceptions, but add to the run time of ORE.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>
  <DomesticCcy>EUR</DomesticCcy>
  <Currencies>
    <Currency>EUR</Currency>
    <Currency>USD</Currency>
    <Currency>GBP</Currency>
    <Currency>CHF</Currency>
    <Currency>JPY</Currency>
  </Currencies>
  <BootstrapTolerance>0.0001</BootstrapTolerance>
  <!-- ... -->
</CrossAssetModel>
\end{minted}
\caption{Simulation model currencies configuration}
\label{lst:simulation_model_currencies_configuration}
\end{listing}
 
Bootstrap tolerance is a global parameter that applies to the calibration of all model components. If the calibration
error of any component exceeds this tolerance, this will trigger an exception at runtime, early in the ORE process.

\medskip

Each interest rate model is specified by a block as follows

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <InterestRateModels>
    <LGM ccy="default">
      <CalibrationType>Bootstrap</CalibrationType>
      <Volatility>
        <Calibrate>Y</Calibrate>
        <VolatilityType>Hagan</VolatilityType>
        <ParamType>Piecewise</ParamType>
        <TimeGrid>1.0,2.0,3.0,4.0,5.0,7.0,10.0</TimeGrid>
        <InitialValue>0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01<InitialValue>
      </Volatility>
      <Reversion>
        <Calibrate>N</Calibrate>
        <ReversionType>HullWhite</ReversionType>
        <ParamType>Constant</ParamType>
        <TimeGrid/>
        <InitialValue>0.03</InitialValue>
      </Reversion>
      <CalibrationSwaptions>
        <Expiries>1Y,2Y,4Y,6Y,8Y,10Y,12Y,14Y,16Y,18Y,19Y</Expiries>
        <Terms>19Y,18Y,16Y,14Y,12Y,10Y,8Y,6Y,4Y,2Y,1Y</Terms>
        <Strikes/>
      </CalibrationSwaptions>
      <ParameterTransformation>
        <ShiftHorizon>0.0</ShiftHorizon>
        <Scaling>1.0</Scaling>
      </ParameterTransformation>
    </LGM>
    <LGM ccy="EUR">
      <!-- ... -->
    </LGM>
    <LGM ccy="USD">
      <!-- ... -->
    </LGM>
  </InterestRateModels>	
  <!-- ... -->		
</CrossAssetModel>
\end{minted}
\caption{Simulation model IR configuration}
\label{lst:simulation_model_ir_configuration}
\end{listing}

We have LGM sections by currency, but starting with a section for currency 'default'. As the name implies, this is used
as default configuration for any currency in the currency list for which we do not provide an explicit
parametrisation. Within each LGM section, the interpretation of elements is as follows:

\begin{itemize}
\item {\tt CalibrationType: } Choose between {\em Bootstrap} and {\em BestFit}, where Bootstrap is chosen when we expect
to be able to achieve a perfect fit (as with calibration of piecewise volatility to a series of co-terminal Swaptions)
\item {\tt Volatility/Calibrate: } Flag to enable/disable calibration of this particular parameter
\item {\tt Volatility/VolatilityType: } Choose volatility parametrisation a la {\em HullWhite} or {\em Hagan}
\item {\tt Volatility/ParamType: } Choose between {\em Constant} and {\em Piecewise}
\item {\tt Volatility/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
\item {\tt Volatility/InitialValue: } Vector of initial values, matching number of entries in time, or single value if
the time grid is empty
\item {\tt Reversion/Calibrate: } Flag to enable/disable calibration of this particular parameter
\item {\tt Reversion/VolatilityType: } Choose reversion parametrisation a la {\em HullWhite} or {\em Hagan}
\item {\tt Reversion/ParamType: } Choose between {\em Constant} and {\em Piecewise}
\item {\tt Reversion/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
\item {\tt Reversion/InitialValue: } Vector of initial values, matching number of entries in time, or single value if
the time grid is empty
\item {\tt CalibrationSwaptions: } Choice of calibration instruments by expiry, underlying Swap term and strike
\item {\tt ParameterTransformation: } LGM model prices are invariant under scaling and shift transformations
\cite{Lichters} with advantages for numerical convergence of results in long term simulations. These transformations can
be chosen here. Default settings are shiftHorizon 0 (time in years) and scaling factor 1.
\end{itemize}

\medskip

Each FX model is specified by a block as follows

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <ForeignExchangeModels>
    <CrossCcyLGM foreignCcy="default">
      <DomesticCcy>EUR</DomesticCcy>
      <CalibrationType>Bootstrap</CalibrationType>
      <Sigma>
        <Calibrate>Y</Calibrate>
        <ParamType>Piecewise</ParamType>
        <TimeGrid>1.0,2.0,3.0,4.0,5.0,7.0,10.0</TimeGrid>
        <InitialValue>0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1</InitialValue>
      </Sigma>
      <CalibrationOptions>
        <Expiries>1Y,2Y,3Y,4Y,5Y,10Y</Expiries>
        <Strikes/>
      </CalibrationOptions>
    </CrossCcyLGM>
    <CrossCcyLGM foreignCcy="USD">
      <!-- ... -->
    </CrossCcyLGM>
    <CrossCcyLGM foreignCcy="GBP">
      <!-- ... -->
    </CrossCcyLGM>
    <!-- ... -->
  </ForeignExchangeModels>
  <!-- ... -->
<CrossAssetModel>	
\end{minted}
\caption{Simulation model FX configuration}
\label{lst:simulation_model_fx_configuration}
\end{listing}

CrossCcyLGM sections are defined by foreign currency, but we also support a default configuration as above for the IR
model parametrisations.  Within each CrossCcyLGM section, the interpretation of elements is as follows:

\begin{itemize}
\item {\tt DomesticCcy: } Domestic currency completing the FX pair
\item {\tt CalibrationType: } Choose between {\em Bootstrap} and {\em BestFit} as in the IR section
\item {\tt Sigma/Calibrate: } Flag to enable/disable calibration of this particular parameter
\item {\tt Sigma/ParamType: } Choose between {\em Constant} and {\em Piecewise}
\item {\tt Sigma/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
\item {\tt Sigma/InitialValue: } Vector of initial values, matching number of entries in time, or single value if the
time grid is empty
\item {\tt CalibrationOptions: } Choice of calibration instruments by expiry and strike, strikes can be empty (implying
the default, ATMF options), or explicitly specified (in terms of FX rates as absolute strike values, in delta notation
such as $\pm 25D$, $ATMF$ for at the money)
\end{itemize}

\medskip
Finally, the instantaneous correlation structure is specified as follows.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <InstantaneousCorrelations>
    <Correlation factor1="IR:EUR" factor2="IR:USD">0.3</Correlation>
    <Correlation factor1="IR:EUR" factor2="IR:GBP">0.3</Correlation>
    <Correlation factor1="IR:USD" factor2="IR:GBP">0.3</Correlation>
    <Correlation factor1="IR:EUR" factor2="FX:USDEUR">0</Correlation>
    <Correlation factor1="IR:EUR" factor2="FX:GBPEUR">0</Correlation>
    <Correlation factor1="IR:GBP" factor2="FX:USDEUR">0</Correlation>
    <Correlation factor1="IR:GBP" factor2="FX:GBPEUR">0</Correlation>
    <Correlation factor1="IR:USD" factor2="FX:USDEUR">0</Correlation>
    <Correlation factor1="IR:USD" factor2="FX:GBPEUR">0</Correlation>
    <Correlation factor1="FX:USDEUR" factor2="FX:GBPEUR">0</Correlation>
    <!-- ... --> 
  </InstantaneousCorrelations>
</CrossAssetModel>
\end{minted}
\caption{Simulation model correlation configuration}
\label{lst:simulation_model_correlation_configuration}
\end{listing}

Any risk factor pair not specified explicitly here will be assumed to have zero correlation.

\subsubsection{Market}\label{sec:sim_market}

The last part of the simulation configuration file covers the specification of the simulated market.  Note that the
simulation model will yield the evolution of risk factors such as short rates which need to be translated into entire
yield curves that can be 'understood' by the instruments which we want to price under scenarios.  Moreover we need to
specify how volatility structures evolve even if we do not explicitly simulate volatility. This translation happens
based on the information in the {\em simulation market} object, which is configured in the section within the enclosing
tags {\tt <Market>} and {\tt </Market}, as shown in the following small two-currency example.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Market>
  <BaseCurrency>EUR</BaseCurrency>
  <Currencies>
    <Currency>EUR</Currency>
    <Currency>USD</Currency>
  </Currencies>
  <YieldCurves>
    <Configuration>
      <Tenors>3M,6M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,12Y,15Y,20Y</Tenors>
      <Interpolation>LogLinear</Interpolation>
      <Extrapolation>Y</Extrapolation>
    </Configuration>
  </YieldCurves>
  <Indices>
    <Index>EUR-EURIBOR-6M</Index>
    <Index>EUR-EURIBOR-3M</Index>
    <Index>EUR-EONIA</Index>
    <Index>USD-LIBOR-3M</Index>
  </Indices>
  <SwapIndices>
    <SwapIndex>
      <Name>EUR-CMS-1Y</Name>
      <ForwardingIndex>EUR-EURIBOR-6M</ForwardingIndex>
      <DiscountingIndex>EUR-EONIA</DiscountingIndex>
    </SwapIndex>
  </SwapIndices>
  <SwaptionVolatilities>
    <ReactionToTimeDecay>ForwardVariance</ReactionToTimeDecay>
    <Currencies>
      <Currency>EUR</Currency>
      <Currency>USD</Currency>
    </Currencies>
    <Expiries>6M,1Y,2Y,3Y,5Y,10Y,12Y,15Y,20Y</Expiries>
    <Terms>1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,30Y</Terms>
    <Strikes/>
  </SwaptionVolatilities> 
  <CapFloorVolatilities>
    <ReactionToTimeDecay>ConstantVariance</ReactionToTimeDecay>
    <Currencies>
      <Currency>EUR</Currency>
      <Currency>USD</Currency>
    </Currencies>
  </CapFloorVolatilities>
  <FxVolatilities>
    <ReactionToTimeDecay>ForwardVariance</ReactionToTimeDecay>
    <CurrencyPairs>
      <CurrencyPair>EURUSD</CurrencyPair>
    </CurrencyPairs>
    <Expiries>6M,1Y,2Y,3Y,4Y,5Y,7Y,10Y</Expiries>
    <Strikes/>
  </FxVolatilities>
  <AdditionalScenarioDataCurrencies>
    <Currency>EUR</Currency>
    <Currency>USD</Currency>
  </AdditionalScenarioDataCurrencies>
  <AdditionalScenarioDataIndices>
    <Index>EUR-EURIBOR-3M</Index>
    <Index>EUR-EONIA</Index>
    <Index>USD-LIBOR-3M</Index>
  </AdditionalScenarioDataIndices>
</Market>
\end{minted}
\caption{Simulation market configuration}
\label{lst:simulation_market_configuration}
\end{listing}

\todo[inline]{Comment on cap/floor surface structure and reaction to time decay}
\todo[inline]{Remove DefaultCurves from SimMarket constructor and configuration section}

%--------------------------------------------------------
\subsection{Curves: {\tt curveconfig.xml}}\label{sec:curveconfig}
%--------------------------------------------------------

The configuration of various term structures required to price a portfolio is covered in a single configuration file
which we will label {\tt curveconfig.xml} in the following though the file name can be chosen by the user. This
configuration determines the composition of yield curves, default curves, Swaption and FX Option volatility structures.
 
\include{yieldcurves}

\subsubsection{Default Curves}

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<DefaultCurves>
  <DefaultCurve>
    <CurveId>BANK_SR_USD</CurveId>
    <CurveDescription>BANK SR CDS USD</CurveDescription>
    <Currency>USD</Currency> 
    <Type>SpreadCDS</Type>
    <DiscountCurve>Yield/USD/USD3M</DiscountCurve>
    <DayCounter>A365</DayCounter>
    <RecoveryRate>RECOVERY_RATE/RATE/BANK/SR/USD</RecoveryRate>
    <Quotes>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/1Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/2Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/3Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/4Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/5Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/7Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/10Y</Quote>
    </Quotes>
    <Conventions>CDS-STANDARD-CONVENTIONS</Conventions>
  </DefaultCurve>
  <DefaultCurve>
    <!-- ... -->
  </DefaultCurve>
</DefaultCurves>
\end{minted}
\caption{Default curve configuration}
\label{lst:defaultcurve_configuration}
\end{listing}

\subsubsection{Swaption Volatility Structures}

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<SwaptionVolatilities>    
  <SwaptionVolatility>
    <CurveId>EUR_SW_N</CurveId>
    <CurveDescription>EUR normal swaption volatilities</CurveDescription>
    <!-- ATM (Smile not yet supported) -->
    <Dimension>ATM</Dimension>
    <!-- Normal or Lognormal or ShiftedLognormal -->
    <VolatilityType>Normal</VolatilityType>
    <!-- Flat or Linear -->
    <Extrapolation>Flat</Extrapolation>
    <!-- Day counter for date to time conversion -->
    <DayCounter>Actual/365 (Fixed)</DayCounter>
    <!--Calendar and Business day convention for option tenor to date conversion -->
    <Calendar>TARGET</Calendar>
    <BusinessDayConvention>Following</BusinessDayConvention>
    <OptionTenors>
      1M,3M,6M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y
    </OptionTenors>
    <SwapTenors>
      1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y
    </SwapTenors>
    <ShortSwapIndexBase>EUR-CMS-1Y</ShortSwapIndexBase>
    <SwapIndexBase>EUR-CMS-30Y</SwapIndexBase>
  </SwaptionVolatility>
    <SwaptionVolatility>
      <!-- ... -->
    </SwaptionVolatility>
</SwaptionVolatilities>
\end{minted}
\caption{Swaption volatility configuration}
\label{lst:swaptionvol_configuration}
\end{listing}

\subsubsection{FX Volatility Structures}

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<FXVolatilities>
  <FXVolatility>
    <CurveId>EURUSD</CurveId>
    <CurveDescription />
    <Dimension>ATM</Dimension>
    <Expiries>
      1M,3M,6M,1Y,2Y,3Y,10Y
    </Expiries>
  </FXVolatility>
  <FXVolatility>
    <!-- ... -->
  </FXVolatility>
</FXVolatilities>
\end{minted}
\caption{FX option volatility configuration}
\label{lst:fxoptionvol_configuration}
\end{listing}


%--------------------------------------------------------
%\subsection{Conventions: {\tt conventions.xml}}
%\label{sec:conventions}
%--------------------------------------------------------
\include{conventions}


%========================================================
%\section{Trade Data}\label{sec:portfolio_data}
%========================================================
\include{portfoliodata}

%========================================================
%\section{Netting Set Definitions}\label{sec:nettingsetinput}
%========================================================
\include{nettingdata}

%========================================================
%\section{Market Data}\label{sec:market_data}
%========================================================
\include{marketdata}

%========================================================
%\section{Fixing History}
%========================================================
\include{fixingdata}

\begin{appendix}

%========================================================
\section{Methodology Summary}
%========================================================

\subsection{Risk Factor Evolution Model}\label{sec:app_rfe}

ORE applies the cross asset model described in detail in \cite{Lichters} to evolve  the market through time. So far the
evolution model in ORE is limited to IR and FX risk factors for any number of currencies, extensions to further risk
factor classes (Inflation, Credit, Equity, Commodity) will follow. \\

The Cross Asset Model is based on the Linear Gauss Markov model (LGM) for interest rates, lognormal FX processes and the
Dodgson-Kainth model for inflation. We identify a single {\em domestic} currency; its LGM process, which is labelled
$z_0$; and a set of $n$ foreign currencies with associated LGM processes that are labelled $z_i$, $i=1,\dots,n$.
Following \cite{Lichters}, 13.27 - 13.29 we write the inflation processes in the domestic LGM measure with state
variables $z_{I,k}$ and $y_{I,k}$ for $k=1,\ldots,K$.  If we consider $n$ foreign exchange rates for converting foreign
currency amounts into the single domestic currency by multiplication, $x_i$, $i=1,\dots,n$, then the cross asset model
is given by the system of SDEs
\begin{eqnarray*}
dz_0 &=& \alpha_0\,dW_0^z \\
dz_i &=& \gamma_i\,dt + \alpha_i\,dW_i^z,  \qquad i>0 \\
  \frac{d x_i}{x_i} &=& \mu_i\, dt + \sigma_i\,dW_i^x, \qquad i > 0 \\
  dz_{I,k} &=& \alpha_{I,k}(t)dW_k^I \\
  dy_{I,k} &=& \alpha_{I,k}(t)H_{I,k}(t)dW_k^I \\ \\
\gamma_i &=&
-\alpha_i^2\,H_i -\rho_{ii}^{zx}\,\sigma_i\,\alpha_i + \rho_{i0}^{zz}\,\alpha_i\,\alpha_0\,H_0\\
\mu_i &=& r_0 - r_i + \rho_{0i}^{zx}\,\alpha_0\,H_0\,\sigma_i\\
r_i &=& f_i(0,t) + z_i(t)\,H'_i(t) + \zeta_i(t)\,H_i(t)\,H'_i(t),
\quad \zeta_i(t) = \int_0^t \alpha_i^2(s)\,ds  \\ \\
dW^\alpha_a\,dW^\beta_b &=& \rho^{\alpha\beta}_{ij}\,dt, \qquad \alpha, \beta \in \{z, x, I\}, \qquad a, b \text{
                              suitable indices }
%\zeta_i(t) &=& \int_0^t \alpha_i^2(s)\,ds,
%\qquad H_i(t) = \int_0^t e^{-\beta_i(s)} \,ds \\
%\beta_i(t) &=& \int_0^t \lambda_i(s)\,ds,
%\qquad \alpha_i(t) = \sigma_i^{HW}(t)\,e^{\beta(t)} \\
\end{eqnarray*}
where we have dropped time dependencies for readability, and $f_i(0,t)$ is the instantaneous forward curve in currency $i$.

\medskip Parameters $H_i(t)$ and $\alpha_i(t)$ (or alternatively $\zeta_i(t)$) are LGM model parameters which determine,
together with the stochastic factor $z_i(t)$, the evolution of numeraire and zero bond prices in the LGM model:
\begin{align}
N(t) &= \frac{1}{P(0,t)}\exp\left\{H_t\, z_t + \frac{1}{2}H^2_t\,\zeta_t \right\}
\label{lgm1f_numeraire} \\
P(t,T,z_t)
&= \frac{P(0,T)}{P(0,t)}\:\exp\left\{ -(H_T-H_t)\,z_t - \frac{1}{2} \left(H^2_T-H^2_t\right)\,\zeta_t\right\}.
\label{lgm1f_zerobond}
\end{align}

Note that the LGM model is closely related to the Hull-White model in T-forward measure \cite{Lichters}.

\medskip The parameters $H_{I,k}(t)$ and $\alpha_{I,k}(t)$ determine together with the factors $z_{I,k}(t), y_{I,k}(t)$
the evolution of the spot Index $I(t)$ and the forward index $\hat{I}(t,T) = P_I(t,T) / P_n(t,T)$ defined as the ratio
of the inflation linked zero bond and the nominal zero bond,

\begin{eqnarray*}
  \hat{I}(t,T) &=& \frac{\hat{I}(0,T)}{\hat{I}(0,t)} e^{(H_{I,k}(T)-H_{I,k}(t))z_{I,k}(t)+\tilde{V}(t,T)} \\
  I(t) &=& I(0) \hat{I}(0,t)e^{H_{I,k}(t)z_{I,k}(t)-y_{I,k}(t)-V(0,t)}
\end{eqnarray*}

with, in case of domestic currency inflation,

\begin{eqnarray*}
  V(t,T) &=& \frac{1}{2} \int_t^T (H_{I,k}(T)-H_{I,k}(s))^2 \alpha_{I,k}^2(s) ds \\
         & & - \rho^{zI}_{0,k} H_0(T) \int_t^T (H_{I,k}(t)-H_{I,k}(s))\alpha_0(s)\alpha_{I,k}(s)ds \\
  \tilde{V}(t,T) &=& V(t,T) - V(0,T) -V(0,t) \\
         &=& -\frac{1}{2}(H_{I,k}^2(T)-H_{I,k}^2(t))\zeta_{I,k}(t,0) \\
         & & +(H_{I,k}(T)-H_{I,k}(t)) \zeta_{I,k}(t,1) \\
         & & +(H_0(T)H_{I,k}(T) - H_0(t)H_{I,k}(t))\zeta_{0I}(t,0) \\
         & & -(H_0(T)-H_0(t))\zeta_{0I}(t,1) \\
  V(0,t) &=& \frac{1}{2}H_{I,k}^2(t)\zeta_{I,k}(t,0)-H_{I,k}(t)\zeta_{I,k}(t,1)+\frac{1}{2}\zeta_{I,k}(t,2) \\
         & & -H_0(t)H_{I,k}(t)\zeta_{0I}(t,0)+H_0(t)\zeta_{0I}(t,1) \\
  \zeta_{I,k}(t,k) &=& \int_0^t H_{I,k}^k(s)\alpha_{I,k}^2(s) ds \\
  \zeta_{0I}(t,k) &=& \rho^{zI}_{0,k}\int_0^t H_{I,k}^k(t) \alpha_0(s) \alpha_{I,k}(s) ds
\end{eqnarray*}

and for foreign currency inflation in currency $i>0$, with

\begin{eqnarray*}
  \tilde{V}(t,T) &=& V(t,T) -V(0,T) + V(0,T)
\end{eqnarray*}

and

\begin{eqnarray*}
  V(t,T) &=& \frac{1}{2}\int_t^T (H_{I,k}(T)-H_{I,k}(s))^2 \alpha_{I,k}(s) ds \\
  & & -\rho^{zI}_{0,k} \int_t^T H_0(s)\alpha_0(s)(H_{I,k}(T)-H_{I,k}(s)\alpha_{I,k}(s)) ds \\
  & & -\rho^{zI}_{i,k} \int_t^T (H_i(T)-H_i(s))\alpha_i(s)(H_{I,k}(T)-H_{I,k}(s))\alpha_{I,k}(s) ds \\
  & & +\rho^{xI}_{i,k} \int_t^T \sigma_i(s)(H_{I,k}(T)-H_{I,k}(s))\alpha_{I,k}(s) ds
\end{eqnarray*}

\subsection{Analytical Moments of the Risk Factor Evolution Model}\label{sec:app_analytical_moments}

We follow \cite{Lichters}, chapter 16. The expectation of the interest rate process $z_i$ conditional on $\mathcal{F}_{t_0}$ at $t_0+\Delta t$ is

\begin{eqnarray*}
  \mathbb{E}_{t_0}[z_i(t_0+\Delta t)] &=& z_i(t_0) + \mathbb{E}_{t_0}[\Delta z_i],
  \qquad\mbox{with}\quad \Delta z_i = z_i(t_0+\Delta t) - z_i(t_0) \\
  &=& z_i(t_0) -\int_{t_0}^{t_0+\Delta t} H^z_i\,(\alpha^z_i)^2\,du + \rho^{zz}_{0i} \int_{t_0}^{t_0+\Delta t}
  H^z_0\,\alpha^z_0\,\alpha^z_i\,du \\
  & & - \epsilon_i  \rho^{zx}_{ii}\int_{t_0}^{t_0+\Delta t} \sigma_i^x\,\alpha^z_i\,du
\end{eqnarray*}

where $\epsilon_i$ is zero for $i=0$ (domestic currency) and one otherwise.

The expectation of the FX process $x_i$ conditional on $\mathcal{F}_{t_0}$ at $t_0+\Delta t$ is

\begin{eqnarray*}
  \mathbb{E}_{t_0}[\ln x_i(t_0+\Delta t)] &=& \ln x_i(t_0) +  \mathbb{E}_{t_0}[\Delta \ln x_i],
  \qquad\mbox{with}\quad \Delta \ln x_i = \ln x_i(t_0+\Delta t) - \ln x_i(t_0) \\
  &=& \ln x_i(t_0) + \left(H^z_0(t)-H^z_0(s)\right) z_0(s) -\left(H^z_i(t)-H^z_i(s)\right)z_i(s)\\
  &&+ \ln \left( \frac{P^n_0(0,s)}{P^n_0(0,t)} \frac{P^n_i(0,t)}{P^n_i(0,s)}\right) \\
  && - \frac12 \int_s^t (\sigma^x_i)^2\,du \\
  &&+\frac12 \left((H^z_0(t))^2 \zeta^z_0(t) -  (H^z_0(s))^2 \zeta^z_0(s)- \int_s^t (H^z_0)^2
  (\alpha^z_0)^2\,du\right)\\
  &&-\frac12 \left((H^z_i(t))^2 \zeta^z_i(t) -  (H^z_i(s))^2 \zeta^z_i(s)-\int_s^t (H^z_i)^2 (\alpha^z_i)^2\,du
  \right)\\
  && + \rho^{zx}_{0i} \int_s^t H^z_0\, \alpha^z_0\, \sigma^x_i\,du \\
  &&  - \int_s^t \left(H^z_i(t)-H^z_i\right)\gamma_i \,du, \qquad\mbox{with}\quad s = t_0, \quad t = t_0+\Delta t
\end{eqnarray*}

with

\begin{eqnarray*}
  \gamma_i = -H^z_i\,(\alpha^z_i)^2  + H^z_0\,\alpha^z_0\,\alpha^z_i\,\rho^{zz}_{0i} - \sigma_i^x\,\alpha^z_i\,
  \rho^{zx}_{ii}
\end{eqnarray*}

The expectation of the Inflation processes $z_{I,k}, y_{I,k}$ conditional on $\mathcal{F}_{t_0}$ at any time $t>t_0$ is
equal to $z_{I,k}(t_0)$ resp. $y_{I,k}(t_0)$ since both processes are drift free.

The IR-IR covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
      \mathrm{Cov} [\Delta z_a, \Delta \ln x_b] &=& \rho^{zz}_{0a}\int_s^t \left(H^z_0(t)-H^z_0\right)
  \alpha^z_0\,\alpha^z_a\,du \nonumber\\
      &&- \rho^{zz}_{ab}\int_s^t \alpha^z_a \left(H^z_b(t)-H^z_b\right) \alpha^z_b \,du \nonumber\\
      &&+\rho^{zx}_{ab}\int_s^t \alpha^z_a \, \sigma^x_b \,du.
\end{eqnarray*}

The IR-FX covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
      \mathrm{Cov} [\Delta z_a, \Delta \ln x_b] &=& \rho^{zz}_{0a}\int_s^t \left(H^z_0(t)-H^z_0\right)
  \alpha^z_0\,\alpha^z_a\,du \nonumber\\
      &&- \rho^{zz}_{ab}\int_s^t \alpha^z_a \left(H^z_b(t)-H^z_b\right) \alpha^z_b \,du \nonumber\\
      &&+\rho^{zx}_{ab}\int_s^t \alpha^z_a \, \sigma^x_b \,du.
\end{eqnarray*}

The FX-FX covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
      \mathrm{Cov}[\Delta \ln x_a, \Delta \ln x_b] &=&
      \int_s^t \left(H^z_0(t)-H^z_0\right)^2 (\alpha_0^z)^2\,du \nonumber\\
      && -\rho^{zz}_{0a} \int_s^t \left(H^z_a(t)-H^z_a\right) \alpha_a^z\left(H^z_0(t)-H^z_0\right) \alpha_0^z\,du
  \nonumber\\
      &&- \rho^{zz}_{0b}\int_s^t \left(H^z_0(t)-H^z_0\right)\alpha_0^z \left(H^z_b(t)-H^z_b\right)\alpha_b^z\,du
  \nonumber\\
      &&+ \rho^{zx}_{0b}\int_s^t \left(H^z_0(t)-H^z_0\right)\alpha_0^z \sigma^x_b\,du \nonumber\\
      &&+ \rho^{zx}_{0a}\int_s^t \left(H^z_0(t)-H^z_0\right)\alpha_0^z\,\sigma^x_a\,du \nonumber\\
      &&- \rho^{zx}_{ab}\int_s^t \left(H^z_a(t)-H^z_a\right)\alpha_a^z \sigma^x_b,du\nonumber\\
      &&- \rho^{zx}_{ba}\int_s^t \left(H^z_b(t)-H^z_b\right)\alpha_b^z\,\sigma^x_a\, du \nonumber\\
      &&+ \rho^{zz}_{ab}\int_s^t \left(H^z_a(t)-H^z_a\right)\alpha_a^z \left(H^z_b(t)-H^z_b\right)\alpha_b^z\,du
  \nonumber\\
      &&+ \rho^{xx}_{ab}\int_s^t\sigma^x_a\,\sigma^x_b \,du
\end{eqnarray*}

The IR-INF covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
  \mathrm{Cov}[ \Delta z_a, \Delta z_{I,b} ] & = & \rho_{ab}^{zI} \int_s^t \alpha_a(s) \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta z_a, \Delta y_{I,b} ] & = & \rho_{ab}^{zI} \int_s^t \alpha_a(s) H_{I,b}(s) \alpha_{I,b}(s) ds
\end{eqnarray*}

The FX-INF covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
  \mathrm{Cov}[ \Delta x_a, \Delta z_{I,b} ] & = & \rho_{0b}^{zI} \int_s^t \alpha_0(s) (H_0(t)-H_0(s)) \alpha_{I,b}(s) ds \\
                                             & & -\rho_{ab}^{zI} \int_s^t \alpha_a(s)(H_a(t)-H_a(s))\alpha_{I,b}(s) ds \\
                                             & & +\rho_{ab}^{xI}\int_s^t \sigma_a(s) \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta x_a, \Delta y_{I,b} ] & = & \rho_{0b}^{zI} \int_s^t \alpha_0(s) (H_0(t)-H_0(s)) H_{I,b}(s)\alpha_{I,b}(s) ds \\
                                             & & -\rho_{ab}^{zI} \int_s^t \alpha_a(s)(H_a(t)-H_a(s))H_{I,b}(s)\alpha_{I,b}(s) ds \\
                                             & & +\rho_{ab}^{xI}\int_s^t \sigma_a(s) H_{I,b}(s)\alpha_{I,b}(s) ds
\end{eqnarray*}

The INF-INF covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
  \mathrm{Cov}[ \Delta z_{I,a}, \Delta z_{I,b} ] & = & \rho_{ab}^{II} \int_s^t \alpha_{I,a}(s) \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta z_{I,a}, \Delta y_{I,b} ] & = & \rho_{ab}^{II} \int_s^t \alpha_{I,a}(s) H_{I,b}(s)
                                                       \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta y_{I,a}, \Delta y_{I,b} ] & = & \rho_{ab}^{II} \int_s^t H_{I,a}(s) \alpha_{I,a}(s) H_{I,b}(s) \alpha_{I,b}(s) ds
\end{eqnarray*}

\begin{eqnarray*}
\end{eqnarray*}

\subsection{Exposures}\label{sec:app_exposure}

In ORE we use the following exposure definitions
\begin{align}
\EE(t) = \EPE(t) &= \E^N\left[ \frac{(NPV(t)-C(t))^+}{N(t)} \right] \label{EE}\\
\ENE(t) &= \E^N\left[ \frac{(-NPV(t)+C(t))^+}{N(t)} \right] \label{ENE}
\end{align}
where $\NPV(t)$ stands for the netting set NPV and $C(t)$ is the collateral balance\footnote{$C(t)>0$ means that we have
  {\em received} collateral from the counterparty} at time $t$. Note that these exposures are expectations of values
discounted with numeraire $N$ (in ORE the Linear Gauss Markov model's numeraire) to today, and expectations are taken in
the measure associated with numeraire $N$. These are the exposures which enter into unilateral CVA and DVA calculation,
respectively, see next section. Note that we sometimes label the expected exposure (\ref{EE}) EPE, not to be confused
with the Basel III Expected Positive Exposure below.

\medskip
Basel III defines a number of exposures each of which is a 'derivative' of Basel's Expected Exposure:
\begin{align}
\intertext{Expected Exposure}
EE_B(t) &= \E[\max(NPV(t) - C(t), 0)] \label{basel_ee}\\
\intertext{Expected Positive Exposure}
EPE_B(T) &= \frac{1}{T} \sum_{t<T} EE_B(t)\cdot \Delta t  \label{basel_epe} \\
\intertext{Effective Expected Exposure, recursively defined as running maximum}
EEE_B(t) &= \max(EEE_B(t-\Delta t), EE_B(t)) \label{basel_eee}\\
\intertext{Effective Expected Positive Exposure}
EEPE_B(T) &= \frac{1}{T} \sum_{t<T} EEE_B(t)\cdot \Delta t \label{basel_eepe}
\end{align}
The last definition, Effective EPE, is used in Basel documents since Basel II for Exposure At Default and capital
calculation. Following \cite{bcbs128,bcbs189} the time averages in the EPE and EEPE calculations are taken over {\em the
  first year} of the exposure evolution (or until maturity if all positions of the netting set mature before one year).

\medskip
To compute $EE_B(t)$ consistently in a risk-neutral setting, we compound (\ref{EE}) with the deterministic discount factor $P(t)$ up to horizon $t$:
$$
EE_B(t) = \frac{1}{P(t)} \:\EE(t)
$$

Finally, we define another common exposure measure, the {\em Potential Future Exposure} (PFE), as a (typically high)
quantile $\alpha$ of the NPV distribution through time, similar to Value at Risk but at the upper end of the NPV
distribution:

\begin{align}
  \PFE_\alpha(t) = \left(\inf\left\{ x | F_t(x) \geq \alpha\right\}\right)^+ \label{PFE}
\end{align}

where $F_t$ is the cumulative NPV distribution function at time $t$. Note that we also take the positive part to ensure
that PFE is a positive measure even if the quantile yields a negative value which is possible in extreme cases.
 
\subsection{CVA and DVA}\label{sec:app_cvadva}

Using the expected exposures in \ref{sec:app_exposure} unilateral discretised CVA and DVA are given by \cite{Lichters}
\begin{align}
\CVA &= \sum_{i} \PD(t_{i-1},t_i)\times\LGD\times \EPE(t_i) \label{CVA}\\
\DVA &= \sum_{i} \PD_{Bank}(t_{i-1},t_i)\times\LGD_{Bank}\times \ENE(t_i) \label{DVA}
\end{align}
where
\begin{align*}
\EPE(t) & \mbox{ expected exposure (\ref{EE})}\\
\ENE(t) & \mbox{ expected negative exposure (\ref{ENE})}\\
PD(t_i,t_j) & \mbox{ counterparty probability of default in } [t_i;t_j]\\
PD_{Bank}(t_i,t_j) & \mbox{ our probability of default in } [t_i;t_j]\\
LGD & \mbox{ counterparty loss given default}\\
LGD_{Bank} & \mbox{ our loss given default}\\
\end{align*}

Note that the choice $t_i$ in the arguments of $\EPE(t_i)$ and $\ENE(t_i)$ means we are choosing the {\em advanced}
rather than the {\em postponed} discretization of the CVA/DVA integral \cite{BrigoMercurio}. This choice can be easily
changed in the ORE source code or made configurable. \\

Moreover, formulas (\ref{CVA}, \ref{DVA}) assume independence of credit and other market risk factors, so that $\PD$ and
$\LGD$ factors are outside the expectations. With the extension of ORE to credit asset classes and in particular for
wrong-way-risk analysis, CVA/DVA formulas will be generalised.

\subsection{FVA}\label{sec:fva}

Any exposure (uncollateralised or residual after taking collateral into account) gives rise to funding cost or benefits
depending on the sign of the residual position. This can be expressed as a Funding Value Adjustment (FVA). A simple
definition of FVA can be given in a very similar fashion as the sum of unilateral CVA and DVA which we defined by
(\ref{CVA},\ref{DVA}), namely as an expectation of exposures times funding spreads:
\begin{align}
  \FVA &= \underbrace{\sum_{i=1}^n f_b(t_{i-1},t_i)\,\delta_i \, \E^N\left[S_C(t_{i-1})\, S_B(t_{i-1})\, (\NPV(t_i))^+\,
         D(t_i)\right]}_{\mbox{Funding Benefit Adjustment (FBA)}}\nonumber\\
       & {} - \underbrace{\sum_{i=1}^n f_l(t_{i-1},t_i)\,\delta_i \, \E^N\left[S_C(t_{i-1})\, S_B(t_{i-1})\, (-\NPV(t_i))^+\, D(t_i)\right]}_{\mbox{Funding Cost Adjustment (FCA)}}\label{eq_simple_fva}
\end{align}
where
\begin{align*}
D(t_i) & \mbox{ stochastic discount factor, $1/N(t_i)$ in LGM}\\
\NPV(t_i) & \mbox{ portfolio value after potential collateralization}\\
S_C(t_j) & \mbox{ survival probability of the counterparty}\\
S_B(t_j) & \mbox{ survival probability of the bank}\\
f_b(t_j) & \mbox{ borrowing spread for the bank relative to the collateral compounding rate}\\
f_l(t_j) & \mbox{ lending spread for the bank relative to the collateral compounding rate}
\end{align*}
For details see e.g. Chapter 14 in Gregory \cite{Gregory12} and the discussion in \cite{Lichters}.

\subsection{COLVA}

When the CSA defines a collateral compounding rate that deviates from the overnight rate, this gives rise to another
value adjustment labeled COLVA \cite{Lichters}. In the simplest case the deviation is just given by a constant spread
$\Delta$:
\begin{align}
\COLVA &= \E^N\left[ \sum_i -C(t_i)\cdot \Delta \cdot \delta_i \cdot D(t_{i+1}) \right]
\label{COLVA}
\end{align}
where $C(t)$ is the collateral balance\footnote{see \ref{sec:app_exposure}, $C(t)>0$ means that we have {\em received}
  collateral from the counterparty} at time $t$ and $D(t)$ is the stochastic discount factor $1/N(t)$ in LGM. Both
$C(t)$ and
$N(t)$ are computed in ORE's Monte Carlo framework, and the expectation yields the desired adjustment. \\
 
Replacing the constant spread by a time-dependent deterministic function in ORE is straight forward. 
  
\subsection{Collateral Floor Value}

A less trivial extension of the simple COLVA calculation above, also covered in ORE, is the case where the deviation
between overnight rate and collateral rate is stochastic itself. A popular example is a CSA under which the collateral
rate is the overnight rate {\em floored at zero}. To work out the value of this CSA feature one can take the difference
of discounted margin cash flows with and without the floor feature. It is shown in \cite{Lichters} that the following
formula is a good approximation to the collateral floor value
\begin{align}
\Pi_{Floor} &= \E^N\left[ \sum_i -C(t_i)\cdot (-r(t_i))^+\cdot\delta_i \cdot D(t_{i+1}) \right]
\label{CSA_floor_value_approx}
\end{align}
where $r$ is the stochastic overnight rate and $(-r)^+ = r^+ - r$ is the difference between floored and 'un-floored' compounding rate. \\

Taking both collateral spread and floor into account, the value adjustment is 
\begin{align}
\Pi_{Floor,\Delta} &= \E^N\left[ \sum_i -C(t_i)\cdot ((r(t_i)-\Delta)^+-r(t_i))\cdot\delta_i \cdot D(t_{i+1}) \right] 
\label{CSA_floor_value_approx_2}
\end{align}

\subsection{Dynamic Initial Margin and MVA}\label{sec:app_dim}

The introduction of Initial Margin posting in non-cleared OTC derivatives business reduces residual credit exposures and
the associated value adjustments, {\bf CVA} and {\bf DVA}.

On the other hand, it gives rise to additional funding cost. The value of the latter is referred to as Margin Value Adjustment ({\bf MVA}).\\

To quantify these two effects one needs to model Initial Margin under future market scenarios, i.e. Dynamic Initial Margin ({\bf DIM}). Potential approaches comprise 
\begin{itemize}
\item Monte Carlo VaR embedded into the Monte Carlo simulation
\item Regression-based methods
\item Delta VaR under scenarios
\item ISDA's Standard Initial Margin (SIMM) under scenarios
\end{itemize} 

We skip the first option as too computationally expensive for ORE. In the first ORE release we focus on a relatively
simple regression approach as in \cite{Anfuso2016}. Consider the netting set values $\NPV(t)$ and $\NPV(t+\Delta)$ that
are spaced one margin period of risk $\Delta$ apart. Moreover, let $F(t,t+\Delta)$ denote cumulative netting set cash
flows between time $t$ and $t+\Delta$, converted into the NPV currency. Let $X(t)$ then denote the netting set value
change during the margin period of risk excluding cash flows in that period:
$$
X(t) = \NPV(t+\Delta) + F(t, t+\Delta) - \NPV(t) 
$$  
ignoring discounting/compounding over the margin period of risk. We actually want to determine the distribution of
$X(t)$ conditional on the `state of the world' at time $t$, and pick a high (99\%) quantile to determine the Initial
Margin amount for each time $t$. Instead of working out the distribution, we content ourselves with estimating the
conditional variance $\V(t)$ or standard deviation $S(t)$ of $X(t)$, assuming a normal distribution and scaling $S(t)$
to the desired 99\% quantile by multiplying with the usual factor $\alpha=2.33$ to get an estimate of the Dynamic
Initial Margin $\DIM$:
$$
\V(t) = \E_t[X^2] - \E_t^2[X], \qquad S(t)=\sqrt{\V(t)}, \qquad \DIM(t) = \alpha \,S(t)
$$ 
We further assume that $\E_t[X]$ is small enough to set it to the expected value of $X(t)$ across all Monte Carlo
samples $X$ at time $t$ (rather than estimating a scenario dependent mean). The remaining task is then to estimate the
conditional expectation $\E_t[X^2]$. We do this in the spirit of the Longstaff Schwartz method using regression of
$X^2(t)$ across all Monte Carlo samples at a given time. As a regressor (in the one-dimensional case) we could use
$\NPV(t)$ itself. However, we rather choose to use an adequate market point (interest rate, FX spot rate) as regression
variable $x$, because this is generalised more easily to the multi-dimensional case. As regression basis functions we
use polynomials, i.e. regression functions of the form $c_0 + c_1\,x + c_2\,x^2 + ...+ c_n\,x^n$ where the order $n$ of
the polynomial can be selected by the user. Choosing the lowest order $n=0$, we obtain the simplest possible estimate,
the variance of $X$ across all samples at time $t$, so that we apply a single $\DIM(t)$ irrespective of the 'state of
the world' at time $t$ in that case.  The extension to multi-dimensional regression is also implemented in ORE. The user
can choose several regressors simultaneously (e.g. a EUR rate, a USD rate, USD/EUR spot FX rate, etc.) in order order to
cover complex multi-currency portfolios.

\medskip
Given the DIM estimate along all paths, we can next work out the Margin Value Adjustment \cite{Lichters} in discrete form
%{\color{red}
\begin{align}
\MVA &= \sum_{i=1}^n (f_b - s_I)\, \delta_i\: S_C(t_i)\: S_B(t_i) \times \E^N\left[
\DIM(t_i)\,D(t_i)\right]. \label{MVA} 
\end{align}
%}
with borrowing spread $f_b$ as in the FVA section \ref{sec:fva} and spread $s_I$ received on initial margin, both
spreads relative to the cash collateral rate.

\subsection{Collateral Model}\label{sec:app_collateral}

The collateral model implemented in ORE is based on the evolution of collateral account balances along each Monte Carlo
path taking in to account thresholds, minimum transfer amounts and independent amounts defined in the CSA, as well as
margin periods of risk.

ORE computes the collateral requirement (aka \emph{Credit Support Amount}) through time along each Monte Carlo path
\begin{align}\label{eq:CSA}
CSA(t_m) &= 
\begin{cases}
\max(0, V_{set}(t_m) - I_A - T_{hold}),& V_{set}(t_m) - I_A \ge 0 \\
\min(0, V_{set}(t_m) - I_A + T_{hold}),& V_{set}(t_m) - I_A < 0
\end{cases}
\end{align}
where
\begin{itemize}
\item $V_{set}(t_m)$ is the value of the netting set as of
  time $t_m$
  \item $T_{hold}$ is the threshold exposure below which no collateral is
  required (possibly asymmetric)
%\item $MTA$ is the minimum transfer amount for collateral margin
%  flow requests (possibly asymmetric)
\item $I_A$ is the sum of all collateral independent amounts attached to
  the underlying portfolio of trades (positive amounts imply that the bank
  has received a net inflow of independent amounts from the
  counterparty), assumed here to be cash.
\end{itemize}

As the collateral account already has a value of $C(t_m)$ at time $t_m$, the collateral shortfall is simply the
difference between $C(t_m)$ and $CSA(t_m)$. However, we also need to account for the possibility that margin calls
issued in the past have not yet been settled (for instance, because of disputes). If $M(t_m)$ denotes the net value of
all outstanding margin calls at $t_m$, and $\Delta(t)$ is the difference $\Delta(t) = CSA(t_m) - C(t_m) - M(t_m)$
between the {\em Credit Support Amount} and the current and outstanding collateral, then the actual margin
\emph{Delivery Amount} $D(t_m)$ is calculated as follows:
\begin{align}\label{eq:DA}
D(t_m) &= 
\begin{cases}
\Delta(t),& \left| \Delta(t) \right| \ge MTA \\
0,& \left| \Delta(t) \right| < MTA
\end{cases}
\end{align}
where $MTA$ is the minimum transfer amount.

\medskip Finally, the {\em Delivery Amount } is settled with a delay specified by the {\em Margin Period of Risk}
(MPoR) which leads to residual exposure and XVA even for daily margining, zero thresholds and minimum transfer amounts,
see for example \cite{Pykhtin2010}. A more detailed framework for collateralised exposure modelling is introduced in the
2016 article \cite{Andersen2016}, indicating a potential route for extending ORE.

\subsection{Exposure Allocation}\label{sec:app_allocation}

XVAs and exposures are typically computed at netting set level. For accounting purposes it is typically required to {\em
  allocate} XVAs from netting set to individual trade level such that the allocated XVAs add up to the netting set
XVA. This distribution is not trivial, since due to netting and imperfect correlation single trade (stand-alone) XVAs
hardly ever add up to the netting set XVA: XVA is sub-additive similar to VaR. ORE provides an allocation method
(labeled {\em marginal allocation } in the following) which slightly generalises the one proposed in
\cite{PykhtinRosen}. Allocation is done pathwise which first leads to allocated expected exposures and then to allocated
CVA/DVA by inserting these exposures into equations (\ref{CVA},\ref{DVA}). The allocation algorithm in ORE is as
follows:
\begin{itemize}
\item Consider the netting set's discounted $\NPV$ after taking collateral into account, on a given path at time $t$:
$$
E(t)=D(0,t)\,(\NPV(t)-C(t))
$$ 
\item On each path, compute contributions $A_i$ of the latter to trade $i$ as
$$
A_{i} (t) = \left\{ \begin{array}{ll} 
E(t) \times \NPV_{i}(t) / \NPV(t), & |\NPV(t)| > \epsilon \\
E(t) / n, & |\NPV(t)| \le \epsilon
\end{array}
\right. 
$$
with number of trades $n$ in the netting set and trade $i$'s value $\NPV_i(t)$.
\item The $\EPE$ fraction allocated to trade $i$ at time $t$ by averaging over paths:
$$
\EPE_i(t) = \E\left[ A_i^+(t) \right]
$$
\end{itemize}
By construction, $\sum_i A_i(t) = E(t)$ and hence $\sum_i \EPE_i(t) = \EPE(t)$.\\

We introduced the {\em cutoff } parameter $\epsilon>0$ above in order to handle the case where the netting set value
$\NPV(t)$ (almost) vanishes due to netting, while the netting set 'exposure' $E(t)$ does not. This is possible in a
model with nonzero MTA and MPoR. Since a single scenario with vanishing $\NPV(t)$ suffices to invalidate the expected
exposure at this time $t$, the cutoff is essential. Despite introducing this cutoff, it is obvious that the marginal
allocation method can lead to spikes in the allocated exposures. And generally, the marginal allocation leads to both
positive and negative $\EPE$ allocations.

\medskip As a an example for a simple alternative to the marginal allocation of $\EPE$ we provide allocation based on
today's single-trade CVAs
$$
w_i = \CVA_i / \sum_i \CVA_i.
$$
This yields allocated exposures proportional to the netting set exposure, avoids spikes and negative $\EPE$, but does
not distinguish the 'direction' of each trade's contribution to $\EPE$ and $\CVA$.

\end{appendix}

%========================================================
%\section{References}
%========================================================

\begin{thebibliography}{*}

\bibitem{ORE} \url{http://www.opensourcerisk.org}

\bibitem{QL} \url{http://www.quantlib.org}
 
\bibitem{QRM} \url{http://www.quaternion.com}

\bibitem{quantlib-install} \url{http://quantlib.org/install/vc10.shtml}

%\bibitem{confluence} https://confluence.atlassian.com/bitbucket/set-up-git-744723531.html

\bibitem{git-download} \url{https://git-scm.com/downloads}

\bibitem{boost-binaries} \url{https://sourceforge.net/projects/boost/files/boost-binaries}

\bibitem{boost} \url{http://www.boost.org}

\bibitem{jupyter} \url{http://jupyter.org}

\bibitem{Anaconda} \url{https://docs.continuum.io/anaconda}

\bibitem{LO} \url{http://www.libreoffice.org}

%\bibitem{xlwings} \url{http://www.xlwings.org}

\bibitem{bcbs128} Basel Committee on Banking Supervision, {\em International Convergence of Capital Measurement and
    Capital Standards, A Revised Framework}, \url{http://www.bis.org/publ/bcbs128.pdf}, June 2006

\bibitem{bcbs189} Basel Committee on Banking Supervision, {\em Basel III: A global regulatory framework for more
    resilient banks and banking systems}, \url{http://www.bis.org/publ/bcbs189.pdf}, June 2011

\bibitem{BrigoMercurio} Damiano Brigo and Fabio Mercurio, {\em Interest Rate Models: Theory and Practice, 2nd Edition},
  Springer, 2006.

\bibitem{Pykhtin2010} Michael Pykhtin, {\em Collateralized Credit Exposure}, in Counterparty Credit Risk, (E. Canabarro,
  ed.), Risk Books, 2010

\bibitem{PykhtinRosen} Michael Pykhtin and Dan Rosen, {\em Pricing Counterparty Risk at the Trade Level and CVA
    Allocations}, Finance and Economics Discussion Series, Divisions of Research \& Statistics and Monetary Affairs,
  Federal Reserve Board, Washington, D.C., 2010

\bibitem{Gregory12} Jon Gregory, {\em Counterparty Credit Risk and Credit Value Adjustment, 2nd Ed.}, Wiley Finance,
  2013.

\bibitem{Lichters} Roland Lichters, Roland Stamm, Donal Gallagher, {\em Modern Derivatives Pricing and Credit Exposure
    Analysis, Theory and Practice of CSA and XVA Pricing, Exposure Simulation and Backtesting}, Palgrave Macmillan,
  2015.

\bibitem{Anfuso2016} Fabrizio Anfuso, Daniel Aziz, Paul Giltinan, Klearchos Loukopoulos, {\em A Sound Modelling and
    Backtesting Framework for Forecasting Initial Margin Requirements},
  \url{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2716279}, 2016

\bibitem{Andersen2016} Leif B. G. Andersen, Michael Pykhtin, Alexander Sokol, {\em Rethinking Margin Period of Risk},
  http://papers.ssrn.com/sol3/papers.cfm?abstract\_id=2719964, 2016

  % \bibitem{SIMM}{SIMM Methodology\\ \tiny
  %   http://www2.isda.org/attachment/ODM1Mw==/ISDA\%20SIMM\%20Methodology\_7\%20April\%202016\_v3.15\%20(PUBLIC).pdf}

  % \bibitem{SIMM_Data_Standards}{SIMM Risk Data Standards\\ \tiny
  %   https://www2.isda.org/attachment/ODQzMg==/Risk\%20Data\%20Standards\_24\%20May\%202016\_v1.22\%20(PUBLIC).pdf}

  % \bibitem{OO} http://www.openoffice.org

\end{thebibliography}

\newpage
\addcontentsline{toc}{section}{Todo}
\listoftodos[Todo]%\todos

\end{document}
