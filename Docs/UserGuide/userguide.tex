\documentclass[12pt, a4paper]{article}

\usepackage[urlcolor=blue]{hyperref}

\usepackage[disable]{todonotes}
%\usepackage{todonotes}

\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}
%\usepackage[miktex]{gnuplottex}
%\ShellEscapetrue
\usepackage{epstopdf}
\usepackage{longtable}
\usepackage{floatrow}
\usepackage{minted}
\usepackage{textcomp}
\usepackage{color,soul}
\usepackage[font={small,it}]{caption}
\floatsetup[listing]{style=Plaintop}    
\floatsetup[longlisting]{style=Plaintop}    

% Turn off indentation but allow \indent command to still work.
\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

\addtolength{\textwidth}{0.8in}
\addtolength{\oddsidemargin}{-.4in}
\addtolength{\evensidemargin}{-.4in}
\addtolength{\textheight}{1.6in}
\addtolength{\topmargin}{-.8in}

\usepackage{longtable,supertabular}
\usepackage{listings}
\lstset{
  frame=top,frame=bottom,
  basicstyle=\ttfamily,
  language=XML,
  tabsize=2,
  belowskip=2\medskipamount
}

%\usepackage{float}
\usepackage{tabu}
\tabulinesep=1.0mm
\restylefloat{table}

\usepackage{siunitx}

\newenvironment{longlisting}{\captionsetup{type=listing}}{}
%\usepackage[colorlinks=true]{hyperref}

\renewcommand\P{\ensuremath{\mathbb{P}}}
\newcommand\E{\ensuremath{\mathbb{E}}}
\newcommand\Q{\ensuremath{\mathbb{Q}}}
\newcommand\I{\mathds{1}}
\newcommand\F{\ensuremath{\mathcal F}}
\newcommand\V{\ensuremath{\mathbb{V}}}
\newcommand\YOY{{\rm YOY}}
\newcommand\Prob{\ensuremath{\mathbb{P}}}
\newcommand{\D}[1]{\mbox{d}#1}
\newcommand{\NPV}{\mathit{NPV}}
\newcommand{\CVA}{\mathit{CVA}}
\newcommand{\DVA}{\mathit{DVA}}
\newcommand{\FVA}{\mathit{FVA}}
\newcommand{\COLVA}{\mathit{COLVA}}
\newcommand{\FCA}{\mathit{FCA}}
\newcommand{\FBA}{\mathit{FBA}}
\newcommand{\KVA}{\mathit{KVA}}
\newcommand{\MVA}{\mathit{MVA}}
\newcommand{\PFE}{\mathit{PFE}}
\newcommand{\EE}{\mathit{EE}}
\newcommand{\EPE}{\mathit{EPE}}
\newcommand{\ENE}{\mathit{ENE}}
\newcommand{\PD}{\mathit{PD}}
\newcommand{\LGD}{\mathit{LGD}}
\newcommand{\DIM}{\mathit{DIM}}
\newcommand{\bs}{\textbackslash}
\newcommand{\REDY}{\color{red}Y}
 
\begin{document}

%\title{Open Source Risk Engine \\ User Guide  }
\title{ORE User Guide}
\author{Quaternion Risk Management}
%\date{7 October 2016}
%\date{28 April 2017}
\date{7 December 2017}
\maketitle

\newpage

%-------------------------------------------------------------------------------
\section*{Document History}

\begin{center}
\begin{supertabular}{|l|l|p{9cm}|}
\hline
Date & Author & Comment \\
\hline
7 October 2016 & Quaternion & initial release\\
28 April 2017 & Quaternion  & updates for release 2\\
7 December 2017 & Quaternion & updates for release 3\\
\hline
\end{supertabular}
\end{center}

\newpage

\tableofcontents
\newpage

\section{Introduction}

The {\em Open Source Risk Project} \cite{ORE} aims at providing a transparent platform for pricing and risk analysis
that serves as
%\medskip
\begin{itemize}
\item a benchmarking, validation, training, and teaching reference,
\item an extensible foundation for tailored risk solutions.
\end{itemize}

Its main software project is {\em Open Source Risk Engine} (ORE), an application that provides
\begin{itemize}
\item a Monte Carlo simulation framework for contemporary risk analytics and value adjustments
\item simple interfaces for trade data, market data and system configuration
\item simple launchers and result visualisation in Jupyter, Excel, LibreOffice
\item unit tests and various examples.  
\end{itemize}
ORE is open source software, provided under the Modified BSD License. It is based 
on QuantLib, the open source library for quantitative finance \cite{QL}.

%\medskip
\subsubsection*{Audience}
The project aims at reaching quantitative risk ma\-nage\-ment practitioners (be it in financial institutions, audit
firms, consulting companies or regulatory bodies) who are looking for accessible software solutions, and quant
developers in charge of the implementation of pricing and risk methods similar to those in ORE. Moreover, the project
aims at reaching academics and students who would like to teach or learn quantitative risk management using a freely
available, contemporary risk application.

\subsubsection*{Contributions}
Quaternion Risk Management \cite{QRM} is committed to sponsoring the Open Source Risk project through ongoing project
administration, through providing an initial release and a series of subsequent releases in order to achieve a wide
analytics, product and risk factor class coverage. The community is invited to contribute to ORE, for example through
feedback, discussions and suggested enhancement in the forum on the ORE site \cite{ORE}, as well as contributions of ORE
enhancements in the form of source code. See the FAQ section on the ORE site \cite{ORE} on how to get involved.

\subsubsection*{Scope and Roadmap}

ORE currently provides portfolio pricing, cash flow generation, sensitivity analysis, stress testing and a range of contemporary derivative portfolio analytics. The latter are based on a Monte Carlo simulation framework which yields 
the evolution of various {\bf credit exposure} measures:
\begin{itemize}
\item EE aka EPE (Expected Exposure or Expected Positive Exposure)
\item ENE (Expected Negative Exposure, i.e. the counterparty's perspective)
\item 'Basel' exposure measures relevant for regulatory capital charges under internal model methods 
\item PFE (Potential Future Exposure at some user defined quantile)
%\item Value at Risk and Expected Shortfall
\end{itemize}
and {\bf derivative value adjustments}
\begin{itemize}
\item CVA (Credit Value Adjustment)
\item DVA (Debit Value Adjustment)
\item FVA (Funding Value Adjustment)
\item COLVA (Collateral Value Adjustment)
\item MVA (Margin Value Adjustment)
\end{itemize}
for portfolios with netting, variation and initial margin agreements. 

\medskip
The sensitivity framework yields further {\bf market risk measures} such as ORE's parametric Value at Risk which takes deltas, vegas, gammas and cross gammas into account. This may be used to benchmark initial margin models such ISDA'S Standard Initial Margin Model. \\

\medskip
Subsequent ORE releases will also compute {\bf regulatory capital charges} for counterparty credit risk under the new standardised approach (SA-CCR), and the Monte Carlo based market risk measures will be complemented by parametric methods, e.g. for benchmarking various initial margin calculation models applied in cleared and non-cleared derivatives business.

\medskip 
%The first release of ORE in October 2016 covers the simulation of interest rate and FX risk factors and
%portfolios of Interest Rate Swaps, Caps/Floors, Swaptions, FX Forwards, Cross Currency Swaps and FX Options. Subsequent
%releases from Q1 2017 onwards will extend the derivative product and the risk factor range to Inflation, Credit, Equity
%and Commodity. With the introduction of credit risk factors, the scope will also be extended to cover cash products
%(loans and bonds) and related portfolio analytics.
The product coverage of the third release of ORE in December 2017 is sketched in the following table.
\begin{table}[hbt]
\scriptsize
\begin{center}
\begin{tabular}{|l|p{1.5cm}|p{1.5cm}|p{1.2cm}|p{1.5cm}|}
\hline
Product & Pricing and Cashflows & Sensitivity Analysis & Stress Testing & Exposure Simulation \& XVA\\
\hline
Fixed and Floating Rate Bonds/Loans & Y & Y & Y & N \\
\hline
Interest Rate Swaps & Y & Y & Y & Y\\
\hline
Caps/Floors & Y & Y & Y & Y\\
\hline
Swaptions & Y & Y & Y &Y \\
\hline
Constant Maturity Swaps, CMS Caps/Floors & Y & Y & Y & Y\\
\hline
FX Forwards & Y & Y & Y & Y \\
\hline
Cross Currency Swaps & Y & Y & Y & Y \\
\hline
FX Options & Y & Y & Y & Y\\
\hline
Equity Forwards & Y & Y & Y & Y\\
\hline
Equity Options & Y & Y & Y & Y \\
\hline
CPI Swaps & Y & Y & N & Y \\
\hline
%CPI Caps/Floors & Y & Y & N \\
%\hline
Year-on-Year Inflation Swaps & Y & Y & N & Y \\
\hline
%Year-on-Year Inflation Caps/Floors & Y & N & N \\
%\hline
Credit Default Swaps & Y & Y & N & N \\
\hline
\end{tabular}
\caption{ORE product coverage.}
\label{tab_coverage}
\end{center}
\end{table}

Future releases will further extend the product and the risk factor range to Commodity, complete the analytics scope indicated in the table above and expand on the market risk analytics, add integrated credit/market risk analytics. 

\medskip The simulation models applied in ORE's risk factor evolution implement the models discussed in detail in {\em
  Modern Derivatives Pricing and Credit Exposure Analysis} \cite{Lichters}: The IR/FX/INF/EQ risk factor evolution is based on
a cross currency model consisting of an arbitrage free combination of Linear Gauss Markov models for all interest rates
and lognormal processes for FX rates and EQ prices, Dodgeson-Kainth models for inflation. The model components are calibrated to cross currency discounting and forward curves, Swaptions, FX Options, EQ Options and CPI caps/floors.

\subsubsection*{Further Resources}
\begin{itemize}
\item Open Source Risk Project site: \url{http://www.opensourcerisk.org}
\item Frequently Asked Questions: \url{http://www.opensourcerisk.org/faqs}
\item Forum: \url{http://www.opensourcerisk.org/forum}
\item Source code and releases: \url{https://github.com/opensourcerisk/engine}
\item Follow ORE on Twitter {\tt @OpenSourceRisk} for updates on releases and events
\end{itemize}
%An ORE Book will follow in 2017 that will elaborate on the engine's design and make the connection between methodology
%and implementation. It will be announced in due course on the channels mentioned above.
 
\subsubsection*{Organisation of this document}

This document focuses on instructions how to use ORE to cover basic workflows from individual deal analysis to portfolio
processing. After an overview over the core ORE data flow in section \ref{sec:process} and installation instructions in
section \ref{sec:installation} we start in section \ref{sec:examples} with a series of examples that illustrate how to
launch ORE using its command line application, and we discuss typical results and reports. We then illustrate in section
\ref{sec:visualisation} interactive analysis of resulting 'NPV cube' data. The final sections of this text document ORE
parametrisation and the structure of trade and market data input.

%========================================================
\section{Release Notes}\label{sec:releasenotes}
%========================================================

This section summarises the high level functionality changes between release 2 (05/2017) and 3 (12/2017).

\medskip
\begin{itemize}
\item Added CMS and CMS Caps/Floors (pricing, sensitivity analysis and simulation)
\item Added Credit Default Swaps (pricing and sensitivity analysis, no simulation yet)
\item Added amortisation structures (fixed, percentage relative to initial or previous notional, annuity) to leg data
\item Extended sensitivity framework (adding inflation, equity and credit) 
\item Added inflation simulation and XVA, Dodgeson-Kainth simulation model calibrated to CPI Caps/Floors
\item Added basic parametric Value at Risk (Delta and Delta-Gamma Normal VaR)
\item Additional unit tests and examples
\item Curve loader refactoring
\item User guide updates
\end{itemize}

%\medskip
%API changes
%\begin{itemize}
%\item 
%\end{itemize}

\todo[inline]{Complete release notes}

%========================================================
\section{ORE Data Flow}\label{sec:process}
%========================================================

The core processing steps followed in ORE to produce risk analytics results are sketched in Figure \ref{fig_process}.
All ORE calculations and output are generated in three fundamental process steps as indicated in the three boxes in the
upper part of the figure. In each of these steps appropriate data (described below) is loaded and results are generated,
either in form of a human readable report, or in an intermediate step as pure data files (e.g. NPV data, exposure data).
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.6]{process.pdf}
\end{center}
\caption{Sketch of the ORE process, inputs and outputs. }
\label{fig_process}
\end{figure}

The overall ORE process needs to be parametrised using a set of configuration XML files which is the subject of section
\ref{sec:configuration}. The portfolio is provided in XML format which is explained in detail in sections
\ref{sec:portfolio_data} and \ref{sec:nettingsetinput}. Note that ORE comes with 'Schema' files for all supported
products so that any portfolio xml file can be validated before running through ORE. Market data is provided in a simple
three-column text file with unique human-readable labelling of market data points, as explained in section
\ref{sec:market_data}.  \\

The first processing step (upper left box) then comprises 
\begin{itemize}
\item loading the portfolio to be analysed, 
\item building any yield curves or other 'term structures' needed for pricing, 
\item calibration of pricing and simulation models.
\end{itemize}

The second processing step (upper middle box) is then 
\begin{itemize}
\item portfolio valuation, cash flow generation,
\item going forward - conventional risk analysis such as sensitivity analysis and stress testing, standard-rule capital
  calculations such as SA-CCR, etc,
\item and in particular, more time-consuming, the market simulation and portfolio valuation through time under Monte
  Carlo scenarios.
\end{itemize}
This process step produces several reports (NPV, cashflows etc) and in particular an {\bf NPV cube}, i.e. NPVs per
trade, scenario and future evaluation date. The cube is written to a file in both condensed binary and human-readable
text format.  \\

The third processing step (upper right box) performs more 'sophisticated' risk ana\-ly\-sis by post-processing the NPV
cube data:
\begin{itemize}
\item aggregating over trades per netting set, 
\item applying collateral rules to compute simulated variation margin as well as simulated (dynamic) initial margin
  posting,
\item computing various XVAs including CVA, DVA, FVA, MVA for all netting sets, with and without taking collateral
  (variation and initial margin) into account, on demand with allocation to the trade level.
\end{itemize}
The output of this process step are XVA reports and the 'net' NPV cube, i.e. after aggregation, netting and collateral. \\

The example section \ref{sec:examples} demonstrates for representative product types how the described processing steps
can be combined in a simple batch process which produces the mentioned reports, output files and exposure evolution
graphs in one 'go'.

Moreover, both NPV cubes can be further analysed interactively using a visualisation tool introduced in section
\ref{sec:jupyter}. And finally, sections \ref{sec:calc} and \ref{sec:excel} demonstrate how ORE processes can be
launched in spreadsheets and key results presented automatically within the same sheet.

%========================================================
\section{Getting and Building ORE}\label{sec:installation}
%========================================================

You can get ORE in two ways, either by downloading a release bundle as described in section \ref{sec:release} (easiest if you just want to use ORE) or by
checking out the source code from the github repository as described in section \ref{sec:build_ore} (easiest if you want to build and develop ORE).

\subsection{ORE Releases}\label{sec:release}

ORE releases are regularly provided in the form of source code archives, Windows exe\-cutables {\tt ore.exe}, example
cases and documentation. Release archives will be provided at \url{https://github.com/opensourcerisk/engine/releases}.

\medskip
The release consists of a single archive in zip format
\begin{itemize}
\item {\tt ORE-<VERSION>.zip}
\end{itemize}

When unpacked, it creates a directory {\tt ORE-<VERSION>} with the following files respectively subdirectories
\begin{enumerate}
%\item {\tt bin/win32/ore.exe}
%\item {\tt bin/x64/ore.exe}
\item {\tt App/}
\item {\tt Docs/}
\item {\tt Examples/}
\item {\tt FrontEnd/}
\item {\tt OREAnalytics/}
\item {\tt OREData/}
\item {\tt QuantExt/}
\item {\tt ThirdPartyLibs/}
\item {\tt tools/}
\item {\tt xsd/}
\item {\tt userguide.pdf}
\end{enumerate} 

The first three items and {\tt userguide.pdf} are sufficient to run the compiled ORE application
on the list of examples described in the user guide (this works on Windows only). The Windows executables are located in {\tt App/bin/Win32/Release/} respectively {\tt App/bin/x64/Release/}. To continue with the compiled
executables:
\begin{itemize}
\item Ensure that the scripting language Python is installed on your computer, see also section \ref{sec:python}
  below;
\item Move on to the examples in section \ref{sec:examples}.
\end{itemize}

\medskip
The release bundle does contain the ORE source code, which is sufficient to build ORE from sources manually as follows (if you build ORE for development purposes, we recommend using git though, see section \ref{sec:build_ore}):
\begin{itemize}
\item Set up Boost as described in section \ref{sec:boost}, unless already installed
\item Set up QuantLib 1.11 \cite{QL,quantlib-install} from its github or sourceforge download page, unless already
  installed; QuantLib needs to be located in this project directory {\tt ORE-<VERSION>}. Alternatively, you can create a
  symbolic link named QuantLib here that points to the actual QuantLib directory
\item Build QuantExt, OREData, OREAnalytics, App (in this order) as described in section \ref{sec:build}
\item Note that ThirdPartyLibs does not need to be built, it contains RapdidXml, header only code for reading and
  writing XML files
\item Move on to section \ref{sec:python} and the examples in section \ref{sec:examples}.
\end{itemize}

Open {\tt Docs/html/index.html} to see the API documentation for QuantExt, OREData and OREAnalytics, generated by
doxygen.

\subsection{Building ORE}\label{sec:build_ore}

ORE's source code is hosted on github.com at \url{https://github.com/opensourcerisk/engine} using {\tt git}, a free and
open source distributed version control system.

\subsubsection{Git}

To access the current code base on GitHub, one needs to get {\tt git} installed first.
   
\begin{enumerate}
\item Install and setup Git on your machine following instructions at \cite{git-download}

\item Fetch ORE from github by running the following: 

{\tt\% git clone https://github.com/opensourcerisk/engine.git ore}      

This will create a folder 'ore' in your current directory that contains the codebase.

\item Initially, the QuantLib subdirectory under {\tt ore} is empty as it is a submodule pointing to the official
  QuantLib repository. To pull down locally, use the following commands:

{\tt
\% cd ore \\
\% git submodule init \\
\% git submodule update
}

\end{enumerate}

\subsubsection{Boost}\label{sec:boost}

QuantLib and ORE depend on the boost C++ libraries. Hence these need to be installed before building QuantLib and
ORE. With Unix (Linux, OS X), we recommend boost version 1\_55 or higher, with Windows we recommend boost version 1\_57
or higher. Older versions may work on some platforms and system configurations, but were not tested.

\subsubsection*{Windows}

\begin{enumerate}
\item Download the pre-compiled binaries for MSVC-14 (MSVC2015) from \cite{boost-binaries}
%, any recent version should work
\begin{itemize}
\item 32-bit: \cite{boost-binaries}{\bs}VERSION{\bs}boost\_VERSION-msvc-14.0-32.exe{\bs}download 
\item 64-bit: \cite{boost-binaries}{\bs}VERSION{\bs}boost\_VERSION-msvc-14.0-64.exe{\bs}download
\end{itemize}
\item Start the installation file and choose an installation folder. Take a note of that folder as it will be needed
  later on.
\item Finish the installation by clicking Next a couple of times.
\end{enumerate}
    
Alternatively, compile all Boost libraries directly from the source code:

\begin{enumerate}
\item Open a Visual Studio Tools Command Prompt
\begin{itemize}
\item 32-bit: VS2015/VS2013 x86 Native Tools Command Prompt
\item 64-bit: VS2015/VS2013 x64 Native Tools Command Prompt
\end{itemize}
\item Navigate to the boost root directory
\item Run bootstrap.bat
\item Build the libraries from the source code
\begin{itemize}
\item 32-bit: \\
  {\footnotesize\tt .{\bs}b2 --stagedir=.{\bs}lib{\bs}Win32{\bs}lib --build-type=complete toolset=msvc-14.0 \bs \\
    address-model=32 --with-test --with-system --with-filesystem  \bs \\
    --with-serialization --with-regex --with-date\_time stage}
\item 64-bit: \\
  {\footnotesize\tt .{\bs}b2 --stagedir=.{\bs}lib{\bs}x64{\bs}lib --build-type=complete toolset=msvc-14.0 \bs \\
    address-model=64 --with-test --with-system --with-filesystem \bs \\
    --with-serialization --with-regex --with-date\_time stage}
\end{itemize}
\end{enumerate}

\subsubsection*{Unix}

\begin{enumerate}
\item Download Boost from \cite{boost} and build following the instructions on the site
%, any recent version should work
\item Define the environment variable BOOST that points to the boost directory
(so includes should be in BOOST and libs should be in BOOST/stage/lib)
\end{enumerate}

\subsubsection{ORE Libraries and Application}\label{sec:build}

\subsubsection*{Windows}

\begin{enumerate}

\item Download and install Visual Studio Community Edition (Version 2013 or later). 
During the installation, make sure you install the Visual
C++ support under the Programming Languages features (disabled by default).

\item To configure the boost paths in Visual Studio open any of the Visual Studio solution files in item 3 below and
  select View $\rightarrow$ Other Windows $\rightarrow$ Property Manager. It does not matter which solution you open, if
  it is for example the Quant\-Ext solution you should see two Projects 'QuantExt' and 'quantexttestsuite' in the property
  manager. Expand any of them (e.g. QuantExt) and then one of the Win32 or x64 configurations. The settings will be
  specific for the Win32 or x64 configuration but otherwise it does not matter which of the projects or configurations
  you expand, they all contain the same configuration file. You should now see 'Microsoft.Cpp.Win32.user' respectively
  'Microsoft.Cpp.x64.user' depending on whether you chose a Win32 or a x64 configuration. Click on this file to open the
  property pages. Select VC++ Directories and then add your boost directory to the 'Include Directories' entry. Likewise
  add your boost library directory to the 'Library Directories' entry. If for example your boost installation is in {\tt
    C:{\bs}boost\_1\_57\_0} and the libraries reside in the {\tt stage{\bs}lib} subfolder, add {\tt
    C:{\bs}boost\_1\_57\_0} to the 'Include Directories' entry and {\tt C:{\bs}boost\_1\_57\_0{\bs}stage{\bs} lib} to the
  'Library Directories' entry. Press OK. 
  (Alternatively, create and use an environment variable {\tt \%BOOST\%} pointing to your directory {\tt C:{\bs}boost\_1\_57\_0} instead of the directory itself.) 
  If you want to configure the boost paths for Win32 resp. x64 as well, repeat
  the previous step for 'Microsoft.Cpp. Win32.user' respectively 'Microsoft.Cpp.x64.user'. To complete the configuration
  just close the property manager window.

  % \item Open any solution file and update path to Boost as described in sections 5 \& 6 in
  %   \cite{quantlib-install}. You only need to do this for a single project as this will update the path to boost
  %   across all projects and solutions on your machine. The paths should be set as follow (in case you compiled Boost
  %   on your own, use the path specified using the --stagedir cmd argument):
%
%\begin{itemize}
%\item Include Directories: [Boost Installation Folder]
%\item Library Directories: [Boost Installation Folder]{\bs}libs
%\end{itemize}
%
%\item Add the additional path to the Boost pre-compiled libraries to the linker setting:
%
%\begin{itemize}
%\item 32-bit: [Boost Installation Folder]{\bs}lib32-msvc-14.0
%\item 64-bit: [Boost Installation Folder]{\bs}lib64-msvc-14.0
%\end{itemize}

\item Open each of the sub-projects and compile them in the following order: QuantLib, QuantExt, OREData, OREAnalytics
  and App. For each project, do the following:

\begin{itemize}
\item Switch to the correct platform (i.e. Win32 or x64) from the Configuration Manager. The selection should match the
  pre-compiled version of Boost. Trying to compile using a mixed configuration (e.g. Boost 64-bit and 32-bit QuantLib)
  will fail.
\item Compile the project: Build $\rightarrow$ Build Solution
\item Once the compilation is complete, run the test suite.
\end{itemize}
Alternatively, open the {\tt oreEverything\_*.sln} and build the entire solution (again, make sure to select the correct platform in the configuration manager first).
\end{enumerate}

\subsubsection*{Unix}

\begin{enumerate}

\item Build QuantLib as usual.

{\tt\footnotesize
\% cd QuantLib \\
\% ./autogen.sh \\
\% ./configure --with-boost-include=\$BOOST --with-boost-lib=\$BOOST/stage/lib \\
\% make -j4 
}

\item Build QuantExt

{\tt\footnotesize
\% cd QuantExt \\
\% ./autogen.sh \\
\% ./configure \\
\% make -j4
}

This will build both the QuantExt library and test suite.

\item Run the test suite

{\tt\footnotesize
\% ./test/quantext-test-suite 
}

\item  Build OREData, OREAnalytics and their test suites. 

Follow the same steps as for QuantExt.
To run the unit test suites, do 

{\tt\footnotesize
\% ./test/ored-test-suite 
}

and 

{\tt\footnotesize
\% ./test/orea-test-suite 
}

in the respective library directories.

\item Build App/ore

{\tt\footnotesize
\% cd App \\
\% ./autogen.sh \\
\% ./configure \\
\% make -j4
}

Note: On Linux systems, the 'locale' settings can negatively affect the ORE process and output. To avoid this, we
recommend setting the environment variable {\tt LC\_NUMERIC} to {\tt C}, e.g. in a bash shell, do

{\tt\footnotesize
\% export LC\_NUMERIC=C
}

before running ORE or any of the examples below. This will suppress thousand separators in numbers when converted to
strings.

\item Run Examples (see section \ref{sec:examples})

{\tt\footnotesize
\% cd Examples/Example\_1 \\
\% python run.py 
}

\end{enumerate}

\subsection{Python and Jupyter}\label{sec:python}

Python (version 3.5 or higher) is required to run the examples in section \ref{sec:examples} and plot exposure
evolutions. Moreover, we use Jupyter \cite{jupyter} in section \ref{sec:visualisation} to visualise simulation
results. Both are part of the 'Anaconda Open Data Science Analytics Platform' \cite{Anaconda}. Anaconda installation
instructions for Windows, OS X and Linux are available on the Anaconda site, with graphical installers for
Windows\footnote{With Windows, after a fresh installation of Python the user may have to run the {\tt python} command
  once in a command shell so that the Python executable will be found subsequently when running the example scripts in
  section \ref{sec:examples}.}, Linux and OS X.

With Linux and OS X, the following environment variable settings are required
\begin{itemize}
\item set {\tt LANG} and {\tt LC\_ALL } to {\tt en\_US.UTF-8} or {\tt en\_GB.UTF-8}
\item set {\tt LC\_NUMERIC} to {\tt C}. 
\end{itemize}
The former is required for both running the Python scripts in the examples section, as well as successful installation
of the following packages. \\

The full functionality of the Jupyter notebook introduced in section \ref{sec:jupyter} requires furthermore installing
\begin{itemize}
\item jupyter\_dashboards: \url{https://github.com/jupyter-incubator/dashboards}
\item ipywidgets: \url{https://github.com/ipython/ipywidgets}
\item pythreejs: \url{https://github.com/jovyan/pythreejs}
\item bqplot: \url{https://github.com/bloomberg/bqplot}
\end{itemize}
With Python and Anaconda already installed, this can be done by running these commands
\begin{itemize}
\item {\tt conda install -c conda-forge ipywidgets}
\item {\tt pip install jupyter\_dashboards}
\item {\tt jupyter dashboards quick-setup --sys-prefix}
\item {\tt conda install -c conda-forge bqplot}
\item {\tt conda install -c conda-forge pythreejs}
\end{itemize}
Note that the bqplot installation requires the environment settings mentioned above.

%========================================================
\section{Examples}\label{sec:examples}
%========================================================

The examples shown in table \ref{tab_0} are intended to help with getting started with ORE, and to serve as plausibility
checks for the simulation results generated with ORE.

\begin{table}[hbt]
\scriptsize
\begin{center}
\begin{tabular}{|c|l|}
\hline
Example & Description \\
\hline
\hline
1 & Vanilla at-the-money Swap with flat yield curve \\
\hline
2 & Vanilla Swap with normal yield curve \\
\hline
3 & European Swaption \\
\hline
4 & Bermudan Swaption \\
\hline
5 & Callable Swap \\
\hline
6 & Cap/Floor \\
\hline
7 & FX Forward \\
  & European FX Option \\ 
\hline
8 & Cross Currency Swap without notional reset \\
\hline
9 & Cross Currency Swap with notional reset \\
\hline
10 & Three-Swap portfolio with netting and collateral \\
   & XVAs - CVA, DVA, FVA, MVA, COLVA \\
   & Exposure and XVA Allocation to trade level \\
\hline
11 & Basel exposure measures - EE, EPE, EEPE \\
\hline
12 & Long term simulation with horizon shift \\
\hline
13 & Dynamic Initial Margin and MVA \\
\hline
14 & Minimal Market Data Setup \\
\hline
15 & Sensitivity Analysis and Stress Testing \\
\hline
16 & Equity Derivatives Exposure \\
\hline
17 & Inflation Swap Exposure \\
\hline
18 & Bonds and Amortisation Structures\\
\hline
19 & Swaption Pricing with Smile\\
\hline
20 & Credit Default Swap Pricing\\
\hline
21 & Constant Maturity Swap Pricing\\
\hline
22 & Option Sensitivity Analysis with Smile\\
\hline
23 & Forward Rate Agreement and Averaging OIS Exposure\\
\hline
\end{tabular}
\caption{ORE examples.}
\label{tab_0}
\end{center}
\end{table}

All example results can be produced with the Python scripts {\tt run.py} in the ORE release's {\tt Examples/Example\_\#}
folders which work on both Windows and Unix platforms. In a nutshell, all scripts call ORE's command line application
with a single input XML file

\medskip
\centerline{\tt ore[.exe] ore.xml}
\medskip

They produce a number of standard reports and exposure graphs in PDF format. The structure of the input file and of the
portfolio, market and other configuration files referred to therein will be explained in section
\ref{sec:configuration}.

\medskip ORE is driven by a number of input files, listed in table \ref{tab_1} and explained in detail in sections
\ref{sec:configuration} to \ref{sec:fixings}. In all examples, these input files are either located in the example's sub
directory {\tt Examples/Example\_\#/Input} or the main input directory {\tt Examples/Input} if used across several
examples. The particular selection of input files is determined by the 'master' input file {\tt ore.xml}.

\begin{table}[h]
\scriptsize
\begin{center}
\begin{tabular}{|l|p{11cm}|}
  \hline
  File Name & Description \\
  \hline
  {\tt ore.xml}&   Master input file, selection of further inputs below and selection of analytics \\
  {\tt portfolio.xml} & Trade data \\
  {\tt netting.xml} &  Collateral (CSA) data \\
  {\tt simulation.xml} & Configuration of simulation model and market\\
  {\tt market.txt} &  Market data snapshot \\
  {\tt fixings.txt} &  Index fixing history \\
  {\tt curveconfig.xml} & Curve and term structure composition from individual market instruments\\
  {\tt conventions.xml} & Market conventions for all market data points\\
  {\tt todaysmarket.xml} &  Configuration of the market composition, relevant for the pricing of the given portfolio as
                           of today (yield curves, FX rates, volatility surfaces etc) \\
  {\tt pricingengines.xml} &  Configuration of pricing methods by product\\
  \hline
\end{tabular}
\end{center}
\caption{ORE input files}
\label{tab_1}
\end{table}

The typical list of output files and reports is shown in table \ref{tab_2}. The names of output files can be configured
through the master input file {\tt ore.xml}. Whether these reports are generated also depends on the setting in {\tt
  ore.xml}. For the examples, all output will be written to the directory {\tt Examples/Example\_\#/Output}.

\begin{table}[h]
\scriptsize
\begin{center}
\begin{tabular}{|l|p{11cm}|}
\hline
File Name & Description \\
\hline
{\tt npv.csv}&   NPV report \\
{\tt flows.csv} & Cashflow report \\
{\tt curves.csv} & Generated yield (discount) curves report \\
{\tt xva.csv} & XVA report, value adjustments at netting set and trade level \\
{\tt exposure\_trade\_*.csv} & Trade exposure evolution reports\\
{\tt exposure\_nettingset\_*.csv} &  Netting set exposure evolution reports\\
{\tt rawcube.csv} & NPV cube in readable text format \\
{\tt netcube.csv} & NPV cube after netting and colateral, in readable text format \\
{\tt *.dat} & Intermediate storage of NPV cube and scenario data in binary format \\
{\tt *.pdf} &  Exposure graphics produced by the python script {\tt run.py} after ORE completed\\
\hline
\end{tabular}
\end{center}
\caption{ORE output files}
\label{tab_2}
\end{table}

Note: When building ORE from sources on Windows platforms, make sure that you copy your {\tt ore.exe} to the binary
directory {\tt bin/win32/} respectively {\tt bin/x64/}. Otherwise the examples may be run using the pre-compiled
executables which come with the ORE release.

%--------------------------------------------------------
\subsection{Interest Rate Swap Exposure}\label{sec:example1}
%--------------------------------------------------------

We start with a vanilla single currency Swap (currency EUR, maturity 20y, notional 10m, receive fixed 2\% annual, pay
6M-Euribor flat). The market yield curves (for both discounting and forward projection) are set to be flat at 2\% for
all maturities, i.e. the Swap is at the money initially and remains at the money on average throughout its life. Running
ORE in directory {\tt Examples/Example\_1} with

\medskip
\centerline{\tt python run.py } 
\medskip

yields the exposure evolution in 

\medskip
\centerline{\tt Examples/Example\_1/Output/*.pdf } 
\medskip

and shown in figure \ref{fig_1}. 
\begin{figure}[h!]
\begin{center}
%\includegraphics[scale=0.45]{mpl_swap_1_1m_sbb_100k.pdf}
\includegraphics[scale=0.45]{mpl_swap_1_1m_sbb_10k_flat.pdf}
\end{center}
\caption{Vanilla ATM Swap expected exposure in a flat market environment from both parties' perspectives. The symbols are European Swaption prices. The simulation was run with monthly time steps and 10,000 Monte Carlo samples to demonstrate the convergence of EPE and ENE profiles. A similar
outcome can be obtained more quickly with 5,000 samples on a quarterly time grid which is the default setting of Example\_1. }
\label{fig_1}
\end{figure}
Both Swap simulation and Swaption pricing are run with calls to the ORE executable, essentially 

\medskip
\centerline{\tt ore[.exe] ore.xml} 

\centerline{\tt ore[.exe] ore\_swaption.xml} 
\medskip

which are wrapped into the script {\tt Examples/Example\_1/run.py} provided with the ORE release.
It is instructive to look into the input folder in Examples/Example\_1, the content of the main input file {\tt
  ore.xml}, together with the explanations in section \ref{sec:configuration}. \\

This simple example is an important test case which is also run similarly in one of the unit test suites of ORE. The
expected exposure can be seen as a European option on the underlying netting set, see also appendix
\ref{sec:app_exposure}. In this exampl«e, the expected exposure at some future point in time, say 10 years, is equal to
the European Swaption price for an option with expiry in 10 years, underlying Swap start in 10 years and underlying Swap
maturity in 20 years. We can easily compute such standard European Swaption prices for all future points in time where
both Swap legs reset, i.e. annually in this case\footnote{Using closed form expressions for standard European Swaption
  prices.}. And if the simulation model has been calibrated to the points on the Swaption surface which are used for
European Swaption pricing, then we can expect to see that the simulated exposure matches Swaption prices at these annual
points, as in figure \ref{fig_1}.  In Example\_1 we used co-terminal ATM Swaptions for both model calibration and
Swaption pricing. Moreover, as the the yield curve is flat in this example, the exposures from both parties'
perspectives (EPE and ENE) match not only at the annual resets, but also for the period between annual reset of both
legs to the point in time when the floating leg resets. Thereafter, between floating leg (only) reset and next joint
fixed/floating leg reset, we see and expect a deviation of the two exposure profiles.

\medskip Moving to {\tt Examples/Example\_2}, we see what changes when using a realistic (non-flat) market
environment. Running the example with

\medskip
\centerline{\tt python run.py } 
\medskip

yields the exposure evolution in 

\medskip
\centerline{\tt Examples/Example\_2/Output/*.pdf } 
\medskip

shown in figure \ref{fig_2}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_swap_3.pdf}
\end{center}
\caption{Vanilla ATM Swap expected exposure in a realistic market environment as of 05/02/2016 from both parties'
  perspectives. The Swap is the same as in figure \ref{fig_1} but receiving fixed 1\%, roughly at the money. The symbols
  are the prices of European payer and receiver Swaptions. Simulation with 5000 paths and monthly time steps.}
\label{fig_2}
\end{figure}
In this case, where the curves (discount and forward) are upward sloping, the receiver Swap is at the money at inception
only and moves (on average) out of the money during its life. Similarly, the Swap moves into the money from the
counterparty's perspective. Hence the expected exposure evolutions from our perspective (EPE) and the counterparty's
perspective (ENE) 'detach' here, while both can still be be reconciled with payer or respectively receiver Swaption
prices.

%--------------------------------------------------------
\subsection{European Swaption Exposure}\label{sec:european_swaption}
%--------------------------------------------------------

This demo case in folder {\tt Examples/Example\_3} shows the exposure evolution of European Swaptions with cash and
physical delivery, respectively, see figure \ref{fig_3}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_swaption.pdf}
\end{center}
\caption{European Swaption exposure evolution, expiry in 10 years, final maturity in 20 years, for cash and physical
  delivery. Simulation with 1000 paths and quarterly time steps. }
\label{fig_3}
\end{figure}
The delivery type (cash vs physical) yields significantly different valuations as of today due to the steepness of the
relevant yield curves (EUR). The cash settled Swaption's exposure graph is truncated at the exercise date, whereas the
physically settled Swaption exposure turns into a Swap-like exposure after expiry. For comparison, the example also
provides the exposure evolution of the underlying forward starting Swap which yields a somewhat higher exposure after
the forward start date than the physically settled Swaption. This is due to scenarios with negative Swap NPV at expiry
(hence not exercised) and positive NPVs thereafter. Note the reduced EPE in case of a Swaption with settlement of the option premium on exercise date.

%--------------------------------------------------------
\subsection{Bermudan Swaption Exposure}
%--------------------------------------------------------

This demo case in folder {\tt Examples/Example\_4} shows the exposure evolution of Bermudan rather than European
Swaptions with cash and physical delivery, respectively, see figure \ref{fig_3b}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_bermudan_swaption.pdf}
\end{center}
\caption{Bermudan Swaption exposure evolution, 5 annual exercise dates starting in 10 years, final maturity in 20 years,
  for cash and physical delivery. Simulation with 1000 paths and quarterly time steps.}
\label{fig_3b}
\end{figure}
The underlying Swap is the same as in the European Swaption example in section \ref{sec:european_swaption}. Note in
particular the difference between the Bermudan and European Swaption exposures with cash settlement: The Bermudan shows
the typical step-wise decrease due to the series of exercise dates. Also note that we are using the same Bermudan option
pricing engines for both settlement types, in contrast to the European case, so that the Bermudan option cash and
physical exposures are identical up to the first exercise date. When running this example, you will notice the
significant difference in computation time compared to the European case (ballpark 30 minutes here for 2 Swaptions, 1000
samples, 90 time steps). The Bermudan example takes significantly more computation time because we use an LGM grid
engine for pricing under scenarios in this case. In a realistic context one would more likely resort to American Monte
Carlo simulation, feasible in ORE, but not provided in the current release. However, this implementation can be used to
benchmark any faster / more sophisticated approach to Bermudan Swaption exposure simulation.

%--------------------------------------------------------
\subsection{Callable Swap Exposure}
%--------------------------------------------------------

This demo case in folder {\tt Examples/Example\_5} shows the exposure evolution of a European callable Swap, represented
as two trades - the non-callable Swap and a Swaption with physical delivery. We have sold the call option, i.e. the
Swaption is a right for the counterparty to enter into an offsetting Swap which economically terminates all future flows
if exercised. The resulting exposure evolutions for the individual components (Swap, Swaption), as well as the callable
Swap are shown in figure \ref{fig_4}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_callable_swap.pdf}
\end{center}
\caption{European callable Swap represented as a package consisiting of non-callable Swap and Swaption. The Swaption has
  physical delivery and offsets all future Swap cash flows if exercised. The exposure evolution of the package is shown
  here as 'EPE NettingSet' (green line). This is covered by the pink line, the exposure evolution of the same Swap but
  with maturity on the exercise date. The graphs match perfectly here, because the example Swap is deep in the money and
  exercise probability is close to one. Simulation with 5000 paths and quarterly time steps.}
\label{fig_4}
\end{figure}
The example is an extreme case where the underlying Swap is deeply in the money (receiving fixed 5\%), and hence the
call exercise probability is close to one. Modify the Swap and Swaption fixed rates closer to the money ($\approx$ 1\%)
to see the deviation between net exposure of the callable Swap and the exposure of a 'short' Swap with maturity on
exercise.

%--------------------------------------------------------
\subsection{Cap/Floor Exposure}\label{sec:capfloor}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_6} generates exposure evolutions of several Swaps, caps and floors. The
example shown in figure \ref{fig_capfloor_1} ('portfolio 1') consists of a 20y Swap receiving 3\% fixed and paying
Euribor 6M plus a long 20y Collar
with both cap and floor at 4\% so that the net exposure corresponds to a Swap paying 1\% fixed. \\

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_capfloor_1.pdf}
\end{center}
\caption{Swap+Collar, portfolio 1. The Collar has identical cap and floor rates at 4\% so that it corresponds to a
  fixed leg which reduces the exposure of the Swap, which receives 3\% fixed. Simulation with 1000 paths and quarterly
  time steps.}
\label{fig_capfloor_1}
\end{figure}

The second example in this folder shown in figure \ref{fig_capfloor_2} ('portfolio 2') consists of a short Cap, long
Floor and a long Collar that exactly offsets the netted Cap and Floor.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_capfloor_2.pdf}
\end{center}
\caption{Short Cap and long Floor vs long Collar, portfolio 2. Simulation with 1000 paths and quarterly time steps.}
\label{fig_capfloor_2}
\end{figure}

Further three test portfolios are provided as part of this example. Run the example and inspect the respective output
directories {\tt Examples/Example\_7/Output/portfolio\_\#}. Note that these directories have to be present/created
before running the batch with {\tt python run.py}.

%--------------------------------------------------------
\subsection{FX Forward Exposure}\label{sec:fxfwd}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_7} generates the exposure evolution for a EUR / USD FX Forward transaction
with value date in 10Y. This is a particularly simple show case because of the single cash flow in 10Y. On the other
hand it checks the cross currency model implementation by means of comparison to analytic limits - EPE and ENE at the
trade's value date must match corresponding Vanilla FX Option prices, as shown in figure \ref{fig_5}.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.45]{mpl_fxforward.pdf}
\end{center}
\caption{EUR/USD FX Forward expected exposure in a realistic market environment as of 26/02/2016 from both parties'
  perspectives. Value date is obviously in 10Y. The flat lines are FX Option prices which coincide with EPE and ENE,
  respectively, on the value date. Simulation with 5000 paths and quarterly time steps.}
\label{fig_5}
\end{figure}

%--------------------------------------------------------
\subsection{FX Option Exposure}\label{sec:fxoption}
%--------------------------------------------------------

This example (in folder {\tt Examples/Example\_7}, as the FX Forward example) illustrates the exposure evolution for an
FX Option, see figure \ref{fig_7}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_fxoption.pdf}
\end{center}
\caption{EUR/USD FX Call and Put Option exposure evolution, same underlying and market data as in section
  \ref{sec:fxfwd}, compared to the call and put option price as of today (flat line). Simulation with 5000 paths and
  quarterly time steps.}
\label{fig_7}
\end{figure}
Recall that the FX Option value $NPV(t)$ as of time $0 \leq t \leq T$ satisfies
\begin{align*}
\frac{NPV(t)}{N(t)} &= \mbox{Nominal}\times\E_t\left[\frac{(X(T) - K)^+}{N(T)}\right]\\
NPV(0) &= \E\left[\frac{NPV(t)}{N(t)}\right] = \E\left[\frac{NPV^+(t)}{N(t)} \right]= \EPE(t) 
\end{align*}
One would therefore expect a flat exposure evolution up to option expiry. The deviation from this in ORE's simulation is
due to the pricing approach chosen here under scenarios. A Black FX option pricer is used with deterministic Black
volatility derived from today's volatility structure (pushed or rolled forward, see section \ref{sec:sim_market}). The
deviation can be removed by extending the volatility modelling, e.g. implying model consistent Black volatilities in
each simulation step on each path.  
%\todo[inline]{Add exposure evolution graph with 'simulated' FX vol}

%--------------------------------------------------------
\subsection{Cross Currency Swap Exposure and FX Reset}
%--------------------------------------------------------

The case in {\tt Examples/Example\_8} is a vanilla cross currency Swap. It shows the typical blend of an Interest Rate
Swap's saw tooth exposure evolution with an FX Forward's exposure which increases monotonically to final maturity, see
figure \ref{fig_6}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_ccswap.pdf}
\end{center}
\caption{Cross Currency Swap exposure evolution without mark-to-market notional reset. Simulation with 1000 paths and
  quarterly time steps.}
\label{fig_6}
\end{figure}

The effect of the FX resetting feature, common in Cross Currency Swaps nowadays, is shown in {\tt Examples/Example\_9}.
The example shows the exposure evolution of a EUR/USD cross currency basis Swap with FX reset at each interest period
start, see figure \ref{fig_6b}. As expected, the notional reset causes an exposure collapse at each period start when
the EUR leg's notional is reset to match the USD notional.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_xccy_reset.pdf}
\end{center}
\caption{Cross Currency Basis Swap exposure evolution with and without mark-to-market notional reset. Simulation with
  1000 paths and quarterly time steps.}
\label{fig_6b}
\end{figure}
  
%--------------------------------------------------------
\subsection{Netting and Collateral}
%--------------------------------------------------------

In this example (see folder {\tt Examples/Example\_10}) we showcase a small netting set consisting of three Swaps in
different currencies, with different collateral choices
\begin{itemize}
\item no collateral - figure \ref{fig_8},
\item collateral with threshold (THR) 1m EUR, minimum transfer amount (MTA) 100k EUR, margin period of risk (MPOR) 2
  weeks - figure \ref{fig_9}
\item collateral with zero THR and MTA, and MPOR 2w - figure \ref{fig_10}
\end{itemize}
The exposure graphs with collateral and positive margin period of risk show typical spikes. What is causing these? As
sketched in appendix \ref{sec:app_collateral}, ORE uses a {\em classical collateral model} that applies collateral
amounts to offset exposure with a time delay that corresponds to the margin period of risk. The spikes are then caused
by instrument cash flows falling between exposure measurement dates $d_1$ and $d_2$ (an MPOR apart), so that a
collateral delivery amount determined at $d_1$ but settled at $d_2$ differs significantly from the closeout amount at
$d_2$ causing a significant residual exposure for a short period of time. See for example \cite{Andersen2016} for a
recent detailed discussion of collateral modelling. The approach currently implemented in ORE corresponds to {\em
  Classical+} in \cite{Andersen2016}, the more conservative approach of the classical methods. The less conservative
alternative, {\em Classical-}, would assume that both parties stop paying trade flows at the beginning of the MPOR, so
that the P\&L over the MPOR does not contain the cash flow effect, and exposure spikes are avoided. Note that the size
and position of the largest spike in figure \ref{fig_9} is consistent with a cash flow of the 40 million GBP Swap in the
example's portfolio that rolls over the 3rd of March and has a cash flow on 3 March 2020, a bit more than four years
from the evaluation date.
  
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_nocollateral_epe.pdf}
\end{center}
\caption{Three Swaps netting set, no collateral. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_8}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.45]{mpl_threshold_break_epe.pdf}
\end{center}
\caption{Three Swaps netting set, THR=1m EUR, MTA=100k EUR, MPOR=2w. The red evolution assumes that the each trade is
  terminated at the next break date. The blue evolution ignores break dates. Simulation with 5000 paths and bi-weekly
  time steps.}
\label{fig_9}
\end{figure}

%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=1.0]{example_mta_epe.pdf}
%\end{center}
%\caption{Three swaps, threshold = 0, mta > 0.}
%\label{fig_7}
%\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_mpor_epe.pdf}
\end{center}
\caption{Three Swaps, THR=MTA=0, MPOR=2w. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_10}
\end{figure}

%--------------------------------------------------------
\subsection{CVA, DVA, FVA, COLVA, MVA, Collateral Floor}
%--------------------------------------------------------

We use one of the cases in {\tt Examples/Example\_10} to demonstrate the
XVA outputs, see folder {\tt Examples/Example\_10/Output/collateral\_threshold\_dim}.

\medskip The summary of all value adjustments (CVA, DVA, FVA, COLVA, MVA, as well as the Collateral Floor) is provided
in file {\tt xva.csv}.  The file includes the allocated CVA and DVA numbers to individual trades as introduced in the
next section. The following table illustrates the file's layout, omitting the three columns containing allocated data.

\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|r|r|r|r|r|r|r|r|r|}
\hline
TradeId & NettingSetId & CVA & DVA & FBA & FCA & COLVA & MVA & CollateralFloor & BaselEPE & BaselEEPE \\
\hline
 & CPTY\_A &  6,521  &  151,193  & -946  &  72,103  &  2,769  & -14,203  &  189,936  &  113,260  &  1,211,770 \\
Swap\_1 & CPTY\_A &  127,688  &  211,936  & -19,624  &  100,584  &  n/a  &  n/a  &  n/a   &  2,022,590  &  2,727,010 \\
Swap\_3 & CPTY\_A &  71,315  &  91,222  & -11,270  &  43,370  &  n/a  &  n/a  &  n/a   &  1,403,320  &  2,183,860 \\
Swap\_2 & CPTY\_A &  68,763  &  100,347  & -10,755  &  47,311  &  n/a  &  n/a  &  n/a   &  1,126,520  &  1,839,590 \\
\hline
\end{tabular}
}
\end{center}

The line(s) with empty TradeId column contain values at netting set level, the others contain uncollateralised
single-trade VAs.  Note that COLVA, MVA and Collateral Floor are only available at netting set level at which collateral
is posted.

\medskip
Detailed output is written for COLVA and Collateral Floor to file {\tt colva\_nettingset\_*.csv} which shows the 
incremental contributions to these two VAs through time.


%--------------------------------------------------------
\subsection{Exposure Reports \& XVA Allocation to Trades}
%--------------------------------------------------------
Using the example in folder {\tt Examples/Example\_10} we illustrate here the layout of an exposure report produced by
ORE. The report shows the exposure evolution of Swap\_1 without collateral which - after running Example\_10 - is found
in folder \\
{\tt Examples/Example\_10/Output/collateral\_none/exposure\_trade\_Swap\_1.csv}:

\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|r|r|r|r|r|r|r|r|}
\hline
TradeId & Date & Time & EPE & ENE & AllocEPE & AllocENE & PFE & BaselEE & BaselEEE \\
\hline
Swap\_1 & 05/02/16 & 0.0000 & 0  & 1,711,748  & 0  & 0  & 0  & 0  & 0 \\
Swap\_1 & 19/02/16 & 0.0383 & 38,203   & 1,749,913  & -1,200,677 & 511,033 & 239,504 & 38,202 & 38,202 \\
Swap\_1 & 04/03/16 & 0.0765 & 132,862  & 1,843,837 & -927,499 & 783,476 & 1,021,715 & 132,845 & 132,845 \\
%Swap\_1 & 18/03/16 & 0.1148 & 299,155  & 1,742,450  & -650,225  & 793,067  & 1,914,150  & 299,091  & 299,091 \\
%Swap\_1 & 01/04/16 & 0.1530 & 390,178  & 1,834,810  & -552,029  & 892,604  & 2,373,560  & 390,058  & 390,058 \\
%Swap\_1 & 15/04/16 & 0.1913 & 471,849  & 1,918,600  & -465,580  & 981,171  & 2,765,710  & 471,659  & 471,659 \\
%Swap\_1 & 29/04/16 & 0.2295 & 550,301  & 2,000,640  & -330,578  & 1,119,760  & 3,106,810  & 550,016  & 550,016 \\
%Swap\_1 & 13/05/16 & 0.2678 & 620,279  & 2,074,880  & -266,042  & 1,188,560  & 3,427,080  & 619,888  & 619,888 \\
%Swap\_1 & 27/05/16 & 0.3060 & 690,018  & 2,140,320  & -190,419  & 1,259,880  & 3,778,570  & 689,509  & 689,509 \\
%Swap\_1 & 10/06/16 & 0.3443 & 763,207  & 2,206,020  & -137,681  & 1,305,130  & 4,052,870  & 762,560  & 762,560 \\
Swap\_1 & ... & ...& ... & ... & ... & ... & ... & ... & ... \\
\hline
\end{tabular}
}
\end{center}

The exposure measures EPE, ENE and PFE, and the Basel exposure measures $EE_B$ and $EEE_B$, are defined in appendix
\ref{sec:app_exposure}. Allocated exposures are defined in appendix \ref{sec:app_allocation}. The PFE quantile and
allocation method are chosen as described in section \ref{sec:analytics}. \\

In addition to single trade exposure files, ORE produces an exposure file per netting set. The example from the same
folder as above is:

\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|r|r|r|r|r|r|r|}
\hline
NettingSet & Date & Time & EPE & ENE & PFE & ExpectedCollateral & BaselEE & BaselEEE \\
\hline
CPTY\_A & 05/02/16 & 0.0000 & 1,203,836 & 0 & 1,203,836 & 0 & 1,203,836 & 1,203,836 \\%1,211,770 & 0 & 1,211,770 & 0 & 1,211,770 & 1,211,770\\
CPTY\_A & 19/02/16 & 0.0383 & 1,337,713 & 137,326 & 3,403,460 & 0 & 1,337,651 & 1,337,651 \\ %0.0383 & 1,344,220 & 137,776 & 3,414,000 & 0 & 1,344,160 & 1,344,160\\
%CPTY\_A & 04/03/16 & 0.0765 & 1,518,610 & 308,381 & 4,354,060 & 0 & 1,518,410 & 1,518,410\\
%CPTY\_A & 18/03/16 & 0.1148 & 1,846,900 & 382,068 & 5,200,730 & 0 & 1,846,500 & 1,846,500\\
%CPTY\_A & 01/04/16 & 0.1530 & 1,961,290 & 494,416 & 5,869,470 & 0 & 1,960,690 & 1,960,690\\
%CPTY\_A & 15/04/16 & 0.1913 & 2,067,240 & 598,283 & 6,384,140 & 0 & 2,066,400 & 2,066,400\\
%CPTY\_A & 29/04/16 & 0.2295 & 2,053,670 & 745,960 & 6,740,070 & 0 & 2,052,610 & 2,066,400\\
%CPTY\_A & 13/05/16 & 0.2678 & 2,149,190 & 845,507 & 6,930,230 & 0 & 2,147,840 & 2,147,840\\
%CPTY\_A & 27/05/16 & 0.3060 & 2,235,630 & 930,218 & 7,295,440 & 0 & 2,233,980 & 2,233,980\\
%CPTY\_A & 10/06/16 & 0.3443 & 2,314,470 & 1,014,690 & 7,753,190 & 0 & 2,312,510 & 2,312,510\\
CPTY\_A & ... & ...& ... & ... & ... & ... & ... & ...\\
%CPTY\_A & 07/07/17 & 1.4167 & 3,320,430 & 2,423,890 & 12,787,900 & 0 & 3,304,650 & 3,304,650\\
%CPTY\_A & 21/07/17 & 1.4551 & 3,351,780 & 2,452,640 & 12,964,200 & 0 & 3,335,420 & 3,335,420\\
%CPTY\_A & 04/08/17 & 1.4934 & 3,302,820 & 2,511,500 & 12,796,100 & 0 & 3,286,260 & 3,335,420\\
%CPTY\_A & 18/08/17 & 1.5318 & 3,339,840 & 2,545,850 & 13,120,000 & 0 & 3,322,640 & 3,335,420\\
%CPTY\_A & 01/09/17 & 1.5701 & 3,371,300 & 2,576,100 & 13,238,700 & 0 & 3,353,480 & 3,353,480\\
%CPTY\_A & 15/09/17 & 1.6085 & 3,279,670 & 2,555,370 & 13,041,300 & 0 & 3,261,880 & 3,353,480\\
%CPTY\_A & 29/09/17 & 1.6468 & 3,305,060 & 2,579,200 & 13,072,800 & 0 & 3,286,680 & 3,353,480\\
%CPTY\_A & 13/10/17 & 1.6852 & 3,332,830 & 2,604,200 & 13,225,600 & 0 & 3,313,850 & 3,353,480\\
%CPTY\_A & 27/10/17 & 1.7236 & 3,280,280 & 2,661,770 & 13,034,600 & 0 & 3,261,150 & 3,353,480\\
%CPTY\_A & 13/11/17 & 1.7701 & 3,316,800 & 2,701,060 & 13,331,600 & 0 & 3,296,880 & 3,353,480\\
%CPTY\_A & 24/11/17 & 1.8003 & 3,337,760 & 2,720,870 & 13,402,400 & 0 & 3,317,280 & 3,353,480\\
%CPTY\_A & ... & ...& ... & ... & ... & ... & ... & ...\\
\hline
\end{tabular}
}
\end{center}

Allocated exposures are missing here, as they make sense at the trade level only, and the expected collateral balance is
added for information (in this case zero as collateralisation is deactivated in this example).

\medskip The allocation of netting set exposure and XVA to the trade level is frequently required by finance
departments. This allocation is also featured in {\tt Examples/Example\_10}. We start again with the uncollateralised
case in figure \ref{fig_12}, followed by the case with threshold 1m EUR in figure \ref{fig_13}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_nocollateral_allocated_epe.pdf}
\end{center}
\caption{Exposure allocation without collateral. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_12}
\end{figure}
In both cases we apply the {\em marginal} (Euler) allocation method as published by Pykhtin and Rosen in 2010, hence we
see the typical negative EPE for one of the trades at times when it reduces the netting set exposure. The case with
collateral moreover shows the typical spikes in the allocated exposures.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_threshold_allocated_epe.pdf}
\end{center}
\caption{Exposure allocation with collateral and threshold 1m EUR. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_13}
\end{figure}
The analytics results also feature allocated XVAs in file {\tt xva.csv} which are derived from the allocated exposure
profiles. Note that ORE also offers alternative allocation methods to the marginal method by Pykhtin/Rosen, which can be
explored with {\tt Examples/Example\_10}.

%--------------------------------------------------------
\subsection{Basel Exposure Measures}\label{sec:basel}
%--------------------------------------------------------

Example {\tt Example\_11} demonstrates the relation between the evolution of the expected exposure (EPE in our notation)
to the `Basel' exposure measures EE\_B, EEE\_B, EPE\_B and EEPE\_B as defined in appendix \ref{sec:app_exposure}. In
particular the latter is used in internal model methods for counterparty credit risk as a measure for the exposure at
default. It is a `derivative' of the expected exposure evolution and defined as a time average over the running maximum
of EE\_B up to the horizon of one year.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_basel_exposures.pdf}
\end{center}
\caption{Evolution of the expected exposure of Vanilla Swap, comparison to the `Basel' exposure measures EEE\_B, EPE\_B and EEPE\_B. Note that the latter two are indistinguishable in this case, because the expected exposure is increasing for the first year.}
\label{fig_14}
\end{figure}

%--------------------------------------------------------
\subsection{Long Term Simulation with Horizon Shift}\label{sec:longterm}
%--------------------------------------------------------

The example in folder {\tt Example\_12} finally demonstrates an effect that, at first glance, seems to cause a serious
issue with long term simulations. Fortunately this can be avoided quite easily in the Linear Gauss Markov model setting
that is used here. \\

In the example we consider a Swap with maturity in 50 years in a flat yield curve environment. If we simulate this
naively as in all previous cases, we obtain a particularly noisy EPE profile that does not nearly reconcile with the
known exposure (analytical Swaption prices). This is shown in figure \ref{fig_15} (`no horizon shift'). The origin of
this issue is the width of the risk-neutral NPV distribution at long time horizons which can turn out to be quite small
so that the Monte Carlo simulation with finite number of samples does not reach far enough into the positive or negative
NPV range to adequately sample the distribution, and estimate both EPE and ENE in a single run.  Increasing the number
of samples may not solve the problem, and may not even be feasible in a realistic setting. \\

The way out is applying a `shift transformation' to the Linear Gauss Markov model, see {\tt
  Example\_12/Input/simulation2.xml} in lines 92-95:
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
        <ParameterTransformation>
          <ShiftHorizon>30.0</ShiftHorizon>
          <Scaling>1.0</Scaling>
        </ParameterTransformation>
\end{minted}
%\hrule
%\caption{LGM Shift transformation}
%\label{lst:shift_transformation}
\end{listing}

The effect of the 'ShiftHorizon' parameter $T$ is to apply a shift to the Linear Gauss Markov model's $H(t)$ parameter
(see appendix \ref{sec:app_rfe}) {\em after} the model has been calibrated, i.e. to replace:
$$ 
H(t) \rightarrow H(t) - H(T) 
$$ 
It can be shown that this leaves all expectations computed in the model (such as EPE and ENE) invariant. As explained in
\cite{Lichters}, subtracting an $H$ shift effectively means performing a change of measure from the `native' LGM measure
to a T-Forward measure with horizon $T$, here 30 years. Both negative and positive shifts are permissible, but only
negative shifts are connected with a T-Forward measure and improve numerical stability. \\

In our experience it is helpful to place the horizon in the middle of the portfolio duration to significantly improve
the quality of long term expectations. The effect of this change (only) is shown in the same figure \ref{fig_15}
(`shifted horizon').
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_longterm.pdf}
\end{center}
\caption{Long term Swap exposure simulation with and without horizon shift.}
\label{fig_15}
\end{figure}
Figure \ref{fig_15b} further illustrates the origin of the problem and its resolution: The rate distribution's mean
(without horizon shift or change of measure) drifts upwards due to convexity effects (note that the yield curve is flat
in this example), and the distribution's width is then too narrow at long horizons to yield a sufficient number of low
rate scenarios with contributions to the Swap's $\EPE$ (it is a floating rate payer). With the horizon shift (change of
measure), the distribution's mean is pulled 'back' at long horizons, because the convexity effect is effectively wiped
out at the chosen horizon, and the expected rate matches the forward rate.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_rates.pdf}
\end{center}
\caption{Evolution of rate distributions with and without horizon shift (change of measure). Thick lines indicate mean
  values, thin lines are contours of the rate distribution at $\pm$ one standard devation.}
\label{fig_15b}
\end{figure}

%--------------------------------------------------------
\subsection{Dynamic Initial Margin and MVA}\label{sec:dim}
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_13} demonstrates Dynamic Initial Margin calculations (see also appendix
\ref{sec:app_dim}) for a number of elementary products:
\begin{itemize}
\item A single currency Swap in EUR (case A), 
\item a European Swaption in EUR with physical delivery (case B), 
\item a single currency Swap in USD (case C), and 
\item a EUR/USD cross currency Swap (case D).
\end{itemize}

The examples can be run as before with 

\medskip
\centerline{\tt python run\_A.py} 

\medskip
and likewise for cases B, C and D. The essential results of each run are are visualised in the form of 
\begin{itemize}
\item evolution of expected DIM
\item regression plots at selected future times 
\end{itemize}
illustrated for cases A and B in figures \ref{fig_ex13a_evolution} - \ref{fig_ex13b_regression}. In all cases the zero
order regression estimate of DIM differs noticeably from the higher orders (one and two). In the three swap cases we
moreover see that first and second order polynomial choice makes hardly any difference; note that case C and D use up to
three-dimensional regressors (simulated USD and EUR rate fixings and the EUR/USD FX rate). In the Swaption case B, first
and second order polynomial choice makes a difference before option expiry. More details on this DIM model and its performance can be found in \cite{Anfuso2016,LichtersEtAl}.
 
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_evolution_A_swap_eur.pdf}
\end{center}
\caption{Evolution of expected Dynamic Initial Margin (DIM) for the EUR Swap of Example 13 A. DIM is evaluated using
  regression of NPV change variances versus the simulated 3M Euribor fixing; regression polynomials are zero, first and
  second order (first and second order curves are not distinguishable here). The simulation uses 1000 samples and a time
  grid with bi-weekly steps in line with the Margin Period of Risk.}
\label{fig_ex13a_evolution}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_regression_A_swap_eur.pdf}
\end{center}
\caption{Regression snapshot at time step 100 for the EUR Swap of Example 13 A.}
\label{fig_ex13a_regression}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_evolution_B_swaption_eur.pdf}
\end{center}
\caption{Evolution of expected Dynamic Initial Margin (DIM) for the EUR Swaption of Example 13 B with expiry in 10Y
  around time step 100. DIM is evaluated using regression of NPV change variances versus the simulated 3M Euribor
  fixing; regression polynomials are zero, first and second order. The simulation uses 1000 samples and a time grid with
  bi-weekly steps in line with the Margin Period of Risk.}
\label{fig_ex13b_evolution}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_regression_B_swaption_eur_t100.pdf}
\end{center}
\caption{Regression snapshot at time step 100 (before expiry) for the EUR Swaption of Example 13 B.}
\label{fig_ex13b_regression}
\end{figure}

%--------------------------------------------------------
\subsection{Minimal Market Data Setup}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_14} demonstrates using a minimal market data setup in order to rerun the vanilla Swap exposure simulation shown in {\tt Examples/Example\_1}. The minimal market data uses single points per curve where possible.

%--------------------------------------------------------
\subsection{Sensitivity Analysis, Stress Testing and Parametric Value-at-Risk}\label{ex:sensitivity_stress}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_15} demonstrates the calculation of sensitivities and stress scenarios. The
portfolio used in this example consists of

\begin{itemize}
\item a vanilla swap in EUR
\item a cross currency swap EUR-USD
\item a resettable cross currency swap EUR-USD
\item a FX forward EUR-USD
\item a FX call option on USD/GBP % commented out?
\item a FX put option on USD/EUR
\item an European swaption
\item a Bermudan swaption 
\item a cap and a floor in USD
\item a cap and a floor in EUR
\item a fixed rate bond
\item an Equity call option, put option and forward on S\&P500
\item an Equity call option, put option and forward on Lufthansa
\item a CPI Swap referencing UKRPI
\item a Year-on-Year inflation swap referencing EUHICPXT
\item a USD CDS.
\end{itemize}

The sensitivity configuration in {\tt sensitivity.xml} aims at computing the following sensitivities

\begin{itemize}
\item discount curve sensitivities in EUR, USD; GBP, CHF, JPY, on pillars 6M, 1Y, 2Y, 3Y, 5Y, 7Y, 10Y, 15Y, 20Y (absolute shift of 0.0001)
\item forward curve sensitivities for EUR-EURIBOR 6M and 3M indices, EUR-EONIA, USD-LIBOR 3M and 6M, GBP-LIBOR 3M and
  6M, CHF-LIBOR-6M and JPY-LIBOR-6M indices (absolute shift of 0.0001)
\item yield curve shifts for a bond benchmark curve in EUR (absolute shift of 0.0001)
\item FX spot sensitivities for USD, GBP, CHF, JPY against EUR as the base currency (relative shift of 0.01)
\item FX vegas for USDEUR, GBPEUR, JPYEUR volatility surfaces (relative shift of 0.01)
\item swaption vegas for the EUR surface on expiries 1Y, 5Y, 7Y, 10Y and underlying terms 1Y, 5Y, 10Y (relative shift of 0.01)
\item caplet vegas for EUR and USD on an expiry grid 1Y, 2Y, 3Y, 5Y, 7Y, 10Y and strikes 0.01, 0.02, 0.03, 0.04,
  0.05. (absolute shift of 0.0001)
\item credit curve sensitivities on tenors 6M, 1Y, 2Y, 5Y, 10Y (absolute shift of 0.0001).
\item Equity spots for S\&P500 and Lufthansa
\item Equity vegas for S\&P500 and Lufthansa at expiries 6M, 1Y, 2Y, 3Y, 5Y
\item Zero inflation curve deltas for UKRPI and EUHICPXT at tenors 6M, 1Y, 2Y, 3Y, 5Y, 7Y, 10Y, 15Y, 20Y
\item Year on year inflation curve deltas for EUHICPXT at tenors 6M, 1Y, 2Y, 3Y, 5Y, 7Y, 10Y, 15Y, 20Y
\end{itemize}

Furthermore, mixed second order derivatives (``cross gammas'') are computed for discount-discount, discount-forward and
forward-forward curves in EUR.

By definition the sensitivities are zero rate sensitivities and optionlet sensitivities, no par sensitivities are
provided. The sensitivity analysis produces three output files.

The first, {\tt scenario.csv}, contains the shift
direction ({\tt UP}, {\tt DOWN}, {\tt CROSS}), the base NPV, the scenario NPV and the difference of these two for each
trade and sensitivity key. For an overview over the possible scenario keys see \ref{sec:sensitivity}.

The second file, {\tt sensitivity.csv}, contains the shift size (in absolute terms always) and first (``Delta'') and second
(``Gamma'') order finite differences computed from the scenario results. Note that the Delta and Gamma results pure
differences, i.e. they are not divided by the shift size.

The third file, {\tt crossgamma.csv} contains second order mixed differences according to the specified cross gamma
filter, along with the shift sizes for the two factors involved. Again the reported result is not divided by the shift
sizes.

The stress scenario definition in {\tt stresstest.xml} defines two stress tests:

\begin{itemize}
\item {\tt parallel\_rates}: Rates are shifted in parallel by 0.01 (absolute). The EUR bond benchmark curve is shifted by
  increasing amounts 0.001, ..., 0.009 on the pillars 6M, ..., 20Y. FX Spots are shifted by 0.01 (relative), FX vols by
  0.1 (relative), swaption and cap floor vols by 0.0010 (absolute).
  Credit curves are not yet shifted.
\item {\tt twist}: The EUR bond benchmark curve is shifted by amounts -0.0050, -0.0040, -0.0030, -0.0020, 0.0020,
  0.0040, 0.0060, 0.0080, 0.0100 on pillars 6M, 1Y, 2Y, 3Y, 5Y, 7Y, 10Y, 15Y, 20Y.
\end{itemize}

The corresponding output file {\tt stresstest.csv} contains the base NPV, the NPV under the scenario shifts and the
difference of the two for each trade and scenario label.

%\todo[inline]{Update after CDS has been added to the example.}
\medskip
Finally, this example demonstrates a parametric VaR calculation based on the sensitivity and cross gamma output from the sensitivity analysis (deltas, vegas, gammas, cross gammas) and an external covariance matrix input. The result in {\tt var.csv} shows a breakdown by portfolio, risk class (All, Interest Rate, FX, Inflation, Equity, Credit) and risk type (All, Delta \& Gamma, Vega). The results shown are Delta Gamma Normal VaRs for the 95\% and 99\% quantile, the holding period is incorporated into the input covariances. Alternatively, one can choose a Monte Carlo VaR which means that the sensitivity based P\&L distribution is evaluated with MC simulation assuming normal respectively log-normal risk factor distribution. 
 
%--------------------------------------------------------
\subsection{Equity Derivatives Exposure}\label{ex:equityderivatives}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_16} demonstrates the computation of NPV, exposures and XVA for a portfolio 
of OTC equity derivatives. The portfolio used in this example consists of:

\begin{itemize}
	\item an equity call option denominated in EUR (``Luft'')
	\item an equity put option denominated in EUR (``Luft'')
	\item an equity forward denominated in EUR (``Luft'')
	\item an equity call option denominated in USD (``SP5'')
	\item an equity put option denominated in USD (``SP5'')
	\item an equity forward denominated in USD (``SP5'')
\end{itemize}

The step-by-step procedure for running ORE is identical for equities as for other asset classes; the same market and 
portfolio data files are used to store the equity market data and trade details, respectively. For the exposure 
simulation, the calibration parameters for the equity risk factors can be set in the usual {\tt simulation.xml} file.

Looking at the MtM results in the output file {\tt npv.csv} we observe that put-call parity ($V_{Fwd} = V_{Call} - 
V_{Put}$) is observed as expected. Looking at Figure \ref{fig_eq_call} we observe that the Expected Exposure profile of 
the equity call option trade is relatively smooth over time, while for the equity forward trade the Expected Exposure 
tends to increase as we approach maturity. This behaviour is similar to what we observe in sections \ref{sec:fxfwd} 
and \ref{sec:fxoption}. 

\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{mpl_eq_call.pdf}
	\end{center}
	\caption{Equity (``Luft'') call option and OTC forward exposure evolution, maturity in approximately 2.5 years. 
	Simulation with 
	10000 paths and quarterly time steps.}
	\label{fig_eq_call}
\end{figure}

%--------------------------------------------------------
\subsection{Inflation Swap Exposures}% Example 17
%--------------------------------------------------------

The example portfolio in folder {\tt Examples/Example\_17} contains two CPI Swaps and one Year-on-Year Inflation Swap.
The terms of the three trades are as follows:

\begin{itemize}
\item CPI Swap 1: Exchanges on 2036-02-05 a fixed amount of 20m GBP for a 10m GBP notional inflated with UKRPI with base CPI 210
\item CPI Swap 2: Notional 10m GBP, maturity 2021-07-18, exchanging GBP Libor for GBP Libor 6M vs. $2\%$ x CPI-Factor (Act/Act), with
\item YOY Swap: Notional 10m EUR, maturity 2021-02-05, exchanging fixed coupons for EUHICPXT year-on-year inflation coupons
\end{itemize}

The example generates cash flows, NPVs, exposure evolutions, XVAs, as well as two exposure graphs for CPI Swap 1 respectively the YOY Swap. Figure \ref{fig_cpi_swap} shows the CPI Swap exposure evolution.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{mpl_cpi_swap.pdf}
	\end{center}
	\caption{CPI Swap 1 exposure evolution. Simulation with 1000 paths and quarterly time steps.}
	\label{fig_cpi_swap}
\end{figure}

Figure \ref{fig_yoy_swap} shows the evolution of the 5Y maturity Year-on-Year inflation swap for comparison. Note that the inflation simulation model (Dodgson-Kainth, see appendix \ref{sec:app_rfe}) yields the evolution of inflation indices and inflation zero bonds which allows spanning future inflation zero curves and the pricing of CPI swaps. To price Year-on-Year inflation Swaps under future scenarios, we imply Year-on-Year inflation curves from zero inflation curves\footnote{Currently we discard the required (small) convexity adjustment. This will be supplemented in a subsequent release.}. Note that for pricing Year-on-Year Swaps as of today we use a separate inflation curve bootstrapped from quoted Year-on-Year inflation Swaps.
 
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{mpl_yoy_swap.pdf}
	\end{center}
	\caption{Year-on-Year Inflation Swap exposure evolution. Simulation with 1000 paths and quarterly time steps.}
	\label{fig_yoy_swap}
\end{figure}

%--------------------------------------------------------
\subsection{Bonds and Amortisation Structures}% Example 18
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_18} computes NPVs and cash flow projections for a vanilla bond portfolio
consisting of a range of bond products, in particular demonstrating amortisation features:
\begin{itemize}
\item fixed rate bond
\item floating rate bond linked to Euribor 6M
\item bond switching from fixed to floating
\item bond with 'fixed amount' amortisation
\item bond with percentage amortisation relative to the initial notional
\item bond with percentage amortisation relative to the previous notional
\item bond with fixed annuity amortisation
\item bond with floating annuity amortisation (this example needs QuantLib 1.10 or higher to work, in particular the amount() method in the Coupon class needs to be virtual)
\item bond with fixed amount amortisation followed by percentage amortisation relative to previous notional
\end{itemize}

After running the example, the results of the computation can be found in the output files {\tt npv.csv} and {\tt
  flows.csv}, respectively.

\medskip
Note that the amortisation features used here are linked to the LegData structure, hence not limited to the Bond instrument, see section \ref{ss:amortisationdata}.

%--------------------------------------------------------
\subsection{Swaption Pricing with Smile}% Example 19
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_19} demonstrates European Swaption pricing with and without smile. Calling

\medskip
\centerline{\tt python run.py}

\medskip
will launch two ORE runs using config files {\tt ore\_flat.xml} and {\tt ore\_smile.xml}, respectively. The only difference in these is referencing alternative market configurations {\tt todaymarket\_flat.xml} and {\tt todaysmarket\_smile.xml} using an ATM Swaption volatility matrix and a Swaption cube, respectively. NPV results are written to {\tt npv\_flat.cvs} and {\tt npv\_smile.csv}.

%--------------------------------------------------------
\subsection{Credit Default Swap Pricing}% Example 20
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_20} demonstrates Credit Default Swap pricing via ORE. Calling

\medskip
\centerline{\tt python run.py}

\medskip
will launch a single ORE run to process a single name CDS example and to generate NPV and cash flows in the usual result files. 

\medskip
CDS can be included in sensitivity analysis and stress testing. Exposure simulation for credit derivatives will follow in the next ORE release.

%--------------------------------------------------------
\subsection{CMS and CMS Cap/Floor Pricing}% Example 21
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_21} demonstrates the pricing of CMS and CMS Cap/Floor using a portfolio consisting of a CMS Swap (CMS leg vs. fixed leg) and a CMS Cap. Calling

\medskip
\centerline{\tt python run.py}

\medskip
will launch a single ORE run to process the portfolio and generate NPV and cash flows in the usual result files. 

\medskip
CMS structures can be included in sensitivity analysis, stress testing and exposure simulation. 

%--------------------------------------------------------
\subsection{Option Sensitivity Analysis with Smile}% Example 22
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_22} demonstrates the current state of sensitivity calculation for European options where the volatility surface has a smile. 

\medskip
The portfolio used in this example consists of
\begin{itemize}
	\item an equity call option denominated in USD (``SP5'')
	\item an equity put option denominated in USD (``SP5'')
	\item a receiver swaption in EUR
	\item an FX call option on EUR/USD
\end{itemize}

\medskip
Refer to appendix \ref{sec:app_sensi} for the current status of sensitivity implementation with smile. In this example the setup is as follows
\begin{itemize}
\item today's market is configured with volatility smile for all three products above
\item simulation market has two configurations, to simulate ``ATM only'' or the ``full surface''; ``ATM only'' means that only ATM volatilities are to be simulated and shifts to ATM vols are propagated to the respective smile section (see appendix \ref{sec:app_sensi});  
\item the sensitivity analysis has two corresponding configurations as well, ``ATM only'' and ``full surface''; note that the ``full surface'' configuration leads to explicit sensitivities by strike only in the case of Swaption volatilities, for FX and Equity volatilities only ATM sensitivity can be specified at the moment and sensitivity output is currently aggregated to the ATM bucket (to be extended in subsequent releases).
\end{itemize}

The respective output files end with ``{\tt\_fullSurface.csv}'' respectively ``{\tt\_atmOnly.csv}''.

%ORE supports two methods of simulating equity volatility smile. The first method simulates the entire surface using specific moneyness levels configured in simulation.xml. The second method simulates only the ATM equity volatilities, the other strikes are shifted relative to this new ATM using the $t_{0}$ smile.  This example compares both methods using the same sensitivity configuration as in {\tt Examples/Example\_15}. For the first method {\tt simulation\_fullSurface.xml} is used and all output files are appended with ``\_fullSurface'', for the second method {\tt simulation\_atmOnly.xml} is used and all output files are appended with ``\_atmOnly''.


%--------------------------------------------------------
\subsection{FRA and Average OIS Exposure}% Example 23
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_23} demonstrates pricing, cash flow projection and exposure simulation for two additional products
\begin{itemize}
\item Forward Rate Agreements
\item Averaging Overnight Index Swaps
\end{itemize}
using a minimal portfolio of four trades, one FRA and three OIS. The essential results are in {\tt npv.csv}, {\tt flows.csv} and 
four {\tt exposure\_trade\_*.csv} files.

\clearpage
%========================================================
\section{Launchers and Visualisation}\label{sec:visualisation}
%========================================================

\subsection{Jupyter}\label{sec:jupyter}

ORE comes with an experimental Jupyter notebook for launching ORE batches and in particular for drilling into NPV cube
data.  The notebook is located in directory {\tt FrontEnd/Python/Visualization/npvcube}. To launch the notebook, change
to this directory and follow instructions in the {\tt Readme.txt}. In a nutshell, type\footnote{With Mac OS X, you may
  need to set the environment variable {\tt LANG} to {\tt en\_US.UTF-8} before running jupyter, as mentioned in the
  installation section \ref{sec:python}.}

\medskip
\centerline{\tt jupyter notebook}
\medskip

to start the ipyton console and open a browser window. From the list of files displayed in the browser then click

\medskip
\centerline{\tt ore\_jupyter\_dashboard.ipynb} 
\medskip

to open the ORE notebook. The notebook offers
\begin{itemize}
\item launching an ORE job
\item selecting an NPV cube file and netting sets or trades therein
\item plotting a 3d exposure probability density surface
\item viewing exposure probability density function at a selected future time
\item viewing expected exposure evolution through time  
\end{itemize}

The cube file loaded here by default when processing all cells of the notebook (without changing it or launching a ORE
batch) is taken from {\tt Example\_7} (FX Forwards and FX Options).

%\todo[inline]{Add Jupyter section}

\subsection{Calc}\label{sec:calc}

ORE comes with a simple LibreOffice Calc \cite{LO} sheet as an ORE launcher and basic result viewer. This is
demonstrated on the example in section \ref{sec:example1}. It is currently based on the stable LibreOffice version 5.0.6
and tested on OS X. \\

To launch Calc, open a terminal, change to directory {\tt Examples/Example\_1}, and run

\medskip
{\centerline{\tt ./launchCalc.sh} }
\medskip

%This will show the blank sheet in figure \ref{fig_14}.
%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=0.4]{demo_calc_1}
%\end{center}
%\caption{Calc sheet after launching.}
%\label{fig_14}
%\end{figure}
The user can choose a configuration (one of the {\tt ore*.xml} files in Example\_1's subfolder Input) by hitting the
'Select' button. Initially Input/ore.xml is pre-selected. The ORE process is then kicked off by hitting 'Run'. Once
completed, standard ORE reports (NPV, Cashflow, XVA) are loaded into several sheets. Moreover, exposure evolutions can
then be viewed by hitting 'View' which shows the result in figure \ref{fig_16}.  \\
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{demo_calc_2}
\end{center}
\caption{Calc sheet after hitting 'Run'.}
\label{fig_16}
\end{figure}

This demo uses simple Libre Office Basic macros which call Python scripts to execute ORE. The Libre Office Python uno
module (which comes with Libre Office) is used to communicate between Python and Calc to upload results into the sheets.

%\todo[inline]{Remove hard-coded file names from Python scripts}
%\todo[inline]{Calc example on Windows and Linux} 
%\todo[inline]{Harmonise layout with Excel launcher} 

\subsection{Excel}\label{sec:excel}

ORE also comes with a basic Excel sheet to demonstrate launching ORE and presenting results in Excel. This demo is more
self-contained than the Calc demo in the previous section, as it uses VBA only rather than calls to external Python
scripts. The Excel demo is available in Example\_1. Launch {\tt Example\_1.xlsm}. Then modify the paths on the first
sheet, and kick off the ORE process.

%========================================================
\section{Parametrisation}\label{sec:configuration}
%========================================================

A run of ORE is kicked off with a single command line parameter 

\medskip
\centerline{\tt ore[.exe] ore.xml}
\medskip

which points to the 'master input file' referred to  as {\tt ore.xml} subsequently. 
This file is the starting point of the engine's configuration explained in the following sub section.
An overview of all input configuration files respectively all output files is shown in Table \ref{tab_1} respectively Table \ref{tab_2}.
To set up your own ORE configuration, it might be not be necessary to start from scratch, but instead use any of the examples discussed in section \ref{sec:examples} as a boilerplate and just change the folders, see section \ref{sec:master_input}, and the trade data, see section \ref{sec:portfolio_data}, together with the netting definitions, see section \ref{sec:nettingsetinput}.

\subsection{Master Input File: {\tt ore.xml}}\label{sec:master_input}

The master input file contains general setup information (paths to configuration, trade data and market data), as well
as the selection and configuration of analytics. The file has an opening and closing root element {\tt <ORE>}, {\tt
  </ORE>} with three sections
\begin{itemize}
\item Setup
\item Markets
\item Analytics
\end{itemize}
which we will explain in the following.

\subsubsection{Setup}

This subset of data is easiest explained using an example, see listing \ref{lst:ore_setup}.
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Setup>
  <Parameter name="asofDate">2016-02-05</Parameter>
  <Parameter name="inputPath">Input</Parameter>
  <Parameter name="outputPath">Output</Parameter>
  <Parameter name="logFile">log.txt</Parameter>
  <Parameter name="logMask">255</Parameter>
  <Parameter name="marketDataFile">../../Input/market_20160205.txt</Parameter>
  <Parameter name="fixingDataFile">../../Input/fixings_20160205.txt</Parameter>
  <Parameter name="implyTodaysFixings">Y</Parameter>
  <Parameter name="curveConfigFile">../../Input/curveconfig.xml</Parameter>
  <Parameter name="conventionsFile">../../Input/conventions.xml</Parameter>
  <Parameter name="marketConfigFile">../../Input/todaysmarket.xml</Parameter>
  <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
  <Parameter name="portfolioFile">portfolio.xml</Parameter>
  <!-- None, Unregister, Defer or Disable -->
  <Parameter name="observationModel">Disable</Parameter>
</Setup>
\end{minted}
%\hrule
\caption{ORE setup example}
\label{lst:ore_setup}
\end{listing}

Parameter names are self explanatory: Input and output path are interpreted relative from the directory where the ORE
executable is executed, but can also be specified using absolute paths. All file names are then interpreted relative to the
'inputPath' and 'outputPath', respectively. The files starting with {\tt ../../Input/} then point to files in the global
Example input directory {\tt Example/Input/*}, whereas files such as {\tt portfolio.xml} are local inputs in {\tt 
Example/Example\_\#/Input/}. 

Parameter {\tt logMask} determines the verbosity of log file output. Log messages are 
internally labelled as Alert, Critical, Error, Warning, Notice, Debug, associated with logMask values 1, 2, 4, 8, ..., 64. 
The logMask allows filtering subsets of these categories and controlling the verbosity of log file output\footnote{by bitwise comparison of the the external logMask value with each message's log level}. LogMask 255 ensures maximum verbosity. \\

When ORE starts, it will initialise today's market, i.e. load market data and fixings, and build all term structures as
specified in {\tt todaysmarket.xml}.  Moreover, ORE will load the trades in {\tt portfolio.xml} and link them with
pricing engines as specified in {\tt pricingengine.xml}. When parameter {\tt implyTodaysFixings} is set to Y, today's
fixings would not be loaded but implied, relevant when pricing/bootstrapping off hypothetical market data as e.g. in
scenario analysis and stress testing.

\medskip The last parameter {\tt observationModel} can be used to control ORE performance during simulation. The choices
{\em Disable } and {\em Unregister } yield similarly improved performance relative to choice {\em None}. For users
familiar with the QuantLib design - the parameter controls to which extent {\em QuantLib observer notifications} are
used when market and fixing data is updated and the evaluation date is shifted:
\begin{itemize}
\item The 'Unregister' option limits the amount of notifications by unregistering floating rate coupons from indices;
\item Option 'Defer' disables all notifications during market data and fixing updates with
{\tt ObservableSettings::instance().disableUpdates(true)}
and kicks off updates afterwards when enabled again
\item The 'Disable' option goes one step further and disables all notifications during market data and fixing updates,
  and in particular when the evaluation date is changed along a path, with \\
  {\tt ObservableSettings::instance().disableUpdates(false)} \\
  Updates are not deferred here. Required term structure and instrument recalculations are triggered explicitly.
\end{itemize}
%\todo[inline]{Expand the technical description of observationModel}

\subsubsection{Markets}\label{sec:master_input_markets}

The {\tt Markets} section (see listing \ref{lst:ore_markets}) is used to choose market configurations for calibrating
the IR, FX and EQ simulation model components, pricing and simulation, respectively. These configurations have to be 
defined
in {\tt todaysmarket.xml} (see section \ref{sec:market}).

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Markets>
  <Parameter name="lgmcalibration">collateral_inccy</Parameter>
  <Parameter name="fxcalibration">collateral_eur</Parameter>
  <Parameter name="eqcalibration">collateral_inccy</Parameter>
  <Parameter name="pricing">collateral_eur</Parameter>
  <Parameter name="simulation">collateral_eur</Parameter>
</Markets>
\end{minted}
%\hrule
\caption{ORE markets}
\label{lst:ore_markets}
\end{listing}

For example, the calibration of the simulation model's interest rate components requires local OIS discounting whereas
the simulation phase requires cross currency adjusted discount curves to get FX product pricing right. So far, the
market configurations are used only to distinguish discount curve sets, but the market configuration concept in ORE
applies to all term structure types.

\subsubsection{Analytics}\label{sec:analytics}

The {\tt Analytics} section lists all permissible analytics using tags {\tt <Analytic type="..."> ... </Analytic>} where
type can be (so far) in
\begin{itemize}
\item npv
\item cashflow
\item curves
\item simulation
\item xva
\item sensitivity
\item stress
\end{itemize}

Each {\tt Analytic} section contains a list of key/value pairs to parameterise the analysis of the form {\tt <Parameter
  name="key">value</Parameter>}. Each analysis must have one key {\tt active} set to Y or N to activate/deactivate this
analysis.  The following listing \ref{lst:ore_analytics} shows the parametrisation of the first three basic analytics in
the list above.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>    
  <Analytic type="npv">
    <Parameter name="active">Y</Parameter>
    <Parameter name="baseCurrency">EUR</Parameter>
    <Parameter name="outputFileName">npv.csv</Parameter>
  </Analytic>      
  <Analytic type="cashflow">
    <Parameter name="active">Y</Parameter>
    <Parameter name="outputFileName">flows.csv</Parameter>
  </Analytic>      
  <Analytic type="curves">
    <Parameter name="active">Y</Parameter>
    <Parameter name="configuration">default</Parameter>
    <Parameter name="grid">240,1M</Parameter>
    <Parameter name="outputFileName">curves.csv</Parameter>
  </Analytic>
  <Analytic type="...">
    <!-- ... -->
  </Analytic>      
</Analytics>      
\end{minted}
\caption{ORE analytics: npv, cashflow and curves}
\label{lst:ore_analytics}
\end{listing}

The cashflow analytic writes a report containing all future cashflows of the portfolio. Table \ref{cashflowreport} shows
a typical output for a vanilla swap.

\begin{table}[hbt]
\scriptsize
\begin{center}
  \begin{tabular}{l|l|l|l|r|l|r|r|l|r}
\hline
\#ID & Type & LegNo & PayDate & Amount & Currency & Coupon & Accrual & fixingDate & fixingValue \\
\hline
\hline
tr123 & Swap & 0 & 13/03/17 & -76212.5 & EUR & -0.00201 & 0.50556 & 08/09/16 & -0.00201 \\
tr123 & Swap & 0 & 12/09/17 & -90683.9212 & EUR & -0.002379 & 0.50833 & 09/03/17 & -0.002381 \\
\ldots
\end{tabular}
\caption{Cashflow Report}
\label{cashflowreport}
\end{center}
\end{table}

The amount column contains the projected amount including embedded caps and floors and convexity (if applicable), the
coupon column displays the corresponding rate estimation. The fixing value on the other hand is the plain fixing
projection as given by the forward value, i.e. without embedded caps and floors or convexity.

Note that the fixing value might deviate from the coupon value even for a vanilla coupon, if the QuantLib library was
compiled {\em without} the flag \verb+QL_USE_INDEXED_COUPON+ (which is the default case). In this case the coupon value
uses a par approximation for the forward rate assuming the index estimation period to be identical to the accrual
period, while the fixing value is the actual forward rate for the index estimation period, i.e. whenever the index estimation
period differs from the accrual period the values will be slightly different.

The curves analytic exports all yield curves that have been built according to the specification in {\tt
  todaysmarket.xml}. Key {\tt configuration} selects the curve set to be used (see explanation in the previous Markets
section).  Key {\tt grid} defines the time grid on which the yield curves are evaluated, in the example above a grid of
240 monthly time steps from today. The discount factors for all curves with configuration default will be exported on
this monthly grid into the csv file specified by key {\tt outputFileName}. The grid can also be specified explicitly by
a comma separated list of tenor points such as {\tt 1W, 1M, 2M, 3M, \dots}.

\medskip The purpose of the {\tt simulation} 'analytics' is to run a Monte Carlo simulation which evolves the market as
specified in the simulation config file. The primary result is an NPV cube file, i.e. valuations of all trades in the
portfolio file (see section Setup), for all future points in time on the simulation grid and for all paths. Apart from
the NPV cube, additional scenario data (such as simulated overnight rates etc) are stored in this process which are
needed for subsequent XVA analytics.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
  <Analytic type="simulation">
    <Parameter name="active">Y</Parameter>
    <Parameter name="simulationConfigFile">simulation.xml</Parameter>
    <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
    <Parameter name="baseCurrency">EUR</Parameter>
    <Parameter name="storeFlows">Y</Parameter>
    <Parameter name="cubeFile">cube_A.dat</Parameter>
    <Parameter name="additionalScenarioDataFileName">scenariodata.dat</Parameter>
    <Parameter name="scenariodump">scenariodump.csv</Parameter>
  </Analytic>
</Analytics>      
\end{minted}
\caption{ORE analytic: simulation}
\label{lst:ore_simulation}
\end{listing}

The pricing engines file specifies how trades are priced under future scenarios which can differ from pricing as of
today (specified in section Setup).  Key base currency determines into which currency all NPVs will be converted. Key
store scenarios (Y or N) determines whether the market scenarios are written to a file for later reuse. And finally, the
key `store flows' (Y or N) controls whether cumulative cash flows between simulation dates are stored in the (hyper-)
cube for post processing in the context of Dynamic Initial Margin and Variation Margin calculations. The additional
scenario data (written to the specified file here) is likewise required in the post processor step. These data comprise
simulated index fixing e.g. for collateral compounding and simulated FX rates for cash collateral conversion into base
currency. The scenario dump file, if specified here, causes ORE to write simulated market data to a human-readable csv
file.
 
\medskip The XVA analytic section offers CVA, DVA, FVA and COLVA calculations which can be selected/deselected here
individually. All XVA calculations depend on a previously generated NPV cube (see above) which is referenced here via
the {\tt cubeFile} parameter. This means one can re-run the XVA analytics without regenerating the cube each time. The
XVA reports depend in particular on the settings in the {\tt csaFile} which determines CSA details such as margining
frequency, collateral thresholds, minimum transfer amounts, margin period of risk. By splitting the processing into
pre-processing (cube generation) and post-processing (aggregation and XVA analysis) it is possible to vary these CSA
details and analyse their impact on XVAs quickly without re-generating the NPV cube.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
  <Analytic type="xva">
    <Parameter name="active">Y</Parameter>
    <Parameter name="csaFile">netting.xml</Parameter>
    <Parameter name="cubeFile">cube.dat</Parameter>
    <Parameter name="hyperCube">Y</Parameter>
    <Parameter name="scenarioFile">scenariodata.dat</Parameter>
    <Parameter name="baseCurrency">EUR</Parameter>
    <Parameter name="exposureProfiles">Y</Parameter>
    <Parameter name="quantile">0.95</Parameter>
    <Parameter name="calculationType">Symmetric</Parameter>      
    <Parameter name="allocationMethod">None</Parameter>    
    <Parameter name="marginalAllocationLimit">1.0</Parameter>
    <Parameter name="exerciseNextBreak">N</Parameter>
    <Parameter name="cva">Y</Parameter>
    <Parameter name="dva">N</Parameter>
    <Parameter name="dvaName">BANK</Parameter>
    <Parameter name="fva">N</Parameter>
    <Parameter name="fvaBorrowingCurve">BANK_EUR_BORROW</Parameter>
    <Parameter name="fvaLendingCurve">BANK_EUR_LEND</Parameter>
    <Parameter name="colva">Y</Parameter>
    <Parameter name="collateralFloor">Y</Parameter>
    <Parameter name="dim">Y</Parameter>
    <Parameter name="mva">Y</Parameter>
    <Parameter name="dimQuantile">0.99</Parameter>
    <Parameter name="dimHorizonCalendarDays">14</Parameter>
    <Parameter name="dimRegressionOrder">1</Parameter>
    <Parameter name="dimRegressors">EUR-EURIBOR-3M,USD-LIBOR-3M,USD</Parameter>
    <Parameter name="dimLocalRegressionEvaluations">100</Parameter>
    <Parameter name="dimLocalRegressionBandwidth">0.25</Parameter>
    <Parameter name="dimScaling">1.0</Parameter>
    <Parameter name="dimEvolutionFile">dim_evolution.txt</Parameter>
    <Parameter name="dimRegressionFiles">dim_regression.txt</Parameter>
    <Parameter name="dimOutputNettingSet">CPTY_A</Parameter>      
    <Parameter name="dimOutputGridPoints">0</Parameter>
    <Parameter name="rawCubeOutputFile">rawcube.csv</Parameter>
    <Parameter name="netCubeOutputFile">netcube.csv</Parameter>
    <Parameter name="fullInitialCollateralisation">true</Parameter>
  </Analytic>
</Analytics>
\end{minted}
\caption{ORE analytic: xva}
\label{lst:ore_xva}
\end{listing}

Parameters:
\begin{itemize}
\item {\tt csaFile:} Netting set definitions file covering CSA details such as margining frequency, thresholds, minimum
transfer amounts, margin period of risk
\item {\tt cubeFile:} NPV cube file previously generated and to be post-processed here
\item {\tt hyperCube:} If set to N, the cube file is expected to have depth 1 (storing NPV data only), if set to Y it is
expected to have depth $>$ 1 (e.g. storing NPVs and cumulative flows)
\item {\tt scenarioFile:} Scenario data previously generated and used in the post-processor (simulated index fixings and
FX rates)
\item {\tt baseCurrency:} Expression currency for all NPVs, value adjustments, exposures
\item {\tt exposureProfiles:} Flag to enable/disable exposure output
\item {\tt quantile} Confidence level for Potential Future Exposure (PFE) reporting
\item {\tt calculationType} Determines the settlement of margin calls: Symmetric - margin for both counterparties
settled after the margin period of risk; AsymmetricCVA - margin requested from the counterparty settles with delay,
margin requested from us settles immediately; AsymmetricDVA - vice versa). \todo[inline]{Move calculationType into the
{\tt csaFile}?}
\item {\tt allocationMethod:} XVA allocation method, choices are {\em None, Marginal, RelativeXVA}
\item {\tt marginalAllocationLimit:} The marginal allocation method a la Pykhtin/Rosen breaks down when the netting set
value vanishes while the exposure does not. This parameter acts as a cutoff for the marginal allocation when the
absolute netting set value falls below this limit and switches to equal distribution of the exposure in this case.
\item {\tt exerciseNextBreak:} Flag to terminate all trades at their next break date before aggregation and the
subsequent analytics
\item {\tt cva, dva, fva, colva, collateralFloor, dim, mva:} Flags to enable/disable these analytics. \todo[inline]{Add
collateral rates floor to the collateral model file (netting.xml)}
\item {\tt dvaName:} Credit name to look up the own default probability curve and recovery rate for DVA calculation
\item {\tt fvaBorrowingCurve:} Identifier of the borrowing yield curve
\item {\tt fvaLendingCurve:} Identifier of the lending yield curve
%\item {\tt collateralSpread:} Deviation between collateral rate and overnight rate, expressed in absolute terms (one
%basis point is 0.0001) assuming the day count convention of the collateral rate. 
\item {\tt dimQuantile:} Quantile for Dynamic Initial Margin (DIM) calculation
\item {\tt dimHorizonCalendarDays:} Horizon for DIM calculation, 14 calendar days for 2 weeks, etc.
\item {\tt dimRegressionOrder:} Order of the regression polynomial (netting set clean NPV move over the simulation
period versus netting set NPV at period start)
\item {\tt dimRegressors:} Variables used as regressors in a single- or multi-dimensional regression; these variable
  names need to match entries in the {\tt simulation.xml}'s AggregationScenarioDataCurrencies and
  AggregationScenarioDataIndices sections (only these scenario data are passed on to the post processor); if the list is
  empty, the NPV will be used as a single regressor
\item {\tt dimLocalRegressionEvaluations:} Nadaraya-Watson local regression evaluated at the given number of points to
validate polynomial regression. Note that Nadaraya-Watson needs a large number of samples for meaningful
results. Evaluating the local regression at many points (samples) has a significant performance impact, hence the option
here to limit the number of evaluations.
\item {\tt dimLocalRegressionBandwidth:} Nadaraya-Watson local regression bandwidth in standard deviations of the
independent variable (NPV)
\item {\tt dimScaling:} Scaling factor applied to all DIM values used, e.g. to reconcile simulated DIM with actual IM at
$t_0$
\item {\tt dimEvolutionFile:} Output file name to store the evolution of zero order DIM and average of nth order DIM
through time
\item {\tt dimRegressionFiles:} Output file name(s) for a DIM regression snapshot, comma separated list
\item {\tt dimOutputNettingSet:} Netting set for the DIM regression snapshot
\item {\tt dimOutputGridPoints:} Grid point(s) (in time) for the DIM regression snapshot, comma separated list
\item {\tt rawCubeOutputFile:} File name for the trade NPV cube in human readable csv file format (per trade, date,
sample)
\item {\tt netCubeOutputFile:} File name for the aggregated NPV cube in human readable csv file format (per netting set,
date, sample) {\em after} taking collateral into account
\item {\tt fullInitialCollateralisation:} If set to {\tt true}, then for every netting set, the collateral balance at $t=0$ will be set to the NPV of the setting set. The resulting effect is that EPE, ENE and PFE are all zero at $t=0$. If set to {\tt false} (default value), then the collateral balance at $t=0$ will be set to zero.
\end{itemize}

The two cube file outputs {\tt rawCubeOutputFile} and {\tt netCubeOutputFile} are provided for interactive analysis and visualisation purposes, see section
\ref{sec:visualisation}.

\medskip The {\tt sensitivity} and {\tt stress} 'analytics' provide computation of bump and revalue (zero rate
resp. optionlet) sensitivities and NPV changes under user defined stress scenarios. Listing \ref{lst:ore_sensitivity}
shows a typical configuration for sensitvity calculation.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
 <Analytic type="sensitivity">
   <Parameter name="active">Y</Parameter>
   <Parameter name="marketConfigFile">simulation.xml</Parameter>
   <Parameter name="sensitivityConfigFile">sensitivity.xml</Parameter>
   <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
   <Parameter name="scenarioOutputFile">scenario.csv</Parameter>
   <Parameter name="sensitivityOutputFile">sensitivity.csv</Parameter>
   <Parameter name="crossGammaOutputFile">crossgamma.csv</Parameter>
   <Parameter name="outputSensitivityThreshold">0.000001</Parameter>
   <Parameter name="recalibrateModels">Y</Parameter>
 </Analytic>
</Analytics>
\end{minted}
\caption{ORE analytic: sensitivity}
\label{lst:ore_sensitivity}
\end{listing}
%   <Parameter name="parRateSensitivityOutputFile">parsensi.csv</Parameter>

The parameters have the following interpretation:

\begin{itemize}
\item {\tt marketConfigFile:} Configuration file defining the simulation market under which sensitivities are computed,
  see \ref{sec:simulation}. Only a subset of the specification is needed (the one given under {\tt Market}, see
  \ref{sec:sim_market} for a detailed description).
\item {\tt sensitivityConfigFile:} Configuration file  for the sensitivity calculation, see section \ref{sec:sensitivity}.
\item {\tt pricingEnginesFile:} Configuration file for the pricing engines to be used for sensitivity calculation.
\item {\tt scenarioOutputFile:} File containing the results of the sensitivity calculation in terms of the base scenario
  NPV, the scenario NPV and their difference.
\item {\tt sensitivityOutputFile:} File containing the results of the sensitivity calculation in terms of the base scenario
  NPV, the shift size and the resulting first and (pure) second order finite differences
\item {\tt crossGammaOutputFile:} File containing the results of the sensitivity calculation in terms of the base scenario
  NPV, two shift sizes and a (mixed) second order finite difference associated to a cross gamma calculation
%\item {\tt parRateSensitivityOutputFile:} File containing par sensitivities (only available in ORE+)
\item {\tt outputSensitivityThreshold:} Only finite differences with absolute value greater than this number are written
  to the output files.
\item {\tt recalibrateModels:} If set to Y, then recalibrate pricing models after each shift of relevant term structures; otherwise do not recalibrate
\end{itemize}

The stress analytics configuration is similar to the one of the sensitivity calculation. Listing \ref{lst:ore_stress}
shows an example.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
 <Analytic type="stress">
   <Parameter name="active">Y</Parameter>
   <Parameter name="marketConfigFile">simulation.xml</Parameter>
   <Parameter name="stressConfigFile">stresstest.xml</Parameter>
   <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
   <Parameter name="scenarioOutputFile">stresstest.csv</Parameter>
   <Parameter name="outputThreshold">0.000001</Parameter>
 </Analytic>
</Analytics>
\end{minted}
\caption{ORE analytic: stress}
\label{lst:ore_stress}
\end{listing}

The parameters have the same interpretation as for the sensitivity analytic. The configuration file for the stress
scenarios is described in more detail in section \ref{sec:stress}.

\medskip The {\tt VaR} 'analytics' provide computation of Value-at-Risk measures based on the sensitivity (delta, gamma, cross gamma) data above. Listing \ref{lst:ore_var} shows a configuration example.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
    <Analytic type="parametricVar"> 
      <Parameter name="active">Y</Parameter> 
      <Parameter name="portfolioFilter">PF1|PF2</Parameter>
      <Parameter name="sensitivityInputFile">
         ../Output/sensitivity.csv,../Output/crossgamma.csv
      </Parameter> 
      <Parameter name="covarianceInputFile">covariance.csv</Parameter> 
      <Parameter name="salvageCovarianceMatrix">N</Parameter>
      <Parameter name="quantiles">0.01,0.05,0.95,0.99</Parameter> 
      <Parameter name="breakdown">Y</Parameter> 
      <!-- Delta, DeltaGammaNormal, MonteCarlo --> 
      <Parameter name="method">DeltaGammaNormal</Parameter> 
      <Parameter name="mcSamples">100000</Parameter> 
      <Parameter name="mcSeed">42</Parameter> 
      <Parameter name="outputFile">var.csv</Parameter> 
    </Analytic> </Analytics>
\end{minted}
\caption{ORE analytic: VaR}
\label{lst:ore_var}
\end{listing}

The parameters have the following interpretation:

\begin{itemize}
\item {\t portfolioFilter:} Regular expression used to filter the portfolio for which VaR is computed; if the filter is not provided, then the full portfolio is processed
\item {\tt sensitivityInputFile:} Reference to the sensitivity (deltas, vegas, gammas) and cross gamma input as generated by ORE in a comma separated list
\item {\tt covarianceFile:} Reference to the covariances input data; these are currently not calculated in ORE and need to be provided externally, in a blank/tab/comma separated file with three columns (factor1, factor2, covariance), where factor1 and factor2 follow the naming convention used in ORE's sensitivity and cross gamma output files. Covariances need to be consistent with the sensitivity data provided. For example, if sensitivity to factor1 is computed by absolute shifts and expressed in basis points, then the covariances with factor1 need to be based on absolute basis point shifts of factor1; if sensitivity is due to a relative factor1 shift of 1\%, then covariances with factor1 need to be based on relative shifts expressed in percentages to, etc. Also note that covariances are expected to include the desired holding period, i.e. no scaling with square root of time etc is performed in ORE; 
\item {\tt salvageCovarianceMatrix:} If set to Y, turn the input covariance matrix into a valid (positive definite) matrix applying a Salvaging algorithm; if set to N, throw an exception if the matrix is not positive definite
\item {\tt quantiles:} Several desired quantiles can be specified here in a comma separated list; these lead to several columns of results in the output file, see below. Note that e.g. the 1\% quantile corresponds to the lower tail of the P\&L distribution (VaR), 99\% to the upper tail.
\item {\tt breakdown:} If yes, VaR is computed by portfolio, risk class (All, Interest Rate, FX, Inflation, Equity, Credit) and risk type (All, Delta \& Gamma, Vega)
\item {\tt method:} Choices are {\em Delta, DeltaGammaNormal, MonteCarlo}, see appendix \ref{sec:app_var}
\item {\tt mcSamples:} Number of Monte Carlo samples used when the {\em MonteCarlo} method is chosen 
\item {\tt mcSeed:} Random number generator seed when the {\em MonteCarlo} method is chosen
\item {\tt outputFile:} Output file name
\end{itemize}

%--------------------------------------------------------
\subsection{Market: {\tt todaysmarket.xml}}\label{sec:market}
%--------------------------------------------------------

This configuration file determines the subset of the 'market' universe which is going to be built by ORE. It is the
user's responsibility to make sure that this subset is sufficient to cover the portfolio to be analysed. If it is not,
the application will complain at run time and exit.

\medskip We assume that the market configuration is provided in file {\tt todaysmarket.xml}, however, the file name can
be chosen by the user. The file name needs to be entered into the master configuration file {\tt ore.xml}, see section
\ref{sec:master_input}.

\medskip 
The file starts and ends with the opening and closing tags {\tt <TodaysMarket>} 
and {\tt </TodaysMarket>}. The file then contains configuration blocks for
\begin{itemize}
\item Discounting curves
\item Index curves (to project index fixings)
\item Yield curves (for other purposes, e.g. as benchmark curve for bond pricing)
\item Swap index curves (to project Swap rates)
\item FX spot rates
\item Inflation index curves (to project zero or yoy inflation fixings)
\item Equity curves (to project forward prices)
\item Default curves
\item Swaption volatility structures
\item Cap/Floor volatility structures
\item FX volatility structures
\item Inflation Cap/Floor price surfaces
\item Equity volatility structures
\item CDS volatility structures
\item Base correlation structures
\item Securities
\end{itemize}

There can be alternative versions of each block each labeled with a unique identifier (e.g. Discount curve block with ID
'default', discount curve block with ID 'ois', another one with ID 'xois', etc). The purpose of these IDs will be
explained at the end of this section. We now discuss each block's layout.

\subsubsection{Discounting Curves} 

We pick one discounting curve block as an example here (see {\tt Examples/Input/todaysmarket.xml}), the one with ID 'ois' 

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <DiscountingCurves Id="ois">
    <DiscountingCurve Currency="EUR">Yield/EUR/EUR1D</DiscountingCurve>
    <DiscountingCurve Currency="USD">Yield/USD/USD1D</DiscountingCurve>
    <DiscountingCurve Currency="GBP">Yield/GBP/GBP1D</DiscountingCurve>
    <DiscountingCurve Currency="CHF">Yield/CHF/CHF6M</DiscountingCurve>
    <DiscountingCurve Currency="JPY">Yield/JPY/JPY6M</DiscountingCurve>
    <!-- ... -->
  </DiscountingCurves>
\end{minted}
\caption{Discount curve block with ID 'ois'}
\label{lst:discountcurve_spec}
\end{listing}

This block instructs ORE to build five discount curves for the indicated currencies. The string within the tags,
e.g. Yield/EUR/EUR1D, uniquely identifies the curve to be built.  Curve Yield/EUR/EUR1D is defined in the curve
configuration file explained in section \ref{sec:curveconfig} below. In this case ORE is instructed to build an Eonia
Swap curve made of Overnight Deposit and Eonia Swap quotes. The right most token of the string Yield/EUR/EUR1D (EUR1D)
is user defined, the first two tokens Yield/EUR have to be used to point to a yield curve in currency EUR.
 
\subsubsection{Index Curves} 

See an excerpt of the index curve block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<IndexForwardingCurves Id="default">
  <Index Name="EUR-EURIBOR-3M">Yield/EUR/EUR3M</Index>
  <Index Name="EUR-EURIBOR-6M">Yield/EUR/EUR6M</Index>
  <Index Name="EUR-EURIBOR-12M">Yield/EUR/EUR6M</Index>
  <Index Name="EUR-EONIA">Yield/EUR/EUR1D</Index>
  <Index Name="USD-LIBOR-3M">Yield/USD/USD3M</Index>
  <!-- ... -->
</IndexForwardingCurves>
\end{minted}
\caption{Index curve block with ID 'default'}
\label{lst:indexcurve_spec}
\end{listing}

This block of curve specifications instructs ORE to build another set of yield curves, unique strings
(e.g. Yield/EUR/EUR6M etc.) point to the {\tt curveconfig.xml} file where these curves are defined. Each curve is then
associated with an index name (of format Ccy-IndexName-Tenor, e.g. EUR-EURIBOR-6M) so that ORE will project the
respective index using the selected curve (e.g. Yield/EUR/EUR6M).

\subsubsection{Yield Curves}

See an excerpt of the yield curve block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<YieldCurves id="default">
  <YieldCurve name="BANK_EUR_LEND">Yield/EUR/BANK_EUR_LEND</YieldCurve>
  <YieldCurve name="BANK_EUR_BORROW">Yield/EUR/BANK_EUR_BORROW</YieldCurve>
  <!-- ... -->
</YieldCurves>
\end{minted}
\caption{Yield curve block with ID 'default'}
\label{lst:yieldcurve_spec}
\end{listing}

This block of curve specifications instructs ORE to build another set of yield curves, unique strings
(e.g. Yield/EUR/EUR6M etc.) point to the {\tt curveconfig.xml} file where these curves are defined. Other than
discounting and index curves the yield curves in this block are not tied to a particular purpose. The curves defined in
this block typically include

\begin{itemize}
\item additional curves needed in the XVA post processor, e.g. for the FVA calculation
\item benchmark curves used for bond pricing
\end{itemize}

\subsubsection{Swap Index Curves}

The following is an excerpt of the swap index curve block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<SwapIndexCurves Id="default">
  <SwapIndex Name="EUR-CMS-1Y">
    <Index>EUR-EURIBOR-6M</Index>
    <Discounting>EUR-EONIA</Discounting>
  </SwapIndex>
  <SwapIndex Name="EUR-CMS-30Y">
    <Index>EUR-EURIBOR-6M</Index>
    <Discounting>EUR-EONIA</Discounting>
  </SwapIndex>
  <!-- ... -->
</SwapIndexCurves>
\end{minted}
\caption{Swap index curve block with ID 'default'}
\label{lst:swapindexcurve_spec}
\end{listing}

These instructions do not build any additional curves. They only build the respective swap index objects and associate
them with the required index forwarding and discounting curves already built above. This enables a swap index to project
the fair rate of forward starting Swaps. Swap indices are also containers for conventions. Swaption volatility surfaces
require two swap indices each available in the market object, a long term and a short term swap index. The curve
configuration file below will show that in particular the required short term index has term 1Y, and the required long
term index has 30Y term. This is why we build these two indices at this point.

\subsubsection{FX Spot}

The following is an excerpt of the FX spot block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<FxSpots Id="default">
  <FxSpot Pair="EURUSD">FX/EUR/USD</FxSpot>
  <FxSpot Pair="EURGBP">FX/EUR/GBP</FxSpot>
  <FxSpot Pair="EURCHF">FX/EUR/CHF</FxSpot>
  <FxSpot Pair="EURJPY">FX/EUR/JPY</FxSpot>
  <!-- ... -->
</FxSpots>
\end{minted}
\caption{FX spot block with ID 'default'}
\label{lst:fxspot_spec}
\end{listing}

This block instructs ORE to provide four FX quotes, all quoted with target currency EUR so
that foreign currency amounts can be converted into EUR via multiplication with that rate.
 
\subsubsection{FX Volatilities}

The following is an excerpt of the FX Volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<FxVolatilities Id="default">
  <FxVolatility Pair="EURUSD">FXVolatility/EUR/USD/EURUSD</FxVolatility>
  <FxVolatility Pair="EURGBP">FXVolatility/EUR/GBP/EURGBP</FxVolatility>
  <FxVolatility Pair="EURCHF">FXVolatility/EUR/CHF/EURCHF</FxVolatility>
  <FxVolatility Pair="EURJPY">FXVolatility/EUR/JPY/EURJPY</FxVolatility>
  <!-- ... -->
</FxVolatilities>
\end{minted}
\caption{FX volatility block with ID 'default'}
\label{lst:fxvol_spec}
\end{listing}

This instructs ORE to build four FX volatility structures for all FX pairs with target currency EUR, see curve
configuration file for the definition of the volatility structure.

\subsubsection{Swaption Volatilities}

The following is an excerpt of the Swaption Volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<SwaptionVolatilities Id="default">
  <SwaptionVolatility Currency="EUR">SwaptionVolatility/EUR/EUR_SW_N</SwaptionVolatility>
  <SwaptionVolatility Currency="USD">SwaptionVolatility/USD/USD_SW_N</SwaptionVolatility>
  <SwaptionVolatility Currency="GBP">SwaptionVolatility/GBP/GBP_SW_N</SwaptionVolatility>
  <SwaptionVolatility Currency="CHF">SwaptionVolatility/CHF/CHF_SW_N</SwaptionVolatility>
  <SwaptionVolatility Currency="JPY">SwaptionVolatility/CHF/JPY_SW_N</SwaptionVolatility>
</SwaptionVolatilities>
\end{minted}
\caption{Swaption volatility block with ID 'default'}
\label{lst:swaptionvol_spec}
\end{listing}

This instructs ORE to build five Swaption volatility structures, see the curve configuration file for the definition of
the volatility structure. The latter token (e.g. EUR\_SW\_N) is user defined and will be found in the curve
configuration's CurveId tag.

\subsubsection{Cap/Floor Volatilities}

The following is an excerpt of the Cap/Floor Volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CapFloorVolatilities id="default">
  <CapFloorVolatility currency="EUR">CapFloorVolatility/EUR/EUR_CF_N</CapFloorVolatility>
  <CapFloorVolatility currency="USD">CapFloorVolatility/USD/USD_CF_N</CapFloorVolatility>
  <CapFloorVolatility currency="GBP">CapFloorVolatility/GBP/GBP_CF_N</CapFloorVolatility>
</CapFloorVolatilities>
\end{minted}
\caption{Cap/Floor volatility block with ID 'default'}
\label{lst:capfloorvol_spec}
\end{listing}

This instructs ORE to build three Cap/Floor volatility structures, see the curve configuration file for the definition
of the volatility structure. The latter token (e.g. EUR\_CF\_N) is user defined and will be found in the curve
configuration's CurveId tag.

\subsubsection{Default Curves}

The following is an excerpt of the Default Curves block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<DefaultCurves Id="default">
  <DefaultCurve Name="BANK">Default/USD/BANK_SR_USD</DefaultCurve>
  <DefaultCurve Name="CPTY_A">Default/USD/CUST_A_SR_USD</DefaultCurve>
  <DefaultCurve Name="CPTY_B">Default/USD/CUST_A_SR_USD</DefaultCurve>
  <!-- ... -->
</DefaultCurves>
\end{minted}
\caption{Default curves block with ID 'default'}
\label{lst:defaultcurve_spec}
\end{listing}

This instructs ORE to build a set of default probability curves, again defined in the curve configuration file. Each
curve is then associated with a name (BANK, CUST\_A) for subsequent lookup.  As before, the last token
(e.g. BANK\_SR\_USD) is user defined and will be found in the curve configuration's CurveId tag.

\subsubsection{Securities}\label{sssec:securities}

The following is an excerpt of the Security block with ID 'default' from the same example file:

\begin{listing}[H]
	%\hrule\medskip
	\begin{minted}[fontsize=\footnotesize]{xml}
<Securities id="default">
  <Security name="SECURITY_1">Security/SECURITY_1</Security>
</Securities>
	\end{minted}
	\caption{Securities block with ID 'default'}
	\label{lst:secspread_spec}
\end{listing}

The pricing of bonds includes (among other components) a security specific spread and rate. 
This block links a security name to a spread and rate pair defined in the curve configuration file. This name may then be referenced 
as the security id in the bond trade definition.

\subsubsection{Equity Curves}
The following is an excerpt of the Equity curves block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<EquityCurves id="default">
  <EquityCurve name="SP5">Equity/USD/SP5</EquityCurve>
  <EquityCurve name="Lufthansa">Equity/EUR/Lufthansa</EquityCurve>
</EquityCurves>
\end{minted}
\caption{Equity curves block with ID 'default'}
\label{lst:eqcurve_spec}
\end{listing}

This instructs ORE to build a set of equity curves, again defined in the curve configuration file. Each equity curve 
after construction will consist of a spot equity price, as well as a term structure of dividend yields, which can be 
used to determine forward prices. This object is then associated with a name (e.g. SP5) for subsequent lookup. 

\subsubsection{Equity Volatilities}

The following is an excerpt of the equity volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<EquityVolatilities id="default">
  <EquityVolatility name="SP5">EquityVolatility/USD/SP5</EquityVolatility>
  <EquityVolatility name="Lufthansa">EquityVolatility/EUR/Lufthansa</EquityVolatility>
</EquityVolatilities>
\end{minted}
\caption{EQ volatility block with ID 'default'}
\label{lst:eqvol_spec}
\end{listing}

This instructs ORE to build two equity volatility structures for SP5 and Lufthansa, respectively. See the curve
configuration file for the definition of the equity volatility structure.


\subsubsection{Inflation Index Curves}

The following is an excerpt of the Zero Inflation Index Curves block with ID 'default' from the sample example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<ZeroInflationIndexCurves id="default">
    <ZeroInflationIndexCurve name="EUHICPXT">
        Inflation/EUHICPXT/EUHICPXT_ZC_Swaps
    </ZeroInflationIndexCurve>
    <ZeroInflationIndexCurve name="FRHICP">
        Inflation/FRHICP/FRHICP_ZC_Swaps
    </ZeroInflationIndexCurve>
    <ZeroInflationIndexCurve name="UKRPI">
        Inflation/UKRPI/UKRPI_ZC_Swaps
    </ZeroInflationIndexCurve>
    <ZeroInflationIndexCurve name="USCPI">
        Inflation/USCPI/USCPI_ZC_Swaps
    </ZeroInflationIndexCurve>
    ...
</ZeroInflationIndexCurves>
\end{minted}
\caption{Zero Inflation Index Curves block with ID 'default'}
\label{lst:zeroinflationindexcurve_spec}
\end{listing}

This instructs ORE to build a set of zero inflation index curves, which are defined in the curve configuration
file. Each curve is then associated with an index name (like e.g. EUHICPXT or UKRPI). The last token
(e.g. EUHICPXT\_ZC\_Swap) is user defined and will be found in the curve configuration's CurveId tag.

In a similar way, Year on Year index curves are specified:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <YYInflationIndexCurves id="default">
      <YYInflationIndexCurve name="EUHICPXT">
          Inflation/EUHICPXT/EUHICPXT_YY_Swaps
      </YYInflationIndexCurve>
      ...
  </YYInflationIndexCurves>
\end{minted}
\caption{YoY Inflation Index Curves block with ID 'default'}
\label{lst:yoyinflationindexcurve_spec}
\end{listing}

Note that the index name is the same as in the corresponding zero index curve definition, but the token corresponding to
the CurveId tag is different. This is because the actual underlying index (and in particular its fixings) are shared
between the two index types, while different projection curves are used to forecast future index realisations.

\subsubsection{Inflation Cap/Floor Price Surfaces}

The following is an excerpt of the Inflation Cap/Floor Price Surfaces block with ID 'default' from the sample example
file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<InflationCapFloorPriceSurfaces id="default">
    <InflationCapFloorPriceSurface name="EUHICPXT">
        InflationCapFloorPrice/EUHICPXT/EUHICPXT_ZC_CF
    </InflationCapFloorPriceSurface>
    <InflationCapFloorPriceSurface name="USCPI">
        InflationCapFloorPrice/USCPI/USCPI_ZC_CF
    </InflationCapFloorPriceSurface>
    <InflationCapFloorPriceSurface name="UKRPI">
        InflationCapFloorPrice/UKRPI/UKRPI_ZC_CF
    </InflationCapFloorPriceSurface>
</InflationCapFloorPriceSurfaces>
\end{minted}
\caption{Inflation Cap/Floor Price Surfaces block with ID 'default'}
\label{lst:yoyinflationindexcurve_spec}
\end{listing}

This instructs ORE to build a set of zero inflation cap floor price surfaces, which are defined in the curve
configuration file. Each surface is associated with an idnex name. The last token (e.g. EUHICPXT\_ZC\_CF) is user defined
and will be found in the curve configuration's CurveId tag.

Currently only zero coupon surfaces are supported, year on year surfaces are not supported.

\subsubsection{CDS Volatility Structures}

CDS volatility structures are configured as follows
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <CDSVolatilities id="default">
   <CDSVolatility name="CDSVOL_A">CDSVolatility/CDXIG</CDSVolatility>
   <CDSVolatility name="CDSVOL_B">CDSVolatility/CDXHY</CDSVolatility>
  </CDSVolatilities>
\end{minted}
\caption{CDS volatility structure block with ID 'default'}
\label{lst:cdsvol_spec}
\end{listing}

The composition of the CDS volatility structures is defined in the curve configuration.

\subsubsection{Base Correlation Structures}

Base correlation structures are configured as follows
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <BaseCorrelations id="default">
   <BaseCorrelation name="CDXIG">BaseCorrelation/CDXIG</BaseCorrelation>
  </BaseCorrelations>
\end{minted}
\caption{Base Correlations block with ID 'default'}
\label{lst:basecorr_spec}
\end{listing}

The composition of the base correlation structure is defined in the curve configuration.

\subsubsection{Market Configurations}

Finally, representatives of each type of block (Discount Curves, Index Curves, Volatility structures, etc, up to
Inflation Cap/Floor Price Surfaces) can be bundled into a market configuration. This is done by adding the following to
the {\tt todaysmarket.xml} file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Configuration Id="default">
  <DiscountingCurvesId>xois_eur</DiscountingCurvesId>
</Configuration>
<Configuration Id="collateral_inccy">
  <DiscountingCurvesId>ois</DiscountingCurvesId>
</Configuration>
<Configuration Id="collateral_eur">
  <DiscountingCurvesId>xois_eur</DiscountingCurvesId>
</Configuration>
<Configuration Id="libor">
  <DiscountingCurvesId>inccy_swap</DiscountingCurvesId>
</Configuration>
\end{minted}
\caption{Market configurations}
\label{lst:config_spec}
\end{listing}

When ORE constructs the market object, all market configurations will be build and labelled using the 'Configuration
Id'.  This allows configuring a market setup for different alternative purposes side by side in the same {\tt
  todaysmarket.xml} file. Typical use cases are
\begin{itemize}
\item different discount curves needed for model calibration and risk factor evolution, respectively
\item different discount curves needed for collateralised and uncollateralised derivatives pricing.
\end{itemize}
The former is actually used throughout the {\tt Examples} section. Each master input file {\tt ore.xml} has a Markets
section (see \ref{sec:master_input}) where four market configuration IDs have to be provided - the ones used for
'lgmcalibration', 'fxcalibration', 'pricing' and 'simulation' (i.e. risk factor evolution).

\medskip The configuration ID concept extends across all curve and volatility objects though currently used only to
distinguish discounting.
 
%--------------------------------------------------------
\subsection{Pricing Engines: {\tt pricingengine.xml}}
%--------------------------------------------------------

The pricing engine configuration file is provided to select pricing models and pricing engines by product type. The
following is an overview over the Example section's {\tt pricingengine.xml}. Further below we discuss the Bermudan Swaption engine parametrisation in more detail.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<PricingEngines>
  <Product type="Swap">
    <Model>DiscountedCashflows</Model>
    <ModelParameters/>
    <Engine>DiscountingSwapEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="CrossCurrencySwap">
    <Model>DiscountedCashflows</Model>
    <ModelParameters/>
    <Engine>DiscountingCrossCurrencySwapEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="FxForward">
    <Model>DiscountedCashflows</Model>
    <ModelParameters/>
    <Engine>DiscountingFxForwardEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="FxOption">
    <Model>GarmanKohlhagen</Model>
    <ModelParameters/>
    <Engine>AnalyticEuropeanEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="EuropeanSwaption">
    <Model>BlackBachelier</Model> <!-- depends on input vol -->
    <ModelParameters/>
    <Engine>BlackBachelierSwaptionEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="Bond">
    <Model>DiscountedCashflows</Model>
    <ModelParameters/>
    <Engine>DiscountingRiskyBondEngine</Engine>
    <EngineParameters>
      <Parameter name="TimestepPeriod">6M</Parameter>
    </EngineParameters>
  </Product>
  <Product type="BermudanSwaption">
    <Model>LGM</Model>
    <ModelParameters>
      <Parameter name="Calibration">Bootstrap</Parameter>
      <Parameter name="BermudanStrategy">CoterminalATM</Parameter>
      <Parameter name="Reversion">0.03</Parameter>
      <Parameter name="ReversionType">HullWhite</Parameter>
      <Parameter name="Volatility">0.01</Parameter>
      <Parameter name="VolatilityType">Hagan</Parameter>
      <Parameter name="ShiftHorizon">0.5</Parameter>
      <Parameter name="Tolerance">0.0001</Parameter>
    </ModelParameters>
    <Engine>Grid</Engine>
    <EngineParameters>
      <Parameter name="sy">3.0</Parameter>
      <Parameter name="ny">10</Parameter>
      <Parameter name="sx">3.0</Parameter>
      <Parameter name="nx">10</Parameter>
    </EngineParameters>
  </Product>
  <Product type="CapFloor">
    <Model>IborCapModel</Model>
    <ModelParameters/>
    <Engine>IborCapEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="CapFlooredIborLeg">
    <Model>BlackOrBachelier</Model>
    <ModelParameters/>
    <Engine>BlackIborCouponPricer</Engine>
    <EngineParameters/>
  </Product>
  <Product type="EquityForward">
    <Model>DiscountedCashflows</Model>
    <ModelParameters/>
    <Engine>DiscountingEquityForwardEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="EquityOption">
    <Model>BlackScholesMerton</Model>
    <ModelParameters/>
    <Engine>AnalyticEuropeanEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="Bond">
    <Model>DiscountedCashflows</Model>
    <ModelParameters/>
    <Engine>DiscountingRiskyBondEngine</Engine>
    <EngineParameters>
      <Parameter name="TimestepPeriod">6M</Parameter>
    </EngineParameters>
  </Product>
  <Product type="CreditDefaultSwap">
    <Model>DiscountedCashflows</Model>
    <ModelParameters/>
    <Engine>MidPointCdsEngine</Engine>
    <EngineParameters/>
  </Product>
  <Product type="CMS">
    <Model>Hagan</Model><!-- or LinearTSR -->
    <ModelParameters/>
    <Engine>Analytic</Engine> <!-- or Numerical -->
    <EngineParameters>
      <!-- Alternative Yield Curve Models: ExactYield, ParallelShifts, NonParallelShifts -->
      <Parameter name="YieldCurveModel">Standard</Parameter> 
      <Parameter name="MeanReversion">0.0</Parameter>
    </EngineParameters>
  </Product>
</PricingEngines>
\end{minted}
\caption{Pricing engine configuration}
\label{lst:pricingengine_config}
\end{longlisting}

These settings will be taken into account when the engine factory is asked to build the respective pricing engines and required models, and to calibrate the required model.

\medskip
For example, in case of the Bermudan Swaption, the parameters are interpreted as follows:

\begin{itemize}
\item The only model currently supported for Bermudan Swaption pricing is the LGM selected here. 

\item The first block of model parameters then provides initial values for the model (Reversion, Volatility) and chooses
  the parametrisation of the LGM model with ReversionType and VolatilityType choices {\em HullWhite} and {\em
    Hagan}. Calibration and BermudanStrategy can be set to {\em None} in order to skip model calibration. Alternatively,
  Calibration is set to {\em Bootstrap} and BermudanStrategy to {\em CoterminalATM} in order to calibrate to
  instrument-specific co-terminal ATM Swaptions, i.e. chosen to match the instruments first expiry and final maturity.
  If {\em CoterminalDealStrike} is chosen, the co-terminal swaptions will match the fixed rate of the deal (if the deal
  has changing fixed rates, the first rate is matched). Finally if the ShiftHorizon parameter is given, its value times
  the remaining maturity time of the deal is chosen as the horizon shift parameter for the LGM model. If not given, this
  parameter defaults to $0.5$.

\item The second block of engine parameters specifies the Numerical Swaption engine parameters which determine the
  number of standard deviations covered in the probability density integrals (sy and sx), and the number of grid points
  used per standard deviation (ny and nx).
\end{itemize}

To see the configuration options for the alternative CMS engines (Hagan Numerical, LinearTSR), please refer to the 
commented parts in {\tt Examples/Input/pricingengine.xml}.

\medskip
This file is relevant in particular for structured products which are on the roadmap of future ORE releases. But it is also
intended to allow the selection of optimised pricing engines for vanilla products such as Interest Rate Swaps.
 
%--------------------------------------------------------
\subsection{Simulation: {\tt simulation.xml}}\label{sec:simulation}
%--------------------------------------------------------

This file determines the behaviour of the risk factor simulation (scenario generation) module.
It is structured in three blocks of data.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Simulation>
  <Parameters> ... </Parameters>
  <CrossAssetModel> ... </CrossAssetModel>
  <Market> ... </Market>
</Simulation>
\end{minted}
\caption{Simulation configuration}
\label{lst:simulation_configuration}
\end{listing}

Each of the three blocks is sketched in the following.

\subsubsection{Parameters}\label{sec:sim_params}

Let us discuss this section using the following example

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Parameters>
  <Discretization>Exact</Discretization>
  <Grid>80,3M</Grid>
  <Calendar>EUR,USD,GBP,CHF</Calendar>
  <Sequence>SobolBrownianBridge</Sequence>
  <Seed>42</Seed>
  <Samples>1000</Samples>
</Parameters>
\end{minted}
\caption{Simulation configuration}
\label{lst:simulation_params_configuration}
\end{listing}

\begin{itemize}
\item {\tt Discretization:} Chooses between time discretization schemes for the risk factor evolution. {\em Exact} means
exploiting the analytcal tractability of the model to avoid any time discretization error. {\em Euler} uses a naive time
discretization scheme which has numerical error and requires small time steps for accurate results (useful for testing
purposes)
\item {\tt Grid:} Specifies the simulation time grid, here 80 quarterly steps.\footnote{For exposure calculation under DIM, the second parameter has to match the Margin Period of Risk, i.e. if {\tt MarginPeriodOfRisk} is set to for instance {\tt 2W} in a netting set definition in {\tt netting.xml}, then one has to set {\tt Grid} to for instance {\tt 80,2W}.}
\item {\tt Calendar:} Calendar or combination of calendars used to adjust the dates of the grid. Date adjustment is
required because the simulation must step over 'good' dates on which index fixings can be stored.
%\item {\tt Scenario: } Choose between {\em Simple } and {\em Complex } implementations, the latter optimized for
% more efficient memory usage. \todo[inline]{Remove Scenario choice}
\item {\tt Sequence:} Choose random sequence generator ({\em MersenneTwister, MersenneTwisterAntithetic, Sobol,
SobolBrownianBridge}).
\item {\tt Seed:} Random number generator seed
\item {\tt Samples:} Number of Monte Carlo paths to be produced
%\item {\tt Fixings: } Choose whether fixings should be simulated or not, and if so which fixing simulation method to
use ({\em Backward, Forward, BestOfForwardBackward, InterpolatedForwardBackward}), which number of forward horizon days
to use if one of the {\em Forward } related methods is chosen.
\end{itemize}

\subsubsection{Model}\label{sec:sim_model}

The {\tt CrossAssetModel} section determines the cross asset model's number of currencies covered, composition, and each
component's calibration. It is currently made of 
\begin{itemize}
\item a sequence of LGM models for each currency (say $n_c$ currencies), 
\item $n_c-1$ FX models for each exchange rate to the base currency, 
\item $n_e$ equity models,
\item $n_i$ inflation models, 
\item a specification of the correlation structure between all components.
\end{itemize}

\medskip The simulated currencies are specified as follows, with clearly identifying the domestic currency which is also
the target currency for all FX models listed subsequently. If the portfolio requires more currencies to be simulated,
this will lead to an exception at run time, so that it is the user's responsibility to make sure that the list of
currencies here is sufficient. The list can be larger than actually required by the portfolio. This will not lead to any
exceptions, but add to the run time of ORE.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>
  <DomesticCcy>EUR</DomesticCcy>
  <Currencies>
    <Currency>EUR</Currency>
    <Currency>USD</Currency>
    <Currency>GBP</Currency>
    <Currency>CHF</Currency>
    <Currency>JPY</Currency>
  </Currencies>
  <BootstrapTolerance>0.0001</BootstrapTolerance>
  <!-- ... -->
</CrossAssetModel>
\end{minted}
\caption{Simulation model currencies configuration}
\label{lst:simulation_model_currencies_configuration}
\end{listing}
 
Bootstrap tolerance is a global parameter that applies to the calibration of all model components. If the calibration
error of any component exceeds this tolerance, this will trigger an exception at runtime, early in the ORE process.

\medskip

Each interest rate model is specified by a block as follows

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <InterestRateModels>
    <LGM ccy="default">
      <CalibrationType>Bootstrap</CalibrationType>
      <Volatility>
        <Calibrate>Y</Calibrate>
        <VolatilityType>Hagan</VolatilityType>
        <ParamType>Piecewise</ParamType>
        <TimeGrid>1.0,2.0,3.0,4.0,5.0,7.0,10.0</TimeGrid>
        <InitialValue>0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01<InitialValue>
      </Volatility>
      <Reversion>
        <Calibrate>N</Calibrate>
        <ReversionType>HullWhite</ReversionType>
        <ParamType>Constant</ParamType>
        <TimeGrid/>
        <InitialValue>0.03</InitialValue>
      </Reversion>
      <CalibrationSwaptions>
        <Expiries>1Y,2Y,4Y,6Y,8Y,10Y,12Y,14Y,16Y,18Y,19Y</Expiries>
        <Terms>19Y,18Y,16Y,14Y,12Y,10Y,8Y,6Y,4Y,2Y,1Y</Terms>
        <Strikes/>
      </CalibrationSwaptions>
      <ParameterTransformation>
        <ShiftHorizon>0.0</ShiftHorizon>
        <Scaling>1.0</Scaling>
      </ParameterTransformation>
    </LGM>
    <LGM ccy="EUR">
      <!-- ... -->
    </LGM>
    <LGM ccy="USD">
      <!-- ... -->
    </LGM>
  </InterestRateModels>	
  <!-- ... -->		
</CrossAssetModel>
\end{minted}
\caption{Simulation model IR configuration}
\label{lst:simulation_model_ir_configuration}
\end{listing}

We have LGM sections by currency, but starting with a section for currency 'default'. As the name implies, this is used
as default configuration for any currency in the currency list for which we do not provide an explicit
parametrisation. Within each LGM section, the interpretation of elements is as follows:

\begin{itemize}
\item {\tt CalibrationType: } Choose between {\em Bootstrap} and {\em BestFit}, where Bootstrap is chosen when we expect
to be able to achieve a perfect fit (as with calibration of piecewise volatility to a series of co-terminal Swaptions)
\item {\tt Volatility/Calibrate: } Flag to enable/disable calibration of this particular parameter
\item {\tt Volatility/VolatilityType: } Choose volatility parametrisation a la {\em HullWhite} or {\em Hagan}
\item {\tt Volatility/ParamType: } Choose between {\em Constant} and {\em Piecewise}
\item {\tt Volatility/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
\item {\tt Volatility/InitialValue: } Vector of initial values, matching number of entries in time, or single value if
the time grid is empty
\item {\tt Reversion/Calibrate: } Flag to enable/disable calibration of this particular parameter
\item {\tt Reversion/VolatilityType: } Choose reversion parametrisation a la {\em HullWhite} or {\em Hagan}
\item {\tt Reversion/ParamType: } Choose between {\em Constant} and {\em Piecewise}
\item {\tt Reversion/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
\item {\tt Reversion/InitialValue: } Vector of initial values, matching number of entries in time, or single value if
the time grid is empty
\item {\tt CalibrationSwaptions: } Choice of calibration instruments by expiry, underlying Swap term and strike
\item {\tt ParameterTransformation: } LGM model prices are invariant under scaling and shift transformations
\cite{Lichters} with advantages for numerical convergence of results in long term simulations. These transformations can
be chosen here. Default settings are shiftHorizon 0 (time in years) and scaling factor 1.
\end{itemize}

\medskip

Each FX model is specified by a block as follows

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <ForeignExchangeModels>
    <CrossCcyLGM foreignCcy="default">
      <DomesticCcy>EUR</DomesticCcy>
      <CalibrationType>Bootstrap</CalibrationType>
      <Sigma>
        <Calibrate>Y</Calibrate>
        <ParamType>Piecewise</ParamType>
        <TimeGrid>1.0,2.0,3.0,4.0,5.0,7.0,10.0</TimeGrid>
        <InitialValue>0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1</InitialValue>
      </Sigma>
      <CalibrationOptions>
        <Expiries>1Y,2Y,3Y,4Y,5Y,10Y</Expiries>
        <Strikes/>
      </CalibrationOptions>
    </CrossCcyLGM>
    <CrossCcyLGM foreignCcy="USD">
      <!-- ... -->
    </CrossCcyLGM>
    <CrossCcyLGM foreignCcy="GBP">
      <!-- ... -->
    </CrossCcyLGM>
    <!-- ... -->
  </ForeignExchangeModels>
  <!-- ... -->
<CrossAssetModel>	
\end{minted}
\caption{Simulation model FX configuration}
\label{lst:simulation_model_fx_configuration}
\end{listing}

CrossCcyLGM sections are defined by foreign currency, but we also support a default configuration as above for the IR
model parametrisations.  Within each CrossCcyLGM section, the interpretation of elements is as follows:

\begin{itemize}
\item {\tt DomesticCcy: } Domestic currency completing the FX pair
\item {\tt CalibrationType: } Choose between {\em Bootstrap} and {\em BestFit} as in the IR section
\item {\tt Sigma/Calibrate: } Flag to enable/disable calibration of this particular parameter
\item {\tt Sigma/ParamType: } Choose between {\em Constant} and {\em Piecewise}
\item {\tt Sigma/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
\item {\tt Sigma/InitialValue: } Vector of initial values, matching number of entries in time, or single value if the
time grid is empty
\item {\tt CalibrationOptions: } Choice of calibration instruments by expiry and strike, strikes can be empty (implying
the default, ATMF options), or explicitly specified (in terms of FX rates as absolute strike values, in delta notation
such as $\pm 25D$, $ATMF$ for at the money)
\end{itemize}


\medskip

Each equity model is specified by a block as follows

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <EquityModels>
    <CrossAssetLGM name="default">
      <Currency>EUR</Currency>
      <CalibrationType>Bootstrap</CalibrationType>
      <Sigma>
        <Calibrate>Y</Calibrate>
        <ParamType>Piecewise</ParamType>
        <TimeGrid>1.0,2.0,3.0,4.0,5.0,7.0,10.0</TimeGrid>
        <InitialValue>0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1</InitialValue>
      </Sigma>
      <CalibrationOptions>
        <Expiries>1Y,2Y,3Y,4Y,5Y,10Y</Expiries>
        <Strikes/>
      </CalibrationOptions>
    </CrossAssetLGM>
    <CrossAssetLGM name="SP5">
      <!-- ... -->
    </CrossAssetLGM>
    <CrossAssetLGM name="Lufthansa">
      <!-- ... -->
    </CrossAssetLGM>
      <!-- ... -->
  </EquityModels>
  <!-- ... -->
<CrossAssetModel>	
\end{minted}
\caption{Simulation model equity configuration}
\label{lst:simulation_model_eq_configuration}
\end{listing}

CrossAssetLGM sections are defined by equity name, but we also support a default configuration as above for the IR and 
FX model parameterisations.  Within each CrossAssetLGM section, the interpretation of elements is as follows:

\begin{itemize}
	\item {\tt Currency: } Currency of denomination
	\item {\tt CalibrationType: } Choose between {\em Bootstrap} and {\em BestFit} as in the IR section
	\item {\tt Sigma/Calibrate: } Flag to enable/disable calibration of this particular parameter
	\item {\tt Sigma/ParamType: } Choose between {\em Constant} and {\em Piecewise}
	\item {\tt Sigma/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
	\item {\tt Sigma/InitialValue: } Vector of initial values, matching number of entries in time, or single value if 
	the time grid is empty
	\item {\tt CalibrationOptions: } Choice of calibration instruments by expiry and strike, strikes can be empty 
	(implying the default, ATMF options), or explicitly specified (in terms of equity prices as absolute strike values)
\end{itemize}

\medskip
The inflation model components are specified by a block as follows

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  ...
  <InflationIndexModels>
    <LGM index="EUHICPXT">
      <Currency>EUR</Currency>
      <!-- As in the LGM parameterisation for any IR components -->
      <CalibrationType> ... </CalibrationType>
      <Volatility> ... </Volatility>
      <Reversion> ... </Reversion> 
      <ParameterTransformation> ... </ParameterTransformation>
      <!-- Inflation model specific -->
      <CalibrationCapFloors>
        <!-- not used yet, as there is only one strategy so far -->
        <CalibrationStrategy> ... </CalibrationStrategy> 
        <CapFloor> Floor </CapFloor> <!-- Cap, Floor -->
        <Expiries> 2Y, 4Y, 6Y, 8Y, 10Y </Expiries>
        <!-- can be empty, this will yield calibration to ATM -->
        <Strikes> 0.03, 0.03, 0.03, 0.03, 0.03 </Strikes> 
      </CalibrationCapFloors>
    </LGM>
    <LGM index="USCPI">
      ...
    </LGM>
    ...
  </InflationIndexModels>
  ...
<CrossAssetModel>	
\end{minted}
\caption{Simulation model inflation component configuration}
\label{lst:simulation_model_inflation_configuration}
\end{listing}

The inflation model parameterisation inherits from the LGM parameterisation for interest rate components, in particular the CalibrationType, Volatility and Reversion elements.  The additional elements specify the model's calibration to a selection of either Caps or Floors.

\medskip
Finally, the instantaneous correlation structure is specified as follows.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <InstantaneousCorrelations>
    <Correlation factor1="IR:EUR" factor2="IR:USD">0.3</Correlation>
    <Correlation factor1="IR:EUR" factor2="IR:GBP">0.3</Correlation>
    <Correlation factor1="IR:USD" factor2="IR:GBP">0.3</Correlation>
    <Correlation factor1="IR:EUR" factor2="FX:USDEUR">0</Correlation>
    <Correlation factor1="IR:EUR" factor2="FX:GBPEUR">0</Correlation>
    <Correlation factor1="IR:GBP" factor2="FX:USDEUR">0</Correlation>
    <Correlation factor1="IR:GBP" factor2="FX:GBPEUR">0</Correlation>
    <Correlation factor1="IR:USD" factor2="FX:USDEUR">0</Correlation>
    <Correlation factor1="IR:USD" factor2="FX:GBPEUR">0</Correlation>
    <Correlation factor1="FX:USDEUR" factor2="FX:GBPEUR">0</Correlation>
    <!-- ... --> 
  </InstantaneousCorrelations>
</CrossAssetModel>
\end{minted}
\caption{Simulation model correlation configuration}
\label{lst:simulation_model_correlation_configuration}
\end{listing}

Any risk factor pair not specified explicitly here will be assumed to have zero correlation.

\subsubsection{Market}\label{sec:sim_market}

The last part of the simulation configuration file covers the specification of the simulated market.  Note that the
simulation model will yield the evolution of risk factors such as short rates which need to be translated into entire
yield curves that can be 'understood' by the instruments which we want to price under scenarios.  

Moreover we need to specify how volatility structures evolve even if we do not explicitly simulate volatility. This 
translation happens based on the information in the {\em simulation market} object, which is configured in the section 
within the enclosing tags {\tt <Market>} and {\tt </Market>}, as shown in the following small example.

It should be noted that equity volatilities are taken to be a curve by default. To simulate an equity volatility surface with smile the xml node {\tt <Surface> } must be supplied.
There are two methods in ORE for equity volatility simulation: 
\begin{itemize}
\item Simulating ATM volatilities only (and shifting other strikes relative to this using the $T_{0}$ smile). In this case set {\tt <SimulateATMOnly>} to true.
\item Simulating the full volatility surface. The node {\tt <SimulateATMOnly>} should be omitted or set to false, and explicit moneyness levels for simulation should be provided.
\end{itemize}

Swaption volatilities are taken to be a surface by default. To simulate a swaption volatility cube with smile the xml node {\tt <Cube> } must be supplied.
There are two methods in ORE for swaption volatility cube simulation: 
\begin{itemize}
\item Simulating ATM volatilities only (and shifting other strikes relative to this using the $T_{0}$ smile). In this case set {\tt <SimulateATMOnly>} to true.
\item Simulating the full volatility cube. The node {\tt <SimulateATMOnly>} should be omitted or set to false, and explicit strike spreads for simulation should be provided.
\end{itemize}

FX volatilities are taken to be a curve by default. To simulate an FX volatility cube with smile the xml node {\tt <Surface> } must be supplied. The surface node contains the moneyness levels to be simulated.

For Yield Curves, Swaption Volatilities, CapFloor Volatilities, Default Curves, Base Correlations and Inflation Curves, a DayCounter may be specified for each riskfactor using the node {\tt <DayCounter name="EXAMPLE\_CURVE">}.  
If no day counter is specified for a given riskfactor then the default Actual365 is used. To specify a new default for a riskfactor type then use the daycounter node without any attribute,  {\tt <DayCounter>}. 

For Yield Curves, Swaption Volatilities, CapFloor Volatilities, Default Curves, Base Correlations and Inflation Curves, a DayCounter may be specified for each riskfactor using the node {\tt <DayCounter name="EXAMPLE\_CURVE">}. 
If no day counter is specified for a given riskfactor then the default Actual365 is used. To specify a new default for a riskfactor type then use the daycounter node without any attribute,  {\tt <DayCounter>}.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Market>
  <BaseCurrency>EUR</BaseCurrency>
  <Currencies>
    <Currency>EUR</Currency>
    <Currency>USD</Currency>
  </Currencies>
  <YieldCurves>
    <Configuration>
      <Tenors>3M,6M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,12Y,15Y,20Y</Tenors>
      <Interpolation>LogLinear</Interpolation>
      <Extrapolation>Y</Extrapolation>
      <DayCounter>ACT/ACT</DayCounter> //Sets a new default for all yieldCurves 
    </Configuration>
  </YieldCurves>
  <Indices>
    <Index>EUR-EURIBOR-6M</Index>
    <Index>EUR-EURIBOR-3M</Index>
    <Index>EUR-EONIA</Index>
    <Index>USD-LIBOR-3M</Index>
  </Indices>
  <SwapIndices>
    <SwapIndex>
      <Name>EUR-CMS-1Y</Name>
      <ForwardingIndex>EUR-EURIBOR-6M</ForwardingIndex>
      <DiscountingIndex>EUR-EONIA</DiscountingIndex>
    </SwapIndex>
  </SwapIndices>
  <DefaultCurves> 
      <Names> 
        <Name>CPTY1</Name> 
        <Name>CPTY2</Name> 
      </Names> 
      <Tenors>6M,1Y,2Y</Tenors> 
      <SimulateSurvivalProbabilities>true</SimulateSurvivalProbabilities> 
      <DayCounter name="CPTY1">ACT/ACT</DayCounter> 
  </DefaultCurves> 
  <SwaptionVolatilities>
    <ReactionToTimeDecay>ForwardVariance</ReactionToTimeDecay>
    <Currencies>
      <Currency>EUR</Currency>
      <Currency>USD</Currency>
    </Currencies>
    <Expiries>6M,1Y,2Y,3Y,5Y,10Y,12Y,15Y,20Y</Expiries>
    <Terms>1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,30Y</Terms>
    <Cube>
     <SimulateATMOnly>false</SimulateATMOnly>
     <StrikeSpreads>-0.02,-0.01,0.0,0.01,0.02</StrikeSpreads>
    </Cube>
    <!-- Sets a new daycounter for just the EUR swaptionVolatility surface -->
    <DayCounter ccy="EUR">ACT/ACT</DayCounter> 
  </SwaptionVolatilities> 
  <CapFloorVolatilities>
    <ReactionToTimeDecay>ConstantVariance</ReactionToTimeDecay>
    <Currencies>
      <Currency>EUR</Currency>
      <Currency>USD</Currency>
    </Currencies>
    <DayCounter ccy="EUR">ACT/ACT</DayCounter>
  </CapFloorVolatilities>
  <FxVolatilities>
    <ReactionToTimeDecay>ForwardVariance</ReactionToTimeDecay>
    <CurrencyPairs>
      <CurrencyPair>EURUSD</CurrencyPair>
    </CurrencyPairs>
    <Expiries>6M,1Y,2Y,3Y,4Y,5Y,7Y,10Y</Expiries>
    <Surface>
     <Moneyness>0.5,0.6,0.7,0.8,0.9</Moneyness>
    </Surface>
  </FxVolatilities>
  <EquityVolatilities>
      <Simulate>true</Simulate>
      <ReactionToTimeDecay>ForwardVariance</ReactionToTimeDecay>
      <!-- Alternative: ConstantVariance -->
      <Names>
        <Name>SP5</Name>
        <Name>Lufthansa</Name>
      </Names>
      <Expiries>6M,1Y,2Y,3Y,4Y,5Y,7Y,10Y</Expiries>
      <Surface>
        <SimulateATMOnly>false</SimulateATMOnly><!-- false -->
        <Moneyness>0.1,0.5,1.0,1.5,2.0,3.0</Moneyness><!-- omitted if SimulateATMOnly true -->
      </Surface>
  </EquityVolatilities>
  <ZeroInflationIndexCurves>
    <Names>
      <Name>EUHICP</Name>
      <Name>UKRPI</Name>
      <Name>USCPI</Name>
      ...
    </Names>
    <Tenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</Tenors>
  </ZeroInflationIndexCurves>
  <YYInflationIndexCurves>
    <Names>
      <Name>EUHICPXT</Name>
      ...
    </Names>
    <Tenors>1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</Tenors>
  </YYInflationIndexCurves>
  <DefaultCurves>
    <Names>
      <Name>ItraxxEuropeCrossoverS26V1</Name>
      ...
    </Names>
    <Tenors>1Y,2Y,3Y,5Y,10Y</Tenors>
    <SimulateSurvivalProbabilities>true</SimulateSurvivalProbabilities>
  </DefaultCurves>
  <BaseCorrelations/>
  <CDSVolatilities/>
  <AdditionalScenarioDataCurrencies>
    <Currency>EUR</Currency>
    <Currency>USD</Currency>
  </AdditionalScenarioDataCurrencies>
  <AdditionalScenarioDataIndices>
    <Index>EUR-EURIBOR-3M</Index>
    <Index>EUR-EONIA</Index>
    <Index>USD-LIBOR-3M</Index>
  </AdditionalScenarioDataIndices>
</Market>
\end{minted}
\caption{Simulation market configuration}
\label{lst:simulation_market_configuration}
\end{longlisting}

\todo[inline]{Comment on cap/floor surface structure and reaction to time decay}

%--------------------------------------------------------
\subsection{Sensitivity Analysis: {\tt sensitivity.xml}}\label{sec:sensitivity}
%--------------------------------------------------------

ORE currently supports sensitivity analysis with respect to
\begin{itemize}
\item Discount curves  (in the zero rate domain)
\item Index curves (in the zero rate domain)
\item Yield curves including e.g. equity forecast yield curves (in the zero rate domain)
\item FX Spots
\item FX volatilities
\item Swaption volatilities, ATM matrix or cube 
\item Cap/Floor volatility matrices (in the caplet/floorlet domain)
\item Default probability curves (in the ``zero rate'' domain, expressing survival probabilities $S(t)$ in term of zero rates $z(t)$ via $S(t)=\exp(-z(t)\times t)$ with Actual/365 day counter)
\item Equity spot prices
\item Equity volatilities, ATM or including strike dimension 
\item Zero inflation curves
\item Year-on-Year inflation curves
\item CDS volatilities
\item Base correlation curves
\end{itemize}

The {\tt sensitivity.xml} file specifies how sensitivities are computed for each market component. 
The general structure is shown in listing \ref{lst:sensitivity_config}, for a more comprehensive case see {\tt Examples/Example\_15}. A subset of the following parameters is used in each market component to specify the sensitivity analysis:

\begin{itemize}
\item {\tt ShiftType:} Both absolute or relative shifts can be used to compute a sensitivity, specified by the key words
  {\tt Absolute} resp. {\tt Relative}.
\item {\tt ShiftSize:} The size of the shift to apply.
\item {\tt ShiftTenors:} For curves, the tenor buckets to apply shifts to, given as a comma separated list of periods.
\item {\tt ShiftExpiries:} For volatility surfaces, the option expiry buckets to apply shifts to, given as a comma
  separated list of periods.
\item {\tt ShiftTerms:} For swaption volatility surfaces, the underlying terms to apply shifts to, given as a comma
  separated list of periods.
\item {\tt ShiftStrikes:} For cap/floor, FX option and equity option volatility surfaces, the strikes to apply shifts to, given as a comma separated
  list of absolute strikes
\item {\tt Index:} For cap / floor volatility surfaces, the index which together with the currency defines the surface.
  list of absolute strikes
\item {\tt CurveType:} In the context of Yield Curves used to identify an equity ``risk free'' rate forecasting curve; set to {\tt EquityForecast} in this case
\end{itemize}

The cross gamma filter section contains a list of pairs of sensitivity keys. For each possible pair of sensitivity keys
matching the given strings, a cross gamma sensitivity is computed. The given pair of keys can be (and usually are)
shorter than the actual sensitivity keys. In this case only the prefix of the actual key is matched. For example, the
pair {\tt DiscountCurve/EUR,DiscountCurve/EUR} matches all actual sensitivity pairs belonging to a cross sensitivity by
one pillar of the EUR discount curve and another (different) pillar of the same curve. We list the possible keys by
giving an example in each category:

\begin{itemize}
\item {\tt DiscountCurve/EUR/5/7Y}: 7y pillar of discounting curve in EUR, the pillar is at position 5 in the list of
  all pillars (counting starts with zero)
\item {\tt YieldCurve/BENCHMARK\_EUR/0/6M}: 6M pillar of yield curve ``BENCHMARK\_EUR'', the index of the 6M pillar is
  zero (i.e. it is the first pillar)
\item {\tt IndexCurve/EUR-EURIBOR-6M/2/2Y}: 2Y pillar of index forwarding curve for the Ibor index ``EUR-EURIBOR-6M'',
  the pillar index is 2 in this case
\item {\tt OptionletVolatility/EUR/18/5Y/0.04}: EUR caplet volatility surface, at 5Y option expiry and $4\%$ strike, the
  running index for this expiry - strike pair is 18; the index counts the points in the surface in lexical order
  w.r.t. the dimensions option expiry, strike
\item {\tt FXSpot/USDEUR/0/spot}: FX spot USD vs EUR (with EUR as base ccy), the index is always zero for FX spots, the
  pillar is labelled as ``spot'' always
\item {\tt SwaptionVolatility/EUR/11/10Y/10Y/ATM}: EUR Swaption volatility surface at 10Y option expiry and 10Y
  underlying term, ATM level, the running index for this expiry, term, strike triple has running index 11; the index
  counts the points in the surface in lexical order w.r.t. the dimensions option expiry, underlying term and strike
\end{itemize}

\begin{longlisting}
%\hrule\medskip
  \begin{minted}[fontsize=\scriptsize]{xml}
<SensitivityAnalysis>
  <DiscountCurves>
    <DiscountCurve ccy="EUR">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</ShiftTenors>
    </DiscountCurve>
    ...
  </DiscountCurves>
  ...
  <IndexCurves>
    <IndexCurve index="EUR-EURIBOR-6M">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</ShiftTenors>
    </IndexCurve>
  </IndexCurves>
  ...
  <YieldCurves>
    <YieldCurve name="BENCHMARK_EUR">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</ShiftTenors>
    </YieldCurve>
  </YieldCurves>
  ...
  <FxSpots>
    <FxSpot ccypair="USDEUR">
      <ShiftType>Relative</ShiftType>
      <ShiftSize>0.01</ShiftSize>
    </FxSpot>
  </FxSpots>
  ...
  <FxVolatilities>
    <FxVolatility ccypair="USDEUR">
      <ShiftType>Relative</ShiftType>
      <ShiftSize>0.01</ShiftSize>
      <ShiftExpiries>1Y,2Y,3Y,5Y</ShiftExpiries>
      <ShiftStrikes/>
    </FxVolatility>
  </FxVolatilities>
  ...
  <SwaptionVolatilities>
    <SwaptionVolatility ccy="EUR">
      <ShiftType>Relative</ShiftType>
      <ShiftSize>0.01</ShiftSize>
      <ShiftExpiries>1Y,5Y,7Y,10Y</ShiftExpiries>
      <ShiftTerms>1Y,5Y,10Y</ShiftTerms>
      <ShiftStrikes/>
    </SwaptionVolatility>
  </SwaptionVolatilities>
  ...
  <CapFloorVolatilities>
    <CapFloorVolatility ccy="EUR">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftExpiries>1Y,2Y,3Y,5Y,7Y,10Y</ShiftExpiries>
      <ShiftStrikes>0.01,0.02,0.03,0.04,0.05</ShiftStrikes>
      <Index>EUR-EURIBOR-6M</Index>
    </CapFloorVolatility>
  </CapFloorVolatilities>
  ...
  <CrossGammaFilter>
    <Pair>DiscountCurve/EUR,DiscountCurve/EUR</Pair>
    <Pair>IndexCurve/EUR,IndexCurve/EUR</Pair>
    <Pair>DiscountCurve/EUR,IndexCurve/EUR</Pair>
  </CrossGammaFilter>
</SensitivityAnalysis>
\end{minted}
\caption{Sensitivity configuration}
\label{lst:sensitivity_config}
\end{longlisting}

%--------------------------------------------------------
\subsection{Stress Scenario Analysis: {\tt stressconfig.xml}}\label{sec:stress}
%--------------------------------------------------------

Stress tests can be applied in ORE to the same market segments and with same granularity as described in the sensitivity section \ref{sec:sensitivity}.

\medskip
This file {\tt stressconfig.xml} specifies how stress tests can be configured. The general structure is shown in listing
\ref{lst:stress_config}.

In this example, two stress scenarios ``parallel\_rates'' and ``twist'' are defined. Each scenario definition contains
the market components to be shifted in this scenario in a similar syntax that is also used for the sensitivity
configuration, see \ref{sec:sensitivity}. Components that should not be shifted, can just be omitted in the definition
of the scenario.

However, instead of specifying one shift size per market component, here a whole vector of shifts can be given, with
different shift sizes applied to each point of the curve (or surface / cube).

In case of the swaption volatility shifts, the single value given as {\tt Shift} (without the attributes {\tt expiry}
and {\tt term}) represents a default value that is used whenever no explicit value is given for a expiry / term pair.

\begin{longlisting}
%\hrule\medskip
  \begin{minted}[fontsize=\scriptsize]{xml}
<StressTesting>
  <StressTest id="parallel_rates">
    <DiscountCurves>
      <DiscountCurve ccy="EUR">
        <ShiftType>Absolute</ShiftType>
        <ShiftTenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</ShiftTenors>
        <Shifts>0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01</Shifts>
      </DiscountCurve>
      ...
    </DiscountCurves>
    <IndexCurves>
      ...
    </IndexCurves>
    <YieldCurves>
      ...
    </YieldCurves>
    <FxSpots>
      <FxSpot ccypair="USDEUR">
        <ShiftType>Relative</ShiftType>
        <ShiftSize>0.01</ShiftSize>
      </FxSpot>
    </FxSpots>
    <FxVolatilities>
      ...
    </FxVolatilities>
    <SwaptionVolatilities>
      <SwaptionVolatility ccy="EUR">
        <ShiftType>Absolute</ShiftType>
        <ShiftExpiries>1Y,10Y</ShiftExpiries>
        <ShiftTerms>5Y</ShiftTerms>
        <Shifts>
          <Shift>0.0010</Shift>
          <Shift expiry="1Y" term="5Y">0.0010</Shift>
          <Shift expiry="1Y" term="5Y">0.0010</Shift>
          <Shift expiry="1Y" term="5Y">0.0010</Shift>
          <Shift expiry="10Y" term="5Y">0.0010</Shift>
          <Shift expiry="10Y" term="5Y">0.0010</Shift>
          <Shift expiry="10Y" term="5Y">0.0010</Shift>
        </Shifts>
      </SwaptionVolatility>
    </SwaptionVolatilities>
    <CapFloorVolatilities>
      <CapFloorVolatility ccy="EUR">
        <ShiftType>Absolute</ShiftType>
        <ShiftExpiries>6M,1Y,2Y,3Y,5Y,10Y</ShiftExpiries>
        <Shifts>0.001,0.001,0.001,0.001,0.001,0.001</Shifts>
      </CapFloorVolatility>
    </CapFloorVolatilities>
  </StressTest>
  <StressTest id="twist">
    ...
  </StressTest>
</StressTesting>
  \end{minted}
\caption{Stress configuration}
\label{lst:stress_config}
\end{longlisting}


%--------------------------------------------------------
\subsection{Curves: {\tt curveconfig.xml}}\label{sec:curveconfig}
%--------------------------------------------------------

The configuration of various term structures required to price a portfolio is covered in a single configuration file
which we will label {\tt curveconfig.xml} in the following though the file name can be chosen by the user. This
configuration determines the composition of 
\begin{itemize}
\item Yield curves % done
\item Default curves % done
\item Inflation curves % done
\item Equity forward price curves % done
\item Swaption volatility structures % done
\item Cap/Floor volatility structures % done
\item FX Option volatility structures % done
\item CDS volatility structures % done
\item Inflation Cap/Floor price surfaces % done
\item Equity volatility structures % done
\item Security spreads and recovery rates % done
\item Base correlation curves % done
\end{itemize}

This file also contains other market objects such as FXSpots, Security Spreads and Security Rates which are necessary
for the construction of a market.
 
\include{yieldcurves}

\subsubsection{Default Curves}

Listing \ref{lst:defaultcurve_configuration} shows the configuration of Default curves built from CDS spread
quotes. Alternatively default curves can be set up as a difference curve of two yield curves as shown in listing
\ref{lst:defaultcurve_configuration_2}. If $P_B(0,t)$ and $P_S(0,t)$ denote the discount factors of the given benchmark
and source curve respectively the resulting default term structures has survival probabilities

$$S(0,t) = P_S(0,t) / P_B(0,t)$$

on the given pillars, and w.r.t. a a zero recovery rate. The interpolation is backward flat in the hazard rate.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<DefaultCurves>
  <DefaultCurve>
    <CurveId>BANK_SR_USD</CurveId>
    <CurveDescription>BANK SR CDS USD</CurveDescription>
    <Currency>USD</Currency> 
    <Type>SpreadCDS</Type>
    <DiscountCurve>Yield/USD/USD3M</DiscountCurve>
    <DayCounter>A365</DayCounter>
    <RecoveryRate>RECOVERY_RATE/RATE/BANK/SR/USD</RecoveryRate>
    <Quotes>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/1Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/2Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/3Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/4Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/5Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/7Y</Quote>
      <Quote>CDS/CREDIT_SPREAD/BANK/SR/USD/10Y</Quote>
    </Quotes>
    <Conventions>CDS-STANDARD-CONVENTIONS</Conventions>
  </DefaultCurve>
  <DefaultCurve>
  ...
  </DefaultCurve>
</DefaultCurves>
\end{minted}
\caption{Default curve configuration based on CDS quotes}
\label{lst:defaultcurve_configuration}
\end{longlisting}

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
    <DefaultCurve>
      <CurveId>BOND_YIELD_EUR_OVER_OIS</CurveId>
      <CurveDescription>Default curve derived as bond yield curve over Eonia</CurveDescription>
      <Currency>EUR</Currency>
      <Type>Benchmark</Type>
      <DayCounter>A365</DayCounter>
      <BenchmarkCurve>Yield/EUR/EUR1D</BenchmarkCurve>
      <SourceCurve>Yield/EUR/BOND_YIELD_EUR</SourceCurve>
      <Pillars>1Y,2Y,3Y,4Y,5Y,7Y,10Y</Pillars>
      <SpotLag>0</SpotLag>
      <Calendar>TARGET</Calendar>
    </DefaultCurve>
  </DefaultCurves>
\end{minted}
\caption{Default curve as a difference of two yield curves}
\label{lst:defaultcurve_configuration_2}
\end{longlisting}

\subsubsection{Swaption Volatility Structures}

Listing \ref{lst:swaptionvol_configuration} shows the configuration of Swaption volatility structures.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<SwaptionVolatilities>    
  <SwaptionVolatility>
    <CurveId>EUR_SW_N</CurveId>
    <CurveDescription>EUR normal swaption volatilities</CurveDescription>
    <!-- ATM (Smile not yet supported) -->
    <Dimension>ATM</Dimension>
    <!-- Normal or Lognormal or ShiftedLognormal -->
    <VolatilityType>Normal</VolatilityType>
    <!-- Flat or Linear -->
    <Extrapolation>Flat</Extrapolation>
    <!-- Day counter for date to time conversion -->
    <DayCounter>Actual/365 (Fixed)</DayCounter>
    <!--Calendar and Business day convention for option tenor to date conversion -->
    <Calendar>TARGET</Calendar>
    <BusinessDayConvention>Following</BusinessDayConvention>
    <!-- ATM matrix specification -->
    <OptionTenors>1M,3M,6M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y</OptionTenors>
    <SwapTenors>1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y</SwapTenors>
    <ShortSwapIndexBase>EUR-CMS-1Y</ShortSwapIndexBase>
    <SwapIndexBase>EUR-CMS-30Y</SwapIndexBase>
    <!-- Smile specification (optional) -->
    <SmileOptionTenors>6M,1Y,10Y</SmileOptionTenors>
    <SmileSwapTenors>2Y,5Y</SmileSwapTenors>
    <SmileSpreads>-0.02,-0.01,0.01,0.02</SmileSpreads> <!-- i.e. strike spreads -->
  </SwaptionVolatility>
  <SwaptionVolatility>
    ...
  </SwaptionVolatility>
</SwaptionVolatilities>
\end{minted}
\caption{Swaption volatility configuration}
\label{lst:swaptionvol_configuration}
\end{longlisting}

\subsubsection{Cap/Floor Volatility Structures}

Listing \ref{lst:capfloorvol_configuration} shows the configuration of Cap/Floor volatility structures.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <CapFloorVolatilities>
    <CapFloorVolatility>
      <CurveId>EUR_CF_N</CurveId>
      <CurveDescription>EUR normal cap floor volatilities</CurveDescription>
      <!-- Normal, Lognormal or ShiftedLognormal -->
      <VolatilityType>Normal</VolatilityType>
      <!-- True or False -->
      <Extrapolation>TRUE</Extrapolation>
      <!-- Include ATM vol quotes also: True or False -->
      <IncludeAtm>FALSE</IncludeAtm>
      <!-- Day counter for date to time conversion -->
      <DayCounter>Actual/365 (Fixed)</DayCounter>
      <!--Calendar and Business day convention for cap/floor term to date conversion -->
      <Calendar>TARGET</Calendar>
      <BusinessDayConvention>Following</BusinessDayConvention>
      <Tenors>1Y,2Y,3Y,4Y,5Y,6Y,7Y,8Y,9Y,10Y,15Y,20Y</Tenors>
      <Strikes>-0.01,0.0,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1</Strikes>
      <IborIndex>EUR-EURIBOR-6M</IborIndex>
      <DiscountCurve>Yield/EUR/EUR1D</DiscountCurve>
    </CapFloorVolatility>
    ...
  </CapFloorVolatilities>
\end{minted}
\caption{Cap/Floor volatility configuration}
\label{lst:capfloorvol_configuration}
\end{longlisting}

\subsubsection{FX Volatility Structures}

Listing \ref{lst:fxoptionvol_configuration} shows the configuration of FX volatility structures.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<FXVolatilities>
  <FXVolatility>
    <CurveId>EURUSD</CurveId>
    <CurveDescription />
    <Dimension>ATM</Dimension> <!-- ATM, Smile -->
    <Expiries>1M,3M,6M,1Y,2Y,3Y,10Y</Expiries>
    <FXSpotID>FX/EUR/USD</FXSpotID>
    <FXForeignCurveID>Yield/USD/USD1D</FXForeignCurveID>
    <FXDomesticCurveID>Yield/EUR/EUR1D</FXDomesticCurveID>
  </FXVolatility>
  <FXVolatility>
    ...
  </FXVolatility>
</FXVolatilities>
\end{minted}
\caption{FX option volatility configuration}
\label{lst:fxoptionvol_configuration}
\end{longlisting}

\subsubsection{Equity Curve Structures}

Listing \ref{lst:eqcurve_configuration} shows the configuration of equity forward price curves.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<EquityCurves>
  <EquityCurve>
    <CurveId>SP5</CurveId>
    <CurveDescription>SP 500 equity price projection curve</CurveDescription>
    <Currency>USD</Currency>
    <ForecastingCurve>EUR1D</ForecastingCurve>
    <Type>DividendYield</Type> <!-- DividendYield, ForwardPrice -->
    <!-- Spot quote from the market data file -->
    <SpotQuote>EQUITY/PRICE/SP5/USD</SpotQuote> 
    <Quotes>
      <Quote>EQUITY_DIVIDEND/RATE/SP5/USD/3M</Quote>
      <Quote>EQUITY_DIVIDEND/RATE/SP5/USD/20160915</Quote>
      <Quote>EQUITY_DIVIDEND/RATE/SP5/USD/1Y</Quote>
      <Quote>EQUITY_DIVIDEND/RATE/SP5/USD/20170915</Quote>
    </Quotes>
    <DayCounter>A365</DayCounter>
  </EquityCurve>
  <EquityCurve>
    ...
  </EquityCurve> 
  </EquityCurves>
\end{minted}
\caption{Equity curve configuration}
\label{lst:eqcurve_configuration}
\end{longlisting}

The equity curves here consists of a spot equity price, as well as a set of either forward prices or else dividend 
yields. Upon construction, ORE stores internally an equity spot price quote, a forecasting curve and a dividend yield 
term structure, which are then used together for projection of forward prices.

\subsubsection{Equity Volatility Structures}

Listing \ref{lst:eqoptionvol_configuration} shows the configuration of equity volatility structures.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<EquityVolatilities>
  <EquityVolatility>
    <CurveId>SP5</CurveId>
    <CurveDescription>Lognormal option implied vols for SP 500</CurveDescription>
    <Currency>USD</Currency>
    <Dimension>Smile</Dimension><!-- Alternative: ATM -->
    <Expiries>1M,5Y,10Y</Expiries>
    <!-- If Dimension is Smile: -->
    <Strikes>1932.8,2147.56,2254.939</Strikes>
  </EquityVolatility>
  <EquityVolatility>
    ...
  </EquityVolatility>
</EquityVolatilities>
\end{minted}
\caption{Equity option volatility configuration}
\label{lst:eqoptionvol_configuration}
\end{longlisting}

\subsubsection{Inflation Curves}

Listing \ref{lst:inflationcurve_configuration} shows the configuration of an inflation curve. The inflation curve specific
elements are the following:

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<InflationCurves>
   <InflationCurve>
       <CurveId>USCPI_ZC_Swaps</CurveId>
       <CurveDescription>Estimation Curve for USCPI</CurveDescription>
       <NominalTermStructure>Yield/USD/USD1D</NominalTermStructure>
       <Type>ZC</Type>
       <Quotes>
           <Quote>ZC_INFLATIONSWAP/RATE/USCPI/1Y</Quote>
           <Quote>ZC_INFLATIONSWAP/RATE/USCPI/2Y</Quote>
           ...
           <Quote>ZC_INFLATIONSWAP/RATE/USCPI/30Y</Quote>
           <Quote>ZC_INFLATIONSWAP/RATE/USCPI/40Y</Quote>
       </Quotes>
       <Conventions>USCPI_INFLATIONSWAP</Conventions>
       <Extrapolation>true</Extrapolation>
       <Calendar>US</Calendar>
       <DayCounter>A365</DayCounter>
       <Lag>3M</Lag>
       <Frequency>Monthly</Frequency>
       <BaseRate>0.01</BaseRate>
       <Tolerance>0.000000000001</Tolerance>
       <Seasonality>
           <BaseDate>20160101</BaseDate>
           <Frequency>Monthly</Frequency>
           <Factors>
               <Factor>SEASONALITY/RATE/MULT/USCPI/JAN</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/FEB</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/MAR</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/APR</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/MAY</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/JUN</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/JUL</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/AUG</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/SEP</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/OCT</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/NOV</Factor>
               <Factor>SEASONALITY/RATE/MULT/USCPI/DEC</Factor>
           </Factors>
       </Seasonality>
   </InflationCurve>
</InflationCurves>
\end{minted}
\caption{Inflation Curve Configuration}
\label{lst:inflationcurve_configuration}
\end{longlisting}

\begin{itemize}
\item {\tt NominalTermStructure}: The interest rate curve to be used to strip the inflation curve.
\item {\tt Type}: The type of the curve, {\tt ZC} for zero coupon, {\tt YY} for year on year.
\item {\tt Quotes}: The instruments' market quotes from which to bootstrap the curve.
\item {\tt Conventions}: The conventions applicable to the curve instruments.
\item {\tt Lag}: The observation lag used in the term structure.
\item {\tt Frequency}: The frequency of index fixings.
\item {\tt BaseRate}: The rate at $t=0$, this introduces an additional degree of freedom to get a smoother curve. If not
  given, it is defaulted to the first market rate.
\end{itemize}

The optional seasonality block defines a multiplicative seasonality and contains the following elements:

\begin{itemize}
\item {\tt BaseDate}: Defines the first inflation period to which to apply the seasonality correction, only day and
  month matters, the year is ignored.
\item {\tt Frequency:} Defines the frequency of the factors (usually identical to the index's fixing frequency).
\item {\tt Factors:} Multiplicative seasonality correction factors, must be part of the market data.
\end{itemize}

We note that if zero coupon swap market quotes are given, but the type is set to YY, the zero coupon swap quotes will be
converted to year on year swap quotes on the fly, using the plain forward rates, i.e. no convexity adjustment is
applied.

\subsubsection{Inflation Cap/Floor Price Surfaces}

Listing \ref{lst:inflationcapfloorpricesurface_configuration} shows the configuration of an zero coupon inflation cap
floor price surface.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<InflationCapFloorPriceSurfaces>
  <InflationCapFloorPriceSurface>
      <CurveId>EUHICPXT_ZC_CF</CurveId>
      <CurveDescription>Price Surface ZC CapFloor EUHICPXT</CurveDescription>
      <Type>ZC</Type>
      <StartRate>0.10</StartRate>
      <ObservationLag>3M</ObservationLag>
      <Calendar>TARGET</Calendar>
      <BusinessDayConvention>MF</BusinessDayConvention>
      <DayCounter>A365</DayCounter>
      <Index>EUHICPXT</Index>
      <IndexCurve>Inflation/EUHICPXT/EUHICPXT_ZC_Swaps</IndexCurve>
      <IndexInterpolated>false</IndexInterpolated>
      <YieldTermStructure>Yield/EUR/EUR1D</YieldTermStructure>
      <CapStrikes>0.01,0.015,0.02,0.025,0.03</CapStrikes>
      <FloorStrikes>-0.02,-0.01,-0.005,0.00,0.01,0.015,0.02,0.025,0.03</FloorStrikes>
      <Maturities>1Y,2Y,3Y,4Y,5Y,6Y,7Y,8Y,9Y,10Y,12Y,15Y,20Y,30Y</Maturities>
  </InflationCapFloorPriceSurface>
</InflationCapFloorPriceSurfaces>
\end{minted}
\caption{Inflation zc cap floor price surface configuration}
\label{lst:inflationcapfloorpricesurface_configuration}
\end{longlisting}

\begin{itemize}
\item {\tt Type}: The type of the surface, {\tt ZC} for zero coupon, {\tt YY} for year on year. Only zero coupon
  surfaces are supported currently.
\item {\tt StartRate}: A fall back value used in case the inflation index does not have a term structure
  attached. Should not be relevant.
\item {\tt Observation Lag}: The observation lag applicable to the term structure.
\item {\tt Index}: The underlying zero inflation index.
\item {\tt IndexCurve}: The curve id of the index's projection curve used to determine the ATM levels for the surface.
\item {\tt IndexInterpolated}: Flag indicating whether the index should be interpolating.
\item {\tt YieldTermStructure}: The nominal term structure.
\item {\tt CapStrikes}: The strikes for which cap prices are quoted (may (and will usually) overlap with the floor
  strike region).
\item {\tt FloorStrikes}: The strikes for which floor prices are quoted (may (and will usually) overlap with the cap
  strike region).
\item {\tt Maturities:} The maturities for which cap and floor prices are quoted
\end{itemize}

\subsubsection{CDS Volatilities}

Listing \ref{lst:cdsvol_configuration} shows the configuration of an ATM CDS volatility structure.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <CDSVolatilities>
    <CDSVolatility>
      <CurveId>CDXIG</CurveId>
      <CurveDescription>Lognormal option implied vols for CDX IG</CurveDescription>
      <Expiries>1M,3M,6M</Expiries>
    </CDSVolatility>
  </CDSVolatilities>
\end{minted}
\caption{CDS Volatility Configuration}
\label{lst:cdsvol_configuration}
\end{longlisting}

\subsubsection{Base Correlations}

Listing \ref{lst:basecorr_configuration} shows the configuration of a Base Correlation curve.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <BaseCorrelations>
    <BaseCorrelation>
      <CurveId>CDXIG</CurveId>
      <CurveDescription>CDX IG Base Correlations</CurveDescription>
      <Terms>1D</Terms>
      <DetachmentPoints>0.03, 0.06, 0.10, 0.20, 1.0</DetachmentPoints>
      <SettlementDays>0</SettlementDays>
      <Calendar>US</Calendar>
      <BusinessDayConvention>F</BusinessDayConvention>
      <DayCounter>A365</DayCounter>
      <Extrapolate>Y</Extrapolate>
    </BaseCorrelation>
  </BaseCorrelations>
\end{minted}
\caption{Base Correlation Configuration}
\label{lst:basecorr_configuration}
\end{longlisting}

\subsubsection{FXSpots}

Listing \ref{lst:fxspot_configuration} shows the configuration of the fxSpots. It is assumed that each FXSpot CurveId is of
the form "Ccy1Ccy2".
\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<FXSpots>
  <FXSpot>
    <CurveId>EURUSD</CurveId>
    <CurveDescription/>
  </FXSpot>
  <FXSpot>
    <CurveId>EURGBP</CurveId>
    <CurveDescription/>
  </FXSpot>
  <FXSpot>
    <CurveId>EURCHF</CurveId>
    <CurveDescription/>
  </FXSpot>
  <FXSpot>
    <CurveId>EURJPY</CurveId>
    <CurveDescription/>
  </FXSpot>
</FXSpots>
\end{minted}
\caption{FXSpot Configuration}
\label{lst:fxspot_configuration}
\end{longlisting}

\subsubsection{Securities}

Listing \ref{lst:security_configuration} shows the configuration of the Securities. Each Security name is associated with
a SpreadQuote and a RecoveryRateQuote.

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Securities>
  <Security>
    <CurveId>SECURITY_1</CurveId>
    <CurveDescription>Security</CurveDescription>
    <SpreadQuote>BOND/YIELD_SPREAD/SECURITY_1</SpreadQuote>
    <RecoveryRateQuote>RECOVERY_RATE/RATE/SECURITY_1</RecoveryRateQuote>
  </Security>
</Securities>
\end{minted}
\caption{Security Configuration}
\label{lst:security_configuration}
\end{longlisting}

%--------------------------------------------------------
%\subsection{Conventions: {\tt conventions.xml}}
%\label{sec:conventions}
%--------------------------------------------------------
\include{conventions}


%========================================================
%\section{Trade Data}\label{sec:portfolio_data}
%========================================================
\include{portfoliodata}

%========================================================
%\section{Netting Set Definitions}\label{sec:nettingsetinput}
%========================================================
\include{nettingdata}

%========================================================
%\section{Market Data}\label{sec:market_data}
%========================================================
\include{marketdata}

%========================================================
%\section{Fixing History}
%========================================================
\include{fixingdata}

\begin{appendix}

%========================================================
\section{Methodology Summary}
%========================================================

\subsection{Risk Factor Evolution Model}\label{sec:app_rfe}

ORE applies the cross asset model described in detail in \cite{Lichters} to evolve  the market through time. So far the
evolution model in ORE supports IR and FX risk factors for any number of currencies, Equity and Inflation as well as Credit (for $t0$-pricing). Extensions to full simulation of Credit and Commodity is planned. \\

The Cross Asset Model is based on the Linear Gauss Markov model (LGM) for interest rates, lognormal FX and equity 
processes and the Dodgson-Kainth model for inflation. We identify a single {\em domestic} currency; its LGM process, 
which is labelled $z_0$; and a set of $n$ foreign currencies with associated LGM processes that are labelled $z_i$, 
$i=1,\dots,n$. 

We denote the equity spot price processes with state variables $s_j$ and the index of the denominating 
currency for the equity process as $\phi(j)$. The dividend yield corresponding to each equity process $s_j$ is denoted 
by $q_j$.

Following \cite{Lichters}, 13.27 - 13.29 we write the inflation processes 
in the domestic LGM measure with state variables $z_{I,k}$ and $y_{I,k}$ for $k=1,\ldots,K$.  If we consider $n$ 
foreign exchange rates for converting foreign currency amounts into the single domestic currency by multiplication, 
$x_i$, $i=1,\dots,n$, then the cross asset model is given by the system of SDEs
\begin{eqnarray*}
dz_0 &=& \alpha_0\,dW_0^z \\
dz_i &=& \gamma_i\,dt + \alpha_i\,dW_i^z,  \qquad i>0 \\
\frac{d x_i}{x_i} &=& \mu_i\, dt + \sigma_i\,dW_i^x, \qquad i > 0 \\
\frac{d s_j}{s_j} &=& \mu_j^S\, dt + \sigma_j^S\,dW_j^S \\
dz_{I,k} &=& \alpha_{I,k}(t)dW_k^I \\
dy_{I,k} &=& \alpha_{I,k}(t)H_{I,k}(t)dW_k^I \\ \\
\gamma_i &=&
-\alpha_i^2\,H_i -\rho_{ii}^{zx}\,\sigma_i\,\alpha_i + \rho_{i0}^{zz}\,\alpha_i\,\alpha_0\,H_0\\
\mu_i &=& r_0 - r_i + \rho_{0i}^{zx}\,\alpha_0\,H_0\,\sigma_i\\
\mu_j^S &=& (r_{\phi(j)}(t) - q_j(t) + \rho_{0j}^{zs} \alpha_0 H_0 \sigma_j^S - \epsilon_{\phi(j)}
\rho_{j \phi(j)}^{sx}\sigma_j^S \sigma_{\phi(j)}) \\
r_i &=& f_i(0,t) + z_i(t)\,H'_i(t) + \zeta_i(t)\,H_i(t)\,H'_i(t),
\quad \zeta_i(t) = \int_0^t \alpha_i^2(s)\,ds  \\ \\
dW^\alpha_a\,dW^\beta_b &=& \rho^{\alpha\beta}_{ij}\,dt, \qquad \alpha, \beta \in \{z, x, I\}, \qquad a, b \text{
                              suitable indices }
%\zeta_i(t) &=& \int_0^t \alpha_i^2(s)\,ds,
%\qquad H_i(t) = \int_0^t e^{-\beta_i(s)} \,ds \\
%\beta_i(t) &=& \int_0^t \lambda_i(s)\,ds,
%\qquad \alpha_i(t) = \sigma_i^{HW}(t)\,e^{\beta(t)} \\
\end{eqnarray*}
where we have dropped time dependencies for readability, $f_i(0,t)$ is the instantaneous forward curve in currency $i$, 
and $\epsilon_i$ is an indicator such that $\epsilon_i = 1 - \delta_{0i}$, where $\delta$ is the Kronecker delta.

\medskip Parameters $H_i(t)$ and $\alpha_i(t)$ (or alternatively $\zeta_i(t)$) are LGM model parameters which determine,
together with the stochastic factor $z_i(t)$, the evolution of numeraire and zero bond prices in the LGM model:
\begin{align}
N(t) &= \frac{1}{P(0,t)}\exp\left\{H_t\, z_t + \frac{1}{2}H^2_t\,\zeta_t \right\}
\label{lgm1f_numeraire} \\
P(t,T,z_t)
&= \frac{P(0,T)}{P(0,t)}\:\exp\left\{ -(H_T-H_t)\,z_t - \frac{1}{2} \left(H^2_T-H^2_t\right)\,\zeta_t\right\}.
\label{lgm1f_zerobond}
\end{align}

Note that the LGM model is closely related to the Hull-White model in T-forward measure \cite{Lichters}.

\medskip The parameters $H_{I,k}(t)$ and $\alpha_{I,k}(t)$ determine together with the factors $z_{I,k}(t), y_{I,k}(t)$
the evolution of the spot Index $I(t)$ and the forward index $\hat{I}(t,T) = P_I(t,T) / P_n(t,T)$ defined as the ratio
of the inflation linked zero bond and the nominal zero bond,

\begin{eqnarray*}
  \hat{I}(t,T) &=& \frac{\hat{I}(0,T)}{\hat{I}(0,t)} e^{(H_{I,k}(T)-H_{I,k}(t))z_{I,k}(t)+\tilde{V}(t,T)} \\
  I(t) &=& I(0) \hat{I}(0,t)e^{H_{I,k}(t)z_{I,k}(t)-y_{I,k}(t)-V(0,t)}
\end{eqnarray*}

with, in case of domestic currency inflation,

\begin{eqnarray*}
  V(t,T) &=& \frac{1}{2} \int_t^T (H_{I,k}(T)-H_{I,k}(s))^2 \alpha_{I,k}^2(s) ds \\
         & & - \rho^{zI}_{0,k} H_0(T) \int_t^T (H_{I,k}(t)-H_{I,k}(s))\alpha_0(s)\alpha_{I,k}(s)ds \\
  \tilde{V}(t,T) &=& V(t,T) - V(0,T) -V(0,t) \\
         &=& -\frac{1}{2}(H_{I,k}^2(T)-H_{I,k}^2(t))\zeta_{I,k}(t,0) \\
         & & +(H_{I,k}(T)-H_{I,k}(t)) \zeta_{I,k}(t,1) \\
         & & +(H_0(T)H_{I,k}(T) - H_0(t)H_{I,k}(t))\zeta_{0I}(t,0) \\
         & & -(H_0(T)-H_0(t))\zeta_{0I}(t,1) \\
  V(0,t) &=& \frac{1}{2}H_{I,k}^2(t)\zeta_{I,k}(t,0)-H_{I,k}(t)\zeta_{I,k}(t,1)+\frac{1}{2}\zeta_{I,k}(t,2) \\
         & & -H_0(t)H_{I,k}(t)\zeta_{0I}(t,0)+H_0(t)\zeta_{0I}(t,1) \\
  \zeta_{I,k}(t,k) &=& \int_0^t H_{I,k}^k(s)\alpha_{I,k}^2(s) ds \\
  \zeta_{0I}(t,k) &=& \rho^{zI}_{0,k}\int_0^t H_{I,k}^k(t) \alpha_0(s) \alpha_{I,k}(s) ds
\end{eqnarray*}

and for foreign currency inflation in currency $i>0$, with

\begin{eqnarray*}
  \tilde{V}(t,T) &=& V(t,T) -V(0,T) + V(0,T)
\end{eqnarray*}

and

\begin{eqnarray*}
  V(t,T) &=& \frac{1}{2}\int_t^T (H_{I,k}(T)-H_{I,k}(s))^2 \alpha_{I,k}(s) ds \\
  & & -\rho^{zI}_{0,k} \int_t^T H_0(s)\alpha_0(s)(H_{I,k}(T)-H_{I,k}(s)\alpha_{I,k}(s)) ds \\
  & & -\rho^{zI}_{i,k} \int_t^T (H_i(T)-H_i(s))\alpha_i(s)(H_{I,k}(T)-H_{I,k}(s))\alpha_{I,k}(s) ds \\
  & & +\rho^{xI}_{i,k} \int_t^T \sigma_i(s)(H_{I,k}(T)-H_{I,k}(s))\alpha_{I,k}(s) ds
\end{eqnarray*}

\subsection{Analytical Moments of the Risk Factor Evolution Model}\label{sec:app_analytical_moments}

We follow \cite{Lichters}, chapter 16. The expectation of the interest rate process $z_i$ conditional on $\mathcal{F}_{t_0}$ at $t_0+\Delta t$ is

\begin{eqnarray*}
  \mathbb{E}_{t_0}[z_i(t_0+\Delta t)] &=& z_i(t_0) + \mathbb{E}_{t_0}[\Delta z_i],
  \qquad\mbox{with}\quad \Delta z_i = z_i(t_0+\Delta t) - z_i(t_0) \\
  &=& z_i(t_0) -\int_{t_0}^{t_0+\Delta t} H^z_i\,(\alpha^z_i)^2\,du + \rho^{zz}_{0i} \int_{t_0}^{t_0+\Delta t}
  H^z_0\,\alpha^z_0\,\alpha^z_i\,du \\
  & & - \epsilon_i  \rho^{zx}_{ii}\int_{t_0}^{t_0+\Delta t} \sigma_i^x\,\alpha^z_i\,du
\end{eqnarray*}

where $\epsilon_i$ is zero for $i=0$ (domestic currency) and one otherwise.

\bigskip

The expectation of the FX process $x_i$ conditional on $\mathcal{F}_{t_0}$ at $t_0+\Delta t$ is

\begin{eqnarray*}
  \mathbb{E}_{t_0}[\ln x_i(t_0+\Delta t)] &=& \ln x_i(t_0) +  \mathbb{E}_{t_0}[\Delta \ln x_i],
  \qquad\mbox{with}\quad \Delta \ln x_i = \ln x_i(t_0+\Delta t) - \ln x_i(t_0) \\
  &=& \ln x_i(t_0) + \left(H^z_0(t)-H^z_0(s)\right) z_0(s) -\left(H^z_i(t)-H^z_i(s)\right)z_i(s)\\
  &&+ \ln \left( \frac{P^n_0(0,s)}{P^n_0(0,t)} \frac{P^n_i(0,t)}{P^n_i(0,s)}\right) \\
  && - \frac12 \int_s^t (\sigma^x_i)^2\,du \\
  &&+\frac12 \left((H^z_0(t))^2 \zeta^z_0(t) -  (H^z_0(s))^2 \zeta^z_0(s)- \int_s^t (H^z_0)^2
  (\alpha^z_0)^2\,du\right)\\
  &&-\frac12 \left((H^z_i(t))^2 \zeta^z_i(t) -  (H^z_i(s))^2 \zeta^z_i(s)-\int_s^t (H^z_i)^2 (\alpha^z_i)^2\,du
  \right)\\
  && + \rho^{zx}_{0i} \int_s^t H^z_0\, \alpha^z_0\, \sigma^x_i\,du \\
  &&  - \int_s^t \left(H^z_i(t)-H^z_i\right)\gamma_i \,du, \qquad\mbox{with}\quad s = t_0, \quad t = t_0+\Delta t
\end{eqnarray*}

with

\begin{eqnarray*}
  \gamma_i = -H^z_i\,(\alpha^z_i)^2  + H^z_0\,\alpha^z_0\,\alpha^z_i\,\rho^{zz}_{0i} - \sigma_i^x\,\alpha^z_i\,
  \rho^{zx}_{ii}
\end{eqnarray*}

The expectation of the Inflation processes $z_{I,k}, y_{I,k}$ conditional on $\mathcal{F}_{t_0}$ at any time $t>t_0$ is
equal to $z_{I,k}(t_0)$ resp. $y_{I,k}(t_0)$ since both processes are drift free.

\bigskip

The expectation of the equity processes $s_j$ conditional on $\mathcal{F}_{t_0}$ at $t_0+\Delta t$ is
\begin{eqnarray*}
\mathbb{E}_{t_0}[\ln s_j(t_0+\Delta t)] &=& \ln s_j(t_0) +  \mathbb{E}_{t_0}[\Delta \ln s_j],
\qquad\mbox{with}\quad \Delta \ln s_j = \ln s_j(t_0+\Delta t) - \ln s_j(t_0) \\
&=& \ln s_j(t_0) +  \ln \left[\frac{P_{\phi(j)}(0,s)}{P_{\phi(j)}(0,t)} \right] - \int_s^t 
q_j(u) 
du - \frac{1}{2} \int_s^t \sigma_{j}^{S}(u) \sigma_{j}^{S}(u) du\\
&&
+\rho_{0j}^{zs} \int_s^t \alpha_0(u) H_0(u) \sigma_j^S(u) du
- \epsilon_{\phi(j)} \rho_{j \phi(j)}^{sx} \int_s^t \sigma_j^S (u)\sigma_{\phi(j)}(u) du\\
&&+\frac{1}{2} \left( H_{\phi(j)}^2(t) \zeta_{\phi(j)}(t) - H_{\phi(j)}^2(s) \zeta_{\phi(j)}(s)
- \int_s^t H_{\phi(j)}^2(u) \alpha_{\phi(j)}^2(u) du \right)\\
&&  + (H_{\phi(j)}(t) - H_{\phi(j)}(s)) z_{\phi(j)}(s) 
+\epsilon_{\phi(j)} \int_s^t \gamma_{\phi(j)} (u) (H_{\phi(j)}(t) - H_{\phi(j)}(u)) du\\
\end{eqnarray*}

The IR-IR covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
      \mathrm{Cov} [\Delta z_a, \Delta \ln x_b] &=& \rho^{zz}_{0a}\int_s^t \left(H^z_0(t)-H^z_0\right)
  \alpha^z_0\,\alpha^z_a\,du \nonumber\\
      &&- \rho^{zz}_{ab}\int_s^t \alpha^z_a \left(H^z_b(t)-H^z_b\right) \alpha^z_b \,du \nonumber\\
      &&+\rho^{zx}_{ab}\int_s^t \alpha^z_a \, \sigma^x_b \,du.
\end{eqnarray*}

The IR-FX covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
      \mathrm{Cov} [\Delta z_a, \Delta \ln x_b] &=& \rho^{zz}_{0a}\int_s^t \left(H^z_0(t)-H^z_0\right)
  \alpha^z_0\,\alpha^z_a\,du \nonumber\\
      &&- \rho^{zz}_{ab}\int_s^t \alpha^z_a \left(H^z_b(t)-H^z_b\right) \alpha^z_b \,du \nonumber\\
      &&+\rho^{zx}_{ab}\int_s^t \alpha^z_a \, \sigma^x_b \,du.
\end{eqnarray*}

The FX-FX covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
      \mathrm{Cov}[\Delta \ln x_a, \Delta \ln x_b] &=&
      \int_s^t \left(H^z_0(t)-H^z_0\right)^2 (\alpha_0^z)^2\,du \nonumber\\
      && -\rho^{zz}_{0a} \int_s^t \left(H^z_a(t)-H^z_a\right) \alpha_a^z\left(H^z_0(t)-H^z_0\right) \alpha_0^z\,du
  \nonumber\\
      &&- \rho^{zz}_{0b}\int_s^t \left(H^z_0(t)-H^z_0\right)\alpha_0^z \left(H^z_b(t)-H^z_b\right)\alpha_b^z\,du
  \nonumber\\
      &&+ \rho^{zx}_{0b}\int_s^t \left(H^z_0(t)-H^z_0\right)\alpha_0^z \sigma^x_b\,du \nonumber\\
      &&+ \rho^{zx}_{0a}\int_s^t \left(H^z_0(t)-H^z_0\right)\alpha_0^z\,\sigma^x_a\,du \nonumber\\
      &&- \rho^{zx}_{ab}\int_s^t \left(H^z_a(t)-H^z_a\right)\alpha_a^z \sigma^x_b,du\nonumber\\
      &&- \rho^{zx}_{ba}\int_s^t \left(H^z_b(t)-H^z_b\right)\alpha_b^z\,\sigma^x_a\, du \nonumber\\
      &&+ \rho^{zz}_{ab}\int_s^t \left(H^z_a(t)-H^z_a\right)\alpha_a^z \left(H^z_b(t)-H^z_b\right)\alpha_b^z\,du
  \nonumber\\
      &&+ \rho^{xx}_{ab}\int_s^t\sigma^x_a\,\sigma^x_b \,du
\end{eqnarray*}

The IR-INF covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
  \mathrm{Cov}[ \Delta z_a, \Delta z_{I,b} ] & = & \rho_{ab}^{zI} \int_s^t \alpha_a(s) \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta z_a, \Delta y_{I,b} ] & = & \rho_{ab}^{zI} \int_s^t \alpha_a(s) H_{I,b}(s) \alpha_{I,b}(s) ds
\end{eqnarray*}

The FX-INF covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
  \mathrm{Cov}[ \Delta x_a, \Delta z_{I,b} ] & = & \rho_{0b}^{zI} \int_s^t \alpha_0(s) (H_0(t)-H_0(s)) \alpha_{I,b}(s) ds \\
                                             & & -\rho_{ab}^{zI} \int_s^t \alpha_a(s)(H_a(t)-H_a(s))\alpha_{I,b}(s) ds \\
                                             & & +\rho_{ab}^{xI}\int_s^t \sigma_a(s) \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta x_a, \Delta y_{I,b} ] & = & \rho_{0b}^{zI} \int_s^t \alpha_0(s) (H_0(t)-H_0(s)) H_{I,b}(s)\alpha_{I,b}(s) ds \\
                                             & & -\rho_{ab}^{zI} \int_s^t \alpha_a(s)(H_a(t)-H_a(s))H_{I,b}(s)\alpha_{I,b}(s) ds \\
                                             & & +\rho_{ab}^{xI}\int_s^t \sigma_a(s) H_{I,b}(s)\alpha_{I,b}(s) ds
\end{eqnarray*}

The INF-INF covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
  \mathrm{Cov}[ \Delta z_{I,a}, \Delta z_{I,b} ] & = & \rho_{ab}^{II} \int_s^t \alpha_{I,a}(s) \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta z_{I,a}, \Delta y_{I,b} ] & = & \rho_{ab}^{II} \int_s^t \alpha_{I,a}(s) H_{I,b}(s)
                                                       \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta y_{I,a}, \Delta y_{I,b} ] & = & \rho_{ab}^{II} \int_s^t H_{I,a}(s) \alpha_{I,a}(s) H_{I,b}(s) \alpha_{I,b}(s) ds
\end{eqnarray*}

The equity/equity covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is
\begin{eqnarray*}
	Cov \left[\Delta ln[s_i], \Delta ln[s_j] \right] &=&
	\rho_{\phi(i) \phi(j)}^{zz}\int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) (H_{\phi(j)} (t)\\
	&& - H_{\phi(j)} (u)) \alpha_{\phi(i)}(u) \alpha_{\phi(j)}(u) du\\
	&&+ \rho_{\phi(i) j}^{zs} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)}(u) \sigma_j^S(u) du\\
	&&+ \rho_{\phi(j) i}^{zs} \int_s^t (H_{\phi(j)} (t) - H_{\phi(j)} (u)) \alpha_{\phi(j)}(u) \sigma_i^S(u) du\\
	&&+ \rho_{ij}^{ss} \int_s^t \sigma_i^S(u) \sigma_j^S(u) du\\
\end{eqnarray*}

The equity/FX covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is
\begin{eqnarray*}
	Cov \left[\Delta ln[s_i], \Delta ln[x_j] \right] &=&
	\rho_{\phi(i)0}^{zz} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) (H_0 (t) - H_0 (u)) \alpha_{\phi(i)}(u) 
	\alpha_0(u) 
	du\\
	&& - \rho_{\phi(i)j}^{zz} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) (H_j (t) - H_j (u)) \alpha_{\phi(i)} 
	(u)\alpha_j(u) du\\
	&& + \rho_{\phi(i)j}^{zx} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)} (u) \sigma_j(u) du\\
	&&+ \rho_{i0}^{sz} \int_s^t (H_0 (t) - H_0 (u)) \alpha_0 (u) \sigma_i^S(u) du\\
	&&- \rho_{ij}^{sz} \int_s^t (H_j (t) - H_j (u)) \alpha_j (u) \sigma_i^S(u) du\\
	&&+ \rho_{ij}^{sx} \int_s^t \sigma_i^S(u) \sigma_j(u) du\\
\end{eqnarray*}

The equity/IR covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is
\begin{eqnarray*}
	Cov \left[\Delta ln[s_i], \Delta z_j \right] &=&
	\rho_{\phi(i)j}^{zz} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)} (u) \alpha_j (u) du\\
	&&+ \rho_{ij}^{sz} \int_s^t \sigma_i^S (u) \alpha_j (u) du\\
\end{eqnarray*}

The equity/inflation covariances over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) are as follows:
\begin{eqnarray*}
	Cov \left[\Delta ln[s_i], \Delta z_{I,j} \right] &=&
	\rho_{\phi(i)j}^{zI} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)} (u) \alpha_{I,j} (u) du\\
	&&+ \rho_{ij}^{sI} \int_s^t \sigma_i^S (u) \alpha_{I,j} (u) du\\	
	Cov \left[\Delta ln[s_i], \Delta y_{I,j} \right] &=&
	\rho_{\phi(i)j}^{zI} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)} (u) H_{I,j} (u) \alpha_{I,j} (u) du\\
	&&+ \rho_{ij}^{sI} \int_s^t \sigma_i^S (u) H_{I,j} (u) \alpha_{I,j} (u) du\\
\end{eqnarray*}



\subsection{Exposures}\label{sec:app_exposure}

In ORE we use the following exposure definitions
\begin{align}
\EE(t) = \EPE(t) &= \E^N\left[ \frac{(NPV(t)-C(t))^+}{N(t)} \right] \label{EE}\\
\ENE(t) &= \E^N\left[ \frac{(-NPV(t)+C(t))^+}{N(t)} \right] \label{ENE}
\end{align}
where $\NPV(t)$ stands for the netting set NPV and $C(t)$ is the collateral balance\footnote{$C(t)>0$ means that we have
  {\em received} collateral from the counterparty} at time $t$. Note that these exposures are expectations of values
discounted with numeraire $N$ (in ORE the Linear Gauss Markov model's numeraire) to today, and expectations are taken in
the measure associated with numeraire $N$. These are the exposures which enter into unilateral CVA and DVA calculation,
respectively, see next section. Note that we sometimes label the expected exposure (\ref{EE}) EPE, not to be confused
with the Basel III Expected Positive Exposure below.

\medskip
Basel III defines a number of exposures each of which is a 'derivative' of Basel's Expected Exposure:
\begin{align}
\intertext{Expected Exposure}
EE_B(t) &= \E[\max(NPV(t) - C(t), 0)] \label{basel_ee}\\
\intertext{Expected Positive Exposure}
EPE_B(T) &= \frac{1}{T} \sum_{t<T} EE_B(t)\cdot \Delta t  \label{basel_epe} \\
\intertext{Effective Expected Exposure, recursively defined as running maximum}
EEE_B(t) &= \max(EEE_B(t-\Delta t), EE_B(t)) \label{basel_eee}\\
\intertext{Effective Expected Positive Exposure}
EEPE_B(T) &= \frac{1}{T} \sum_{t<T} EEE_B(t)\cdot \Delta t \label{basel_eepe}
\end{align}
The last definition, Effective EPE, is used in Basel documents since Basel II for Exposure At Default and capital
calculation. Following \cite{bcbs128,bcbs189} the time averages in the EPE and EEPE calculations are taken over {\em the
  first year} of the exposure evolution (or until maturity if all positions of the netting set mature before one year).

\medskip
To compute $EE_B(t)$ consistently in a risk-neutral setting, we compound (\ref{EE}) with the deterministic discount factor $P(t)$ up to horizon $t$:
$$
EE_B(t) = \frac{1}{P(t)} \:\EE(t)
$$

Finally, we define another common exposure measure, the {\em Potential Future Exposure} (PFE), as a (typically high)
quantile $\alpha$ of the NPV distribution through time, similar to Value at Risk but at the upper end of the NPV
distribution:

\begin{align}
  \PFE_\alpha(t) = \left(\inf\left\{ x | F_t(x) \geq \alpha\right\}\right)^+ \label{PFE}
\end{align}

where $F_t$ is the cumulative NPV distribution function at time $t$. Note that we also take the positive part to ensure
that PFE is a positive measure even if the quantile yields a negative value which is possible in extreme cases.
 
\subsection{CVA and DVA}\label{sec:app_cvadva}

Using the expected exposures in \ref{sec:app_exposure} unilateral discretised CVA and DVA are given by \cite{Lichters}
\begin{align}
\CVA &= \sum_{i} \PD(t_{i-1},t_i)\times\LGD\times \EPE(t_i) \label{CVA}\\
\DVA &= \sum_{i} \PD_{Bank}(t_{i-1},t_i)\times\LGD_{Bank}\times \ENE(t_i) \label{DVA}
\end{align}
where
\begin{align*}
\EPE(t) & \mbox{ expected exposure (\ref{EE})}\\
\ENE(t) & \mbox{ expected negative exposure (\ref{ENE})}\\
PD(t_i,t_j) & \mbox{ counterparty probability of default in } [t_i;t_j]\\
PD_{Bank}(t_i,t_j) & \mbox{ our probability of default in } [t_i;t_j]\\
LGD & \mbox{ counterparty loss given default}\\
LGD_{Bank} & \mbox{ our loss given default}\\
\end{align*}

Note that the choice $t_i$ in the arguments of $\EPE(t_i)$ and $\ENE(t_i)$ means we are choosing the {\em advanced}
rather than the {\em postponed} discretization of the CVA/DVA integral \cite{BrigoMercurio}. This choice can be easily
changed in the ORE source code or made configurable. \\

Moreover, formulas (\ref{CVA}, \ref{DVA}) assume independence of credit and other market risk factors, so that $\PD$ and
$\LGD$ factors are outside the expectations. With the extension of ORE to credit asset classes and in particular for
wrong-way-risk analysis, CVA/DVA formulas will be generalised.

\subsection{FVA}\label{sec:fva}

%Any exposure (uncollateralised or residual after taking collateral into account) gives rise to funding cost or benefits
%depending on the sign of the residual position. This can be expressed as a Funding Value Adjustment (FVA). A simple
%definition of FVA can be given in a very similar fashion as the sum of unilateral CVA and DVA which we defined by
%(\ref{CVA},\ref{DVA}), namely as an expectation of exposures times funding spreads:
%\begin{align}
%  \FVA &= \underbrace{\sum_{i=1}^n f_b(t_{i-1},t_i)\,\delta_i \, \E^N\left[S_C(t_{i-1})\, S_B(t_{i-1})\, (\NPV(t_i))^+\,
%         D(t_i)\right]}_{\mbox{Funding Benefit Adjustment (FBA)}}\nonumber\\
%       & {} - \underbrace{\sum_{i=1}^n f_l(t_{i-1},t_i)\,\delta_i \, \E^N\left[S_C(t_{i-1})\, S_B(t_{i-1})\, (-\NPV(t_i))^+\, D(t_i)\right]}_{\mbox{Funding Cost Adjustment (FCA)}}\label{eq_simple_fva}
%\end{align}
%where
%\begin{align*}
%D(t_i) & \mbox{ stochastic discount factor, $1/N(t_i)$ in LGM}\\
%\NPV(t_i) & \mbox{ portfolio value after potential collateralization}\\
%S_C(t_j) & \mbox{ survival probability of the counterparty}\\
%S_B(t_j) & \mbox{ survival probability of the bank}\\
%f_b(t_j) & \mbox{ borrowing spread for the bank relative to the collateral compounding rate}\\
%f_l(t_j) & \mbox{ lending spread for the bank relative to the collateral compounding rate}
%\end{align*}
%For details see e.g. Chapter 14 in Gregory \cite{Gregory12} and the discussion in \cite{Lichters}.

Any exposure (uncollateralised or residual after taking collateral into account) gives rise to funding cost or benefits
depending on the sign of the residual position. This can be expressed as a Funding Value Adjustment (FVA). A simple
definition of FVA can be given in a very similar fashion as the sum of unilateral CVA and DVA which we defined by
(\ref{CVA},\ref{DVA}), namely as an expectation of exposures times funding spreads:
\begin{align}
  \FVA &= \underbrace{\sum_{i=1}^n f_l(t_{i-1},t_i)\,\delta_i \, \E^N\left\{S_C(t_{i-1})\, S_B(t_{i-1})\, [-\NPV(t_i)+C(t_i)]^+\,
         D(t_i)\right\}}_{\mbox{Funding Benefit Adjustment (FBA)}}\nonumber\\
       & {} - \underbrace{\sum_{i=1}^n f_b(t_{i-1},t_i)\,\delta_i \, \E^N\left\{S_C(t_{i-1})\, S_B(t_{i-1})\, [\NPV(t_i)-C(t_i)]^+\, D(t_i)\right\}}_{\mbox{Funding Cost Adjustment (FCA)}}\label{eq_simple_fva}
%  \FVA &= - \underbrace{\sum_{i=1}^n f_b(t_{i-1},t_i)\,\delta_i \, \E^N\left[S_C(t_{i-1})\, S_B(t_{i-1})\, (\NPV(t_i))^+\,
 %        D(t_i)\right]}_{\mbox{Funding Cost Adjustment (FCA)}}\nonumber\\
 %      & {} \underbrace{\sum_{i=1}^n f_l(t_{i-1},t_i)\,\delta_i \, \E^N\left[S_C(t_{i-1})\, S_B(t_{i-1})\, (-\NPV(t_i))^+\, D(t_i)\right]}_{\mbox{Funding Benefit Adjustment (FBA)}}\label{eq_simple_fva}
\end{align}
where
\begin{align*}
D(t_i) & \mbox{ stochastic discount factor, $1/N(t_i)$ in LGM}\\
\NPV(t_i) & \mbox{ portfolio value at time } t_i\\
C(t_i) & \mbox{Collateral account balance at time } t_i \\ 
S_C(t_j) & \mbox{ survival probability of the counterparty}\\
S_B(t_j) & \mbox{ survival probability of the bank}\\
f_b(t_j) & \mbox{ borrowing spread for the bank relative to OIS flat}\\
f_l(t_j) & \mbox{ lending spread for the bank relative to OIS flat}
\end{align*}
For details see e.g. Chapter 14 in Gregory \cite{Gregory12} and the discussion in \cite{Lichters}.

\medskip
The reasoning leading to the expression above is as follows. Consider, for example, a single partially collateralised derivative (no collateral at all or CSA with a significant threshold) between us (the Bank) and counterparty 1 (trade 1). 

We assume that we enter into an offsetting trade with (hypothetical) counterparty 2 which is perfectly collateralised (trade 2). We label the NPV of trade 1 and 2 $\NPV_{1,2}$ respectively (from our perspective, excluding CVA). Then $\NPV_2=-\NPV_1$. The respective collateral amounts due to trade 1 and 2 are $C_1$ and $C_2$ from our perspective. Because of the perfect collateralisation of trade 2 we assume $C_2=\NPV_2$. The imperfect collateralisation of trade 1 means $C_1 \ne \NPV_1$. The net collateral balance from our perspective is then $C=C_1+C_2$ which can be written $C=C_1+C_2 = C_1 + \NPV_2 = -\NPV_1 + C_1$.

\begin{itemize}
\item If $C>0$ we receive net collateral and pay the overnight rate on this notional amount. On the other hand we can invest the received collateral and earn our lending rate, so that we have a benefit proportional to the lending spread $f_l$ (lending rate minus overnight rate). It is a benefit assuming $f_l >0$. $C>0$ means $-\NPV_1 + C_1 > 0$ so that we can cover this case with ``lending notional'' $[-\NPV_1 + C_1]^+$.
\item If $C<0$ we post collateral amount $-C$ and receive the overnight rate on this amount. Amount $-C$ needs to be funded in the market, and we pay our borrowing rate on it. This leads to a funding cost proportional to the borrowing spread $f_b$ (borrowing rate minus overnight). $C<0$ means $\NPV_1 - C_1 > 0$, so that we can cover this case with ``borrowing notional'' $[\NPV_1 - C_1]^+$. If the borrowing spread is positive, this term proportional to $f_b \times [\NPV_1 - C_1]^+$ is indeed a cost and therefore needs to be subtracted from the benefit above.
\end{itemize}
   
Formula \eqref{eq_simple_fva} evaluates these funding cost components on the basis of the original trade's or portfolio's $\NPV$. Perfectly collateralised portfolios hence do not contribute to FVA because under the hedging fiction, they are hedged with a perfectly collateralised opposite portfolio, so any collateral payments on portfolio 1 are canceled out by those of the opposite sign on portfolio 2.

\subsection{COLVA}

When the CSA defines a collateral compounding rate that deviates from the overnight rate, this gives rise to another
value adjustment labeled COLVA \cite{Lichters}. In the simplest case the deviation is just given by a constant spread
$\Delta$:
\begin{align}
\COLVA &= \E^N\left[ \sum_i -C(t_i)\cdot \Delta \cdot \delta_i \cdot D(t_{i+1}) \right]
\label{COLVA}
\end{align}
where $C(t)$ is the collateral balance\footnote{see \ref{sec:app_exposure}, $C(t)>0$ means that we have {\em received}
  collateral from the counterparty} at time $t$ and $D(t)$ is the stochastic discount factor $1/N(t)$ in LGM. Both
$C(t)$ and
$N(t)$ are computed in ORE's Monte Carlo framework, and the expectation yields the desired adjustment. \\
 
Replacing the constant spread by a time-dependent deterministic function in ORE is straight forward. 
  
\subsection{Collateral Floor Value}

A less trivial extension of the simple COLVA calculation above, also covered in ORE, is the case where the deviation
between overnight rate and collateral rate is stochastic itself. A popular example is a CSA under which the collateral
rate is the overnight rate {\em floored at zero}. To work out the value of this CSA feature one can take the difference
of discounted margin cash flows with and without the floor feature. It is shown in \cite{Lichters} that the following
formula is a good approximation to the collateral floor value
\begin{align}
\Pi_{Floor} &= \E^N\left[ \sum_i -C(t_i)\cdot (-r(t_i))^+\cdot\delta_i \cdot D(t_{i+1}) \right]
\label{CSA_floor_value_approx}
\end{align}
where $r$ is the stochastic overnight rate and $(-r)^+ = r^+ - r$ is the difference between floored and 'un-floored' compounding rate. \\

Taking both collateral spread and floor into account, the value adjustment is 
\begin{align}
\Pi_{Floor,\Delta} &= \E^N\left[ \sum_i -C(t_i)\cdot ((r(t_i)-\Delta)^+-r(t_i))\cdot\delta_i \cdot D(t_{i+1}) \right] 
\label{CSA_floor_value_approx_2}
\end{align}

\subsection{Dynamic Initial Margin and MVA}\label{sec:app_dim}

The introduction of Initial Margin posting in non-cleared OTC derivatives business reduces residual credit exposures and
the associated value adjustments, {\bf CVA} and {\bf DVA}.

On the other hand, it gives rise to additional funding cost. The value of the latter is referred to as Margin Value Adjustment ({\bf MVA}).\\

To quantify these two effects one needs to model Initial Margin under future market scenarios, i.e. Dynamic Initial Margin ({\bf DIM}). Potential approaches comprise 
\begin{itemize}
\item Monte Carlo VaR embedded into the Monte Carlo simulation
\item Regression-based methods
\item Delta VaR under scenarios
\item ISDA's Standard Initial Margin (SIMM) under scenarios
\end{itemize} 

We skip the first option as too computationally expensive for ORE. In the current ORE release we focus on a relatively
simple regression approach as in \cite{Anfuso2016,LichtersEtAl}. Consider the netting set values $\NPV(t)$ and $\NPV(t+\Delta)$ that
are spaced one margin period of risk $\Delta$ apart. Moreover, let $F(t,t+\Delta)$ denote cumulative netting set cash
flows between time $t$ and $t+\Delta$, converted into the NPV currency. Let $X(t)$ then denote the netting set value
change during the margin period of risk excluding cash flows in that period:
$$
X(t) = \NPV(t+\Delta) + F(t, t+\Delta) - \NPV(t) 
$$  
ignoring discounting/compounding over the margin period of risk. We actually want to determine the distribution of
$X(t)$ conditional on the `state of the world' at time $t$, and pick a high (99\%) quantile to determine the Initial
Margin amount for each time $t$. Instead of working out the distribution, we content ourselves with estimating the
conditional variance $\V(t)$ or standard deviation $S(t)$ of $X(t)$, assuming a normal distribution and scaling $S(t)$
to the desired 99\% quantile by multiplying with the usual factor $\alpha=2.33$ to get an estimate of the Dynamic
Initial Margin $\DIM$:
$$
\V(t) = \E_t[X^2] - \E_t^2[X], \qquad S(t)=\sqrt{\V(t)}, \qquad \DIM(t) = \alpha \,S(t)
$$ 
We further assume that $\E_t[X]$ is small enough to set it to the expected value of $X(t)$ across all Monte Carlo
samples $X$ at time $t$ (rather than estimating a scenario dependent mean). The remaining task is then to estimate the
conditional expectation $\E_t[X^2]$. We do this in the spirit of the Longstaff Schwartz method using regression of
$X^2(t)$ across all Monte Carlo samples at a given time. As a regressor (in the one-dimensional case) we could use
$\NPV(t)$ itself. However, we rather choose to use an adequate market point (interest rate, FX spot rate) as regression
variable $x$, because this is generalised more easily to the multi-dimensional case. As regression basis functions we
use polynomials, i.e. regression functions of the form $c_0 + c_1\,x + c_2\,x^2 + ...+ c_n\,x^n$ where the order $n$ of
the polynomial can be selected by the user. Choosing the lowest order $n=0$, we obtain the simplest possible estimate,
the variance of $X$ across all samples at time $t$, so that we apply a single $\DIM(t)$ irrespective of the 'state of
the world' at time $t$ in that case.  The extension to multi-dimensional regression is also implemented in ORE. The user
can choose several regressors simultaneously (e.g. a EUR rate, a USD rate, USD/EUR spot FX rate, etc.) in order order to
cover complex multi-currency portfolios.

\medskip
Given the DIM estimate along all paths, we can next work out the Margin Value Adjustment \cite{Lichters} in discrete form
%{\color{red}
\begin{align}
\MVA &= \sum_{i=1}^n (f_b - s_I)\, \delta_i\: S_C(t_i)\: S_B(t_i) \times \E^N\left[
\DIM(t_i)\,D(t_i)\right]. \label{MVA} 
\end{align}
%}
with borrowing spread $f_b$ as in the FVA section \ref{sec:fva} and spread $s_I$ received on initial margin, both
spreads relative to the cash collateral rate.

\subsection{Collateral Model}\label{sec:app_collateral}

The collateral model implemented in ORE is based on the evolution of collateral account balances along each Monte Carlo
path taking into account thresholds, minimum transfer amounts and independent amounts defined in the CSA, as well as
margin periods of risk.

ORE computes the collateral requirement (aka \emph{Credit Support Amount}) through time along each Monte Carlo path
\begin{align}\label{eq:CSA}
CSA(t_m) &= 
\begin{cases}
\max(0, V_{set}(t_m) - I_A - T_{hold}),& V_{set}(t_m) - I_A \ge 0 \\
\min(0, V_{set}(t_m) - I_A + T_{hold}),& V_{set}(t_m) - I_A < 0
\end{cases}
\end{align}
where
\begin{itemize}
\item $V_{set}(t_m)$ is the value of the netting set as of
  time $t_m$,
  \item $T_{hold}$ is the threshold exposure below which no collateral is
  required (possibly asymmetric),
%\item $MTA$ is the minimum transfer amount for collateral margin
%  flow requests (possibly asymmetric)
\item $I_A$ is the sum of all collateral independent amounts attached to
  the underlying portfolio of trades (positive amounts imply that the bank
  has received a net inflow of independent amounts from the
  counterparty), assumed here to be cash.
\end{itemize}

As the collateral account already has a value of $C(t_m)$ at time $t_m$, the collateral shortfall is simply the
difference between $C(t_m)$ and $CSA(t_m)$. However, we also need to account for the possibility that margin calls
issued in the past have not yet been settled (for instance, because of disputes). If $M(t_m)$ denotes the net value of
all outstanding margin calls at $t_m$, and $\Delta(t)$ is the difference $\Delta(t) = CSA(t_m) - C(t_m) - M(t_m)$
between the {\em Credit Support Amount} and the current and outstanding collateral, then the actual margin
\emph{Delivery Amount} $D(t_m)$ is calculated as follows:
\begin{align}\label{eq:DA}
D(t_m) &= 
\begin{cases}
\Delta(t),& \left| \Delta(t) \right| \ge MTA \\
0,& \left| \Delta(t) \right| < MTA
\end{cases}
\end{align}
where $MTA$ is the minimum transfer amount.

\medskip Finally, the {\em Delivery Amount } is settled with a delay specified by the {\em Margin Period of Risk}
(MPoR) which leads to residual exposure and XVA even for daily margining, zero thresholds and minimum transfer amounts,
see for example \cite{Pykhtin2010}. A more detailed framework for collateralised exposure modelling is introduced in the
2016 article \cite{Andersen2016}, indicating a potential route for extending ORE.

\subsection{Exposure Allocation}\label{sec:app_allocation}

XVAs and exposures are typically computed at netting set level. For accounting purposes it is typically required to {\em
  allocate} XVAs from netting set to individual trade level such that the allocated XVAs add up to the netting set
XVA. This distribution is not trivial, since due to netting and imperfect correlation single trade (stand-alone) XVAs
hardly ever add up to the netting set XVA: XVA is sub-additive similar to VaR. ORE provides an allocation method
(labeled {\em marginal allocation } in the following) which slightly generalises the one proposed in
\cite{PykhtinRosen}. Allocation is done pathwise which first leads to allocated expected exposures and then to allocated
CVA/DVA by inserting these exposures into equations (\ref{CVA},\ref{DVA}). The allocation algorithm in ORE is as
follows:
\begin{itemize}
\item Consider the netting set's discounted $\NPV$ after taking collateral into account, on a given path at time $t$:
$$
E(t)=D(0,t)\,(\NPV(t)-C(t))
$$ 
\item On each path, compute contributions $A_i$ of the latter to trade $i$ as
$$
A_{i} (t) = \left\{ \begin{array}{ll} 
E(t) \times \NPV_{i}(t) / \NPV(t), & |\NPV(t)| > \epsilon \\
E(t) / n, & |\NPV(t)| \le \epsilon
\end{array}
\right. 
$$
with number of trades $n$ in the netting set and trade $i$'s value $\NPV_i(t)$.
\item The $\EPE$ fraction allocated to trade $i$ at time $t$ by averaging over paths:
$$
\EPE_i(t) = \E\left[ A_i^+(t) \right]
$$
\end{itemize}
By construction, $\sum_i A_i(t) = E(t)$ and hence $\sum_i \EPE_i(t) = \EPE(t)$.\\

We introduced the {\em cutoff } parameter $\epsilon>0$ above in order to handle the case where the netting set value
$\NPV(t)$ (almost) vanishes due to netting, while the netting set 'exposure' $E(t)$ does not. This is possible in a
model with nonzero MTA and MPoR. Since a single scenario with vanishing $\NPV(t)$ suffices to invalidate the expected
exposure at this time $t$, the cutoff is essential. Despite introducing this cutoff, it is obvious that the marginal
allocation method can lead to spikes in the allocated exposures. And generally, the marginal allocation leads to both
positive and negative $\EPE$ allocations.

\medskip As a an example for a simple alternative to the marginal allocation of $\EPE$ we provide allocation based on
today's single-trade CVAs
$$
w_i = \CVA_i / \sum_i \CVA_i.
$$
This yields allocated exposures proportional to the netting set exposure, avoids spikes and negative $\EPE$, but does
not distinguish the 'direction' of each trade's contribution to $\EPE$ and $\CVA$.

\subsection{Sensitivity Analysis}\label{sec:app_sensi}

ORE's sensitivity analysis framework uses ``bump and revalue'' to compute Interest Rate, FX, Inflation, Equity and Credit sensitivities to
\begin{itemize}
\item Discount curves  (in the zero rate domain)
\item Index curves (in the zero rate domain)
\item Yield curves including e.g. equity forecast yield curves (in the zero rate domain)
\item FX Spots
\item FX volatilities
\item Swaption volatilities, ATM matrix or cube 
\item Cap/Floor volatility matrices (in the caplet/floorlet domain)
\item Default probability curves (in the ``zero rate'' domain, expressing survival probabilities $S(t)$ in term of zero rates $z(t)$ via $S(t)=\exp(-z(t)\times t)$ with Actual/365 day counter)
\item Equity spot prices
\item Equity volatilities, ATM or including strike dimension 
\item Zero inflation curves
\item Year-on-Year inflation curves
\item CDS volatilities
\item Base correlation curves
\end{itemize}

Apart from first order sensitivities (deltas), ORE computes second order sensitivities (gammas and cross gammas) as well. Deltas are computed using up-shifts and base values as
$$
\delta = \frac{f(x+\Delta)-f(x)}{\Delta},
$$ 
where the shift $\Delta$ can be absolute or expressed as a relative move $\Delta_r$ from the current level, $\Delta=x\,\Delta_r$. Gammas are computed using up- and down-shifts
$$
\gamma = \frac{f(x+\Delta)+f(x-\Delta) - 2\,f(x)}{\Delta^2},
$$ 
cross gammas using up-shifts and base values as
$$
\gamma_{cross} = \frac{f(x+\Delta_x,y+\Delta_y)-f(x+\Delta_x,y) -f(x,y+\Delta_y) + f(x,y)}{\Delta_x\,\Delta_y}.
$$ 

From the above it is clear that this involves the application of 1-d shifts (e.g. to discount zero curves) and 2-d shifts (e.g. to Swaption volatility matrices). The structure of the shift curves/matrices does not have to match the structure of the underlying data to be shifted, in particular the shift ``curves/matrices'' can be less granular than the market to be shifted. 
Figure \ref{fig_shiftcurve} illustrates for the one-dimensional case how shifts are applied.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.6]{shiftcurve.pdf}
\end{center}
\caption{1-d shift curve (bottom) applied to a more granular underlying curve (top). }
\label{fig_shiftcurve}
\end{figure} 

Shifts at the left and right end of the shift curve are extrapolated flat, i.e. applied to all data of the original curve to the left and to the right of the shift curve ends. In between, all shifts are distributed linearly as indicated to the left and right up to the adjacent shift grid points. As a result, a parallel shift of the all points on the shift curve yields a parallel shift of all points on the underlying curve.   \\

The two-dimensional case is covered in an analogous way, applying flat extrapolation at the boundaries and ``pyramidal-shaped'' linear interpolation for the bulk of the points. 

The details of the computation of sensitivities to implied volatilities in strike direction can be summarised as
follows, see also table \ref{sensi_config_overview} for an overview of the admissible configurations and the results
that are obtained using them.

\medskip
For {\em Swaption Volatilities}, the initial market setup can be an ATM surface only or a full cube. The simulation
market can be set up to simulate ATM only or to simulate the full cube, but the latter choice is only possible if a full cube is set
up in the initial market. The sensitivity set up must match the simulation setup with regards to the strikes (i.e. it
is ATM only if and only if the simulation setup is ATM only, or it must contain exactly the same strike spreads relative
to ATM as the simulation setup). Finally, if the initial market setup is a full cube, and the simulation / sensitivity
setup is to simulate ATM only, then sensitivities are computed by shifting the ATM volatility w.r.t. the given shift size and type and
shifting the non-ATM volatilities by the same absolute amount as the ATM volatility.

\medskip
For {\em Cap/Floor Volatilities}, the initial market setup always contains a set of fixed strikes, i.e. there is no
distinction between ATM only and a full surface. The same holds for the simulation market setup. The sensitivity setup
may contain a different strike grid in this case than the simulation market. Sensitivity are computed per expiry and
per strike in every case.

\medskip
For {\em Equity Volatilities}, the initial market setup can be an ATM curve or a full surface. The simulation market can
be set up to simulate ATM only or to simulate the full surface, where a full surface is allowed even if the initial market setup in an
ATM curve only. If we have a full surface in the initial market and simulate the ATM curve only in the simulation market, sensitivities
are computed as in the case of Swaption Volatilities, i.e. the ATM volatility is shifted w.r.t. the specified shift size
and type and the non-ATM volatilities are shifted by the same absolute amount as the ATM volatility. If the simulation
market is set up to simulate the full surface, then all volatilities are shifted individually using the specified shift size and type. In
every case the sensitivities are aggregated on the ATM bucket in the sensitivity report.

\medskip
For {\em FX Volatilities}, the treatment is similar to Equity Volatilities, except for the case of a full surface
definition in the initial market and an ATM only curve in the simulation market. In this case, the pricing in the
simulation market is using the ATM curve only, i.e. the initial market's smile structure is lost.

\medskip
For {\em CDS Volatilities} only an ATM curve can be defined.

\medskip
In all cases the smile dynamics is ``sticky strike'', i.e. the implied vol used for pricing a deal does not change if
the underlying spot price changes.

\begin{table}[hbt]
  \scriptsize
  \begin{center}
    \begin{tabular}{l | l | l | l | l | l}
      \hline
      Type & Init Mkt. Config. & Sim. Mkt Config. & Sensitivity Config. & Pricing & Sensitivities w.r.t. \\
      \hline
      Swaption & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      Swaption & Cube & Simulate Cube & Shift Smile Strikes & Full Cube & Smile Strike Shifts\footnote{smile
                                                                          strike spreads must match simulation market configuration} \\
      Swaption & Cube & Simulate ATM only & Shift ATM only & Full Cube & ATM Shifts\footnote{smile is shifted in parallel\label{sensismileparallel}} \\
      \hline
      Cap/Floor & Surface & Simulate Surface & Shift Smile Strikes & Full Surface & Smile Strike Shifts \\
      \hline
      Equity & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      Equity & ATM & Simulate Surface & Shift ATM only & ATM Curve & Smile Strike Shifts\footnote{result sensitivities
                                                                     are aggregated on ATM\label{sensiaggatm}} \\
      Equity & Surface & Simulate ATM only & Shift ATM only & Full Surface & ATM Shifts\textsuperscript{\ref{sensismileparallel}} \\
      Equity & Surface & Simulate Surface & Shift ATM only & Full Surface & Smile Strike Shifts\textsuperscript{\ref{sensiaggatm}} \\
      \hline
      FX & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      FX & ATM & Simulate Surface & Shift ATM only & ATM Curve & Smile Strike Shifts\textsuperscript{\ref{sensiaggatm}} \\
      FX & Surface & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      FX & Surface & Simulate Surface & Shift ATM only & Full Surface & Smile Strike Shifts\textsuperscript{\ref{sensiaggatm}} \\
      \hline
      CDS & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
    \end{tabular}
    \caption{Admissible configurations for Sensitivity computation in ORE}
    \label{sensi_config_overview}
  \end{center}
  \end{table}

\subsection{Value at Risk}\label{sec:app_var}

For the computation of the parametric, or variance-covariance VaR, we rely on a second order sensitivity-based P\&L approximation

\begin{eqnarray}\label{taylorPl2}
  \pi_S & = & \sum_{i=1}^n D^i_{T_i}\,V\cdot Y_i 
        + \frac{1}{2} \sum_{i,j=1}^n D^{i,j}_{T_i,T_j}\,V\cdot Y_i\cdot Y_j
\end{eqnarray}

with 
\begin{itemize}
\item portfolio value $V$
\item random variables $Y_i$ representing risk factor returns; these are assumed to be multivariate normally distributed with zero mean
and covariance matrix matrix $C = \{ \rho_{i,k} \sigma_i \sigma_k \}_{i,k}$, where $\sigma_i$ denotes the standard
deviation of $Y_i$; covariance matrix $C$ may be estimated using the Pearson estimator on historical return data
$\{ r_i(j) \}_{i,j}$. Since the raw estimate might not be positive semidefinite, we apply a salvaging algorithm to
ensure this property, which basically replaces negative Eigenvalues by zero and renormalises the resulting matrix, see
\cite{corrSalv};
\item first or second order derivative operators $D$, depending
on the market factor specific shift type $T_i \in \{ A,R,L \}$ (absolute shifts, relative shifts, absolute log-shifts), i.e.
\begin{eqnarray*}\label{derivs}
  D^i_A \,V(x) &=& \frac{\partial V(x)}{\partial x_i} \\
  D^i_R \,V(x) = D^i_L f(x) &=& x_i\frac{\partial V(x)}{\partial x_i}
\end{eqnarray*}
and using the short hand notation
\begin{equation*}
  D^{i,j}_{T_i,T_j} V(x) = D^i_{T_i} D^j_{T_j} V(x)
\end{equation*}
In ORE, these first and second order sensitivities are computed as finite difference
approximations (``bump and revalue'').
\end{itemize}

To approximate the $p$-quantile of $\pi_S$ in \eqref{taylorPl2} ORE offers the techniques outlined below.

\subsubsection*{Delta Gamma Normal Approximation}
 
The distribution of \eqref{taylorPl2} is non-normal due to the second order terms. 
The delta gamma normal approximation in ORE computes mean $m$ and variance $v$ of the portfolio value change $\pi_S$ (discarding moments higher than two) following \cite{alexander} and provides a simple VaR estimate 
$$
VaR = m + N^{-1}(q)\,\sqrt{v}
$$
for the desired quantile $q$ ($N$ is the cumulative standard normal distribution). Omitting the second order terms in \eqref{taylorPl2} yields the delta normal approximation.
 
\subsubsection*{Monte Carlo Simulation}

By simulating a large number of realisations of the return vector $Y=\{ Y_i \}_i$ and computing the corresponding
realisations of $\pi_S$ in \eqref{taylorPl2} we can estimate the desired quantile as the quantile of the empirical
distribution generated by the Monte Carlo samples. Apart from the Monte Carlo Error no approximation is involved in this
method, so that albeit slow it is well suited to produce values against which any other approximate approaches can be tested. Numerically, the simulation is implemented using a Cholesky Decomposition
of the covariance matrix $C$ in conjunction with a pseudo random number generator (Mersenne Twister) and an
implementation of the inverse cumulative normal distribution to transform $U[0,1]$ variates to $N(0,1)$ variates.

\end{appendix}

%========================================================
%\section{References}
%========================================================

\begin{thebibliography}{*}

\bibitem{ORE} \url{http://www.opensourcerisk.org}

\bibitem{QL} \url{http://www.quantlib.org}
 
\bibitem{QRM} \url{http://www.quaternion.com}

\bibitem{quantlib-install} \url{http://quantlib.org/install/vc10.shtml}

%\bibitem{confluence} https://confluence.atlassian.com/bitbucket/set-up-git-744723531.html

\bibitem{git-download} \url{https://git-scm.com/downloads}

\bibitem{boost-binaries} \url{https://sourceforge.net/projects/boost/files/boost-binaries}

\bibitem{boost} \url{http://www.boost.org}

\bibitem{jupyter} \url{http://jupyter.org}

\bibitem{Anaconda} \url{https://docs.continuum.io/anaconda}

\bibitem{LO} \url{http://www.libreoffice.org}

%\bibitem{xlwings} \url{http://www.xlwings.org}

\bibitem{bcbs128} Basel Committee on Banking Supervision, {\em International Convergence of Capital Measurement and
    Capital Standards, A Revised Framework}, \url{http://www.bis.org/publ/bcbs128.pdf}, June 2006

\bibitem{bcbs189} Basel Committee on Banking Supervision, {\em Basel III: A global regulatory framework for more
    resilient banks and banking systems}, \url{http://www.bis.org/publ/bcbs189.pdf}, June 2011

\bibitem{BrigoMercurio} Damiano Brigo and Fabio Mercurio, {\em Interest Rate Models: Theory and Practice, 2nd Edition},
  Springer, 2006.

\bibitem{Pykhtin2010} Michael Pykhtin, {\em Collateralized Credit Exposure}, in Counterparty Credit Risk, (E. Canabarro,
  ed.), Risk Books, 2010

\bibitem{PykhtinRosen} Michael Pykhtin and Dan Rosen, {\em Pricing Counterparty Risk at the Trade Level and CVA
    Allocations}, Finance and Economics Discussion Series, Divisions of Research \& Statistics and Monetary Affairs,
  Federal Reserve Board, Washington, D.C., 2010

\bibitem{Gregory12} Jon Gregory, {\em Counterparty Credit Risk and Credit Value Adjustment, 2nd Ed.}, Wiley Finance,
  2013.

\bibitem{Lichters} Roland Lichters, Roland Stamm, Donal Gallagher, {\em Modern Derivatives Pricing and Credit Exposure
    Analysis, Theory and Practice of CSA and XVA Pricing, Exposure Simulation and Backtesting}, Palgrave Macmillan,
  2015.

\bibitem{Anfuso2016} Fabrizio Anfuso, Daniel Aziz, Paul Giltinan, Klearchos Loukopoulos, {\em A Sound Modelling and
    Backtesting Framework for Forecasting Initial Margin Requirements},
  \url{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2716279}, 2016

\bibitem{Andersen2016} Leif B. G. Andersen, Michael Pykhtin, Alexander Sokol, {\em Rethinking Margin Period of Risk},
  http://papers.ssrn.com/sol3/papers.cfm?abstract\_id=2719964, 2016

  % \bibitem{SIMM}{SIMM Methodology\\ \tiny
  %   http://www2.isda.org/attachment/ODM1Mw==/ISDA\%20SIMM\%20Methodology\_7\%20April\%202016\_v3.15\%20(PUBLIC).pdf}

  % \bibitem{SIMM_Data_Standards}{SIMM Risk Data Standards\\ \tiny
  %   https://www2.isda.org/attachment/ODQzMg==/Risk\%20Data\%20Standards\_24\%20May\%202016\_v1.22\%20(PUBLIC).pdf}

  % \bibitem{OO} http://www.openoffice.org

\bibitem{LichtersEtAl} Peter Caspers, Paul Giltinan, Paul; Lichters, Roland; Nowaczyk , Nikolai. {\em Forecasting Initial Margin Requirements – A Model Evaluation}, Journal of Risk Management in Financial Institutions, Vol. 10 (2017), No. 4, \url{https://ssrn.com/abstract=2911167}

\bibitem{corrSalv} R. Rebonato and P. Jaeckel, The most general methodology to create a valid correlation matrix for
  risk management and option pricing purposes, The Journal of Risk, 2(2), Winter 1999/2000,
  \url{http://www.quarchome.org/correlationmatrix.pdf}

\bibitem{alexander} Carol Alexander, Market Risk Analysis, Volume IV, Value at Risk Models, Wiley 2009

\end{thebibliography}

\newpage
\addcontentsline{toc}{section}{Todo}
\listoftodos[Todo]
%\todos

\end{document}
