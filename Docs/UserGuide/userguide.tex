\documentclass[12pt, a4paper]{article}

% Avoid useless warnings about included images
% Reference: https://tex.stackexchange.com/a/78020
%\pdfsuppresswarningpagegroup=1

% Use \hypersetup here to get rid of the ugly boxes around links
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue
}

% Avoid warning about missing font for \textbackslash character.
\usepackage[T1]{fontenc}

% For nicer paragraph spacing
\usepackage{parskip}

\usepackage[disable]{todonotes}
%\usepackage{todonotes}

\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}
%\usepackage[miktex]{gnuplottex}
%\ShellEscapetrue
\usepackage{epstopdf}
\usepackage{longtable}
\usepackage{floatrow}
\usepackage{makecell}

% Use \setminted here to get horizontal line above and below listings
\usepackage{minted}
\setminted{
   frame=lines,
   framesep=2mm
}

\usepackage{textcomp}
\usepackage{color,soul}
\usepackage[font={small,it}]{caption}
\floatsetup[listing]{style=Plaintop}    
\floatsetup[longlisting]{style=Plaintop}    

% Turn off indentation but allow \indent command to still work.
\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

\addtolength{\textwidth}{0.8in}
\addtolength{\oddsidemargin}{-.4in}
\addtolength{\evensidemargin}{-.4in}
\addtolength{\textheight}{1.6in}
\addtolength{\topmargin}{-.8in}

\usepackage{longtable,supertabular}
\usepackage{listings}
\lstset{
  frame=top,frame=bottom,
  basicstyle=\ttfamily,
  language=XML,
  tabsize=2,
  belowskip=2\medskipamount
}

% All listings have a left aligned caption. This looks better as the listings themselves are left aligned.
\captionsetup[listing]{justification=justified,singlelinecheck=false}

\usepackage{tabu}
\tabulinesep=1.0mm
\restylefloat{table}

\usepackage{siunitx}
\usepackage{tablefootnote}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{footmisc}
\newenvironment{longlisting}{\captionsetup{type=listing}}{}
%\usepackage[colorlinks=true]{hyperref}

% Inline code fragments can run over the page boundary without ragged right.
\AtBeginDocument{\raggedright}

\newcommand{\UserGuide}{}

\renewcommand\P{\ensuremath{\mathbb{P}}}
\newcommand\E{\ensuremath{\mathbb{E}}}
\newcommand\Q{\ensuremath{\mathbb{Q}}}
\newcommand\I{\mathds{1}}
\newcommand\F{\ensuremath{\mathcal F}}
\newcommand\V{\ensuremath{\mathbb{V}}}
\newcommand\YOY{{\rm YOY}}
\newcommand\Prob{\ensuremath{\mathbb{P}}}
\newcommand{\D}[1]{\mbox{d}#1}
\newcommand{\NPV}{\mathit{NPV}}
\newcommand{\CVA}{\mathit{CVA}}
\newcommand{\DVA}{\mathit{DVA}}
\newcommand{\FVA}{\mathit{FVA}}
\newcommand{\COLVA}{\mathit{COLVA}}
\newcommand{\FCA}{\mathit{FCA}}
\newcommand{\FBA}{\mathit{FBA}}
\newcommand{\KVA}{\mathit{KVA}}
\newcommand{\MVA}{\mathit{MVA}}
\newcommand{\PFE}{\mathit{PFE}}
\newcommand{\EE}{\mathit{EE}}
\newcommand{\EPE}{\mathit{EPE}}
\newcommand{\ENE}{\mathit{ENE}}
\newcommand{\EEPE}{\mathit{EEPE}}
\newcommand{\EEE}{\mathit{EEE}}
\newcommand{\EAD}{\mathit{EAD}}
\newcommand{\PE}{\mathit{PE}}
\newcommand{\NE}{\mathit{NE}}
\newcommand{\RC}{\mathit{RC}}
\newcommand{\RW}{\mathit{RW}}
\newcommand{\NS}{\mathit{NS}}
\newcommand{\PD}{\mathit{PD}}
\newcommand{\LGD}{\mathit{LGD}}
\newcommand{\DIM}{\mathit{DIM}}
\newcommand{\DF}{\mathit{DF}}
\newcommand{\MA}{\mathit{MA}}
\newcommand{\SCVA}{\mathit{SCVA}}
\newcommand{\bs}{\textbackslash}
\newcommand{\REDY}{\color{red}Y}
\newcommand{\IA}{\mathit{IA}}
\newcommand{\Th}{\mathit{TH}}
\newcommand{\CSA}{\mathit{CSA}}

\begin{document}

%\title{Open Source Risk Engine \\ User Guide  }
\title{ORE User Guide}
%\author{Quaternion Risk Management Ltd.}
%\author{Acadia Inc.}
\date{24 May 2024}
\maketitle

\newpage

%-------------------------------------------------------------------------------
\section*{Document History}

\begin{center}
\begin{supertabular}{|l|l|p{9cm}|}
\hline
Date & Author & Comment \\
\hline
7 October 2016 & Quaternion & initial release\\
28 April 2017 & Quaternion  & updates for release 2\\
7 December 2017 & Quaternion & updates for release 3\\
20 March 2019 & Quaternion & updates for release 4\\
19 June 2020 & Quaternion & updates for release 5\\
30 June 2021 & Acadia & updates for release 6\\
16 September 2022 & Acadia & updates for release 7\\
6 December 2022 & Acadia & updates for release 8\\
31 March 2023 & Acadia & updates for release 9\\
16 June 2023 & Acadia & updates for release 10\\
16 October 2023 & Acadia & updates for release 11\\
24 May 2024 & Acadia & updates for release 12\\
\hline
\end{supertabular}
\end{center}

\newpage

\tableofcontents
\newpage

\section{Introduction}

The {\em Open Source Risk Project} \cite{ORE} aims at providing a transparent platform for pricing and risk analysis
that serves as
%\medskip
\begin{itemize}
\item a benchmarking, validation, training, and teaching reference,
\item an extensible foundation for tailored risk solutions.
\end{itemize}

Its main software project is {\em Open Source Risk Engine} (ORE), an application that provides
\begin{itemize}
\item a Monte Carlo simulation framework for contemporary risk analytics and value adjustments
\item simple interfaces for trade data, market data and system configuration
\item simple launchers and result visualisation in Jupyter, Excel, LibreOffice
\item unit tests and various examples.  
\end{itemize}
ORE is open source software, provided under the Modified BSD License. It is based 
on QuantLib, the open source library for quantitative finance \cite{QL}.

%\medskip
\subsubsection*{Audience}
The project aims at reaching quantitative risk ma\-nage\-ment practitioners (be it in financial institutions, audit
firms, consulting companies or regulatory bodies) who are looking for accessible software solutions, and quant
developers in charge of the implementation of pricing and risk methods similar to those in ORE. Moreover, the project
aims at reaching academics and students who would like to teach or learn quantitative risk management using a freely
available, contemporary risk application. And in the meantime, as ORE is used in risk services at industrial scale since 2018
with over 150 users, ORE is aimed at firms that consider the replacement of third party licensed software. 

\subsubsection*{Contributions}
Quaternion Risk Management \cite{QRM} has been committed to sponsoring the Open Source Risk project through ongoing project
administration, through providing an initial release and a series of subsequent releases in order to achieve a wide
analytics, product and risk factor class coverage. Since Quaternion's acquisition by Acadia Inc. in February 2021,
Acadia \cite{acadia} has been committed to continue the sponsorship. The Open Source Risk project work continues with
former Quaternion operating as Acadia's Quantitative Services unit. And with Acadia's acquisiton by London Stock
Exchange Group (LSEG) in 2023, the journey continues under the roof of LSEG Post Trade.

The community is invited to contribute to ORE, for example through
feedback, discussions and suggested enhancement in the forum on the ORE site \cite{ORE}, as well as contributions of ORE
enhancements in the form of source code. See the FAQ section on the ORE site \cite{ORE} on how to get involved.

\subsection{Scope}

ORE currently provides portfolio pricing, cash flow generation, market risk analysis and a range of contemporary derivative
portfolio analytics. The latter are based on a Monte Carlo simulation framework which yields the evolution of various exposure measures:
\begin{itemize}
\item EE aka EPE (Expected Exposure or Expected Positive Exposure)
\item ENE (Expected Negative Exposure, i.e. the counterparty's perspective)
\item 'Basel' exposure measures relevant for regulatory capital charges under internal model methods 
\item PFE (Potential Future Exposure at some user defined quantile)
\end{itemize}
and derivative value adjustments (xVA)
\begin{itemize}
\item CVA (Credit Value Adjustment)
\item DVA (Debit Value Adjustment)
\item FVA (Funding Value Adjustment)
\item COLVA (Collateral Value Adjustment)
\item MVA (Margin Value Adjustment)
\end{itemize}
for portfolios with netting, variation and initial margin agreements. 

\medskip
The market risk framework provides
\begin{itemize}
\item sensitivity analysis, also in the ``par'' domain
\item stress testing, also in the ``par'' domain
\item several parametric VaR versions (Delta VaR, Delta-Gamma Normal VaR, Delta-Gamma VaR with Cornish-Fisher expansion and Saddlepoint method)
\item historical simulation VaR
\item P\&L and P\&L explain
\end{itemize}
across all asset classes and products. 

\medskip
Thanks to Acadia's open-source strategy, ORE's financial instrument scope was extended beyond the initial vanilla scope with quarterly releases since version 7 to cover
\begin{itemize}
\item "First Generation" Equity and FX Exotics, released September with ORE v7
\item Commodity products (Swaps, Basis Swaps, Average Price Options, Swaptions), released December 22 with ORE v8
\item Credit products (Index CDS and Index CDS Options, Credit-Linked Swaps, Synthetic CDOs), released March 23 with ORE v9
\item Bond products and Hybrids (Bond Options, Bond Repos, Bond TRS, Composite Trades, Convertible Bonds, Generic TRS with mixed basket underlyings, CFDs), released in June 23 with ORE v10
\item a Scripted Trade framework in October 23 with ORE v11: This allows the modelling of complex hybrid payoffs
  such as Accumulators, TARFs, PRDCs, Basket Options, etc, across IR, FX, INF, EQ, COM classes.  Scripted Trades are fully
  integrated into the market risk and exposure simulation frameworks, supported by American Monte Carlo methods for pricing
  and exposure simulation. The user can now extend the instrument scope conveniently by adding payoff scripts (embedded into
  the trade XML or in separate script "library" XML) and without recompiling the code base.
\item Formula-based legs, Callable Swaps, Flexi Swaps, Balance Guaranteed Swaps and American Swaptions in May 24 with ORE v12
\end{itemize}

These contributions were accompanied by analytics extensions to enhance ORE usability
\begin{itemize}
\item Exposure simulation for xVA and PFE, adding Commodity to the asset class coverage, and adding American Monte Carlo for Exotics, released in December 22 with ORE v8
\item Market Risk including multi-threaded sensitivity analysis, par sensitivity conversion, parametric delta/gamma VaR with Cornish-Fisher expansion and Saddlepoint method, released in March 23 with ORE v9
\item Portfolio Credit Model, released in June 23 with ORE v10
\item ISDA's Standard Initial Margin Model (SIMM), released in June 23 with ORE v10
\item Historical Simulation VaR, P\&L and P\&L Explain, released in May 24 with ORE v12
\end{itemize}

\medskip 
The product coverage of the latest release of ORE is sketched in Table \ref{tab_coverage}.
\begin{table}[hbt]
\scriptsize
\begin{center}
\begin{tabular}{|l|p{1.5cm}|p{1.5cm}|p{1.2cm}|p{1.5cm}|}
\hline
Product & Pricing and Cashflows & Sensitivity Analysis & Stress Testing & Exposure Simulation \& XVA\\
\hline
Fixed and Floating Rate Bonds/Loans & Y & Y & Y & N \\
\hline
Interest Rate Swaps & Y & Y & Y & Y\\
\hline
Caps/Floors & Y & Y & Y & Y\\
\hline
Swaptions, Callable Swaps & Y & Y & Y &Y \\
\hline
Constant Maturity Swaps, CMS Caps/Floors & Y & Y & Y & Y\\
\hline
FX Forwards and Average Forwards & Y & Y & Y & Y \\
\hline
Cross Currency Swaps & Y & Y & Y & Y \\
\hline
FX European and Asian Options & Y & Y & Y & Y\\
\hline
FX Exotic Options (see below) & Y & Y & Y & Y\\
\hline
Equity Forwards & Y & Y & Y & Y\\
\hline
Equity Swaps & Y & Y & Y & N\\
\hline
Equity European and Asian Options & Y & Y & Y & Y \\
\hline
Equity Exotic Options (see below)  & Y & Y & Y & Y \\
\hline
Equity Future Options & Y & Y & Y & Y \\
\hline
Commodity Forwards and Swaps & Y & Y & Y & Y\\
\hline
Commodity European and Asian Options & Y & Y & Y & Y \\
\hline
Commodity Digital Options & Y & Y & Y & Y \\
\hline
Commodity Swaptions & Y & Y & Y & Y\\
\hline
CPI Swaps & Y & Y & N & Y \\
\hline
CPI Caps/Floors & Y & Y & N & N\\
\hline
Year-on-Year Inflation Swaps & Y & Y & N & Y \\
\hline
Year-on-Year Inflation Caps/Floors & Y & Y & N & N\\
\hline
Credit Default Swaps, Options & Y & Y & N & Y \\
\hline
Index Credit Default Swaps, Options & Y & Y & N & Y \\
\hline
Credit Linked Swaps & Y & Y & N & Y \\
\hline
Index Tranches, Synthetic CDOs & Y & Y & N & Y \\
\hline
Composite Trades & Y & Y & Y & Y \\
\hline
Total Return Swaps and Contracts for Difference & Y & Y & Y & Y \\
\hline
Convertible Bonds & Y & Y & Y & N \\
\hline
ASCOTs & Y & Y & Y & Y \\
\hline
Scripted Trades & Y & Y & Y & Y \\
\hline
Flexi Swaps and Balance Guaranteed Swaps & Y & Y & Y & Y \\
\hline
\end{tabular}
\caption{ORE product coverage. FX/Equity Exotics include Barrier, Digital, Digital Barrier (FX only), Double Barrier, European Barrier, KIKO Barrier (FX only), Touch and Double Touch Options, Outperformance options and Pairwise Variance Swaps. Scripted Trades cover single and multi-asset products across all asset classes except Credit (so far), see Example\_52 and the separate documentation in Docs/ScriptedTrade.}
\label{tab_coverage}
\end{center}
\end{table}

\medskip The simulation models applied in ORE's risk factor evolution implement the models discussed in detail in {\em
  Modern Derivatives Pricing and Credit Exposure Analysis} \cite{Lichters}: The IR/FX/INF/EQ risk factor evolution is based on
a cross currency model consisting of an arbitrage free combination of Linear Gauss Markov models for all interest rates
and lognormal processes for FX rates and EQ prices, Dodgson-Kainth (or Jarrow-Yildirim) models for inflation. The model components are calibrated to cross currency discounting and forward curves, Swaptions, FX Options, EQ Options and CPI caps/floors. With the 8th release, Commodity simulation has been added, as well as the foundation for a multi-factor Hull-White based IR/FX/COM simulation model. 

\subsection{ORE in Python or Java}

ORE is written in C++ and comes with a command line executable {\tt ore.exe} that supports batch processes. 
But since early versions of ORE we also provide language bindings following QuantLib's example using SWIG, in ORE's case with focus on Python and Java modules. 
The ORE SWIG module extends (contains) the QuantLib SWIG module and offers moreover access to a part of ORE's functionality.
Since ORE v9, Python {\em wheels} are provided for each release, so that users can install the most recent ORE Python module by calling

\medskip
\centerline{\tt pip install open-source-risk-engine}
 
\medskip
See section \ref{example:42} on how to use ORE-Python. 

\subsection{Roadmap}

It is planned that subsequent ORE releases will also provide the calculation of {\bf regulatory capital charges} 
\begin{itemize}
\item for Counterparty Credit Risk under the standardised approach (SA-CCR)
\item for Market Risk (SMRC, FRTB-SA)
\item for CVA Risk (BA-CVA, SA-CVA)
\end{itemize}

{\bf Performance:} ORE v12 contains applications of {\bf AAD} for sensitivity analysis, CVA sensitivity (proof-of concept stage),
as well as applications of {\bf GPUs} to parallelize computations (see Examples 56 and 61), with significant
speed-ups. Both areas are active work in progress, and further enhancements and tests should be released with the next versions.

{\bf ORE Python:} Moreover, there is demand among ORE users for extended coverage of the ORE-Python version, so that we also expect steady growth
of the Python wrapper around ORE.

{\bf ORE Service:} ORE v12 contains an implementation of a restful API around ORE, see folder ore/Api and the example therein.
This is written in Python, uses the ORE-Python module and the Flask web framework. This implementation is proof of concept,
for demonstration purposes and to encourage the community to extend and contribute alternatives.

\subsection{Further Resources}
\begin{itemize}
\item Open Source Risk Project site: \url{http://www.opensourcerisk.org}
\item Frequently Asked Questions: \url{http://www.opensourcerisk.org/faqs}
\item Forum: \url{http://www.opensourcerisk.org/forum}
\item Source code and releases: \url{https://github.com/opensourcerisk/engine}
\item Language bindings: \url{https://github.com/opensourcerisk/ore-swig}
\item Follow ORE on Twitter {\tt @OpenSourceRisk} for updates on releases and events
\end{itemize}
 
\subsubsection*{Organisation of this document}

This document focuses on instructions how to use ORE to cover basic workflows from individual deal analysis to portfolio
processing. After an overview over the core ORE data flow in section \ref{sec:process} and installation instructions in
section \ref{sec:installation} we start in section \ref{sec:examples} with a series of examples that illustrate how to
launch ORE using its command line application, and we discuss typical results and reports. We then illustrate in section
\ref{sec:visualisation} interactive analysis of resulting 'NPV cube' data. The final sections of this text document ORE
parametrisation and the structure of trade and market data input.

%========================================================
\section{Release Notes}\label{sec:releasenotes}
%========================================================

See the full history of release notes in {\tt News.txt} in the top level directory of the ORE's github repository.

\medskip
This section summarises the notable changes between release 11 (October 2023) and 12 (May 2024).

\bigskip
INSTRUMENTS \& PRICING
\begin{itemize}
\item add formula-based leg (ticket 12263), see Example 64
\item add Callable Swap (ticket 12370), Example, see extended Example 5
\item add Flexi Swap (ticket 12370), see Example 65
\item add Balance Guaranteed Swap (ticket 12370), see Example 66
\item add American Swaption with finite difference pricing in LGM (ticket 11618), see extended Example 4
\item add Equity Outperformance Option (ticket 12456)
\item add Pairwise Variance Swap (ticket 12456)
\item add analytic pricer for Overnight Index Swaptions (ticket 11974)
\item add finite difference LGM pricers for Bermudan and European Swaptions
\item add numerical integration LGM pricer for European Swaptions
\item support indexed cashflows in AMC (ticket 12206)
\item scripted trades: support LGM1F using finite difference and numerical integration (ticket 12306)
\item scripted trades: support generating past cashflows (ticket 12417)
\item scripted trades: handle near-collinear predictor variables in linear regression for AMC (ticket 12419)
\item support for rules-based Bermudan exercise dates (ticket 11947)
\item improved AMC regression model for resetting XCCY Swaps (ticket 12119)
\item add Burley 2020 scrambled Sobol sequence (ticket 12140), used in Example 56
\item rates curve building: support mixed interpolations (ticket 12159), used in Example 53
\item add priorities to yield curve segments (ticket 9557)
\item configurable settlement date flows handling (ticket 12372)
\item introduce notional payment lag for resetting Cross Currency Swaps (ticket 12379)
\item support security spreads over discount in Single Currency Swaps, Swaptions and Scripted Trades
  (ticket 12201 and 12275)
\item add pricing engines that provide analytical deltas and gammas for Swaps, Swaptions, FX Forwards,
  FX Options (ticket 12456)
\end{itemize}

\bigskip
MARKETS \& TERMSTRUCTURES
\begin{itemize}
\item SABR pricing for Swaptions and Caps/Floors (ticket 12325), see Example 59
\item allow importing external SABR parameters (ticket 12491)
\end{itemize}

\bigskip
ANALYTICS
\begin{itemize}
\item add XVA Sensitivitiy Proof of Concept using AAD (ticket 12028), see Example 56
\item support overlapping close-out date grid in exposure/XVA (ticket 9859), see Example 60
\item add scenario analytic (ticket 11990), see Example 57
\item add historical simulation VaR analytic (ticket 9793), see Example 58
\item add IM Schedule analytic (ticket 11730), see extended Example 44
\item add P\&L and P\&L Explain analytics (ticket 12458), see Example 62
\item add par stress test support (ticket 12148), see Example 63
\item add XVA stress testing (ticket 12376), see Example 67
\item add XVA bump \& revalue sensitivities (ticket 12437), see Example 68
\item add bond spread imply to ORE (ticket 12370)
\item add market risk backtest analytic (ticket 12370)
\item add calculation of stoplight bounds for backtests (ticket 12370)
\item feed t0 Variation Margin into xVA calculation (ticket 12241)
\item support flat Initial Margin projection in exposure calculations (ticket 12318)
\item fix the Independent Amount implementation in exposure calculations (ticket 12277)
\item introduce configurable shift size and difference scheme per product for sensitivity analysis (ticket 12014)
\item improve performance of SIMM calculations for large CRIF files (ticket 12397)
\item output effective SOFR / RFR cap volatilities in reports (ticket 12192)
\end{itemize}

\bigskip
TESTS
\begin{itemize}
\item add a test suite for ORE-SWIG (ticket 11823)
\item add AD unit tests (ticket 12188)
\item add delta/gamma engine tests (ticket 12370)
\item add pairwise variance swap tests (ticket 12370)
\item add equity outperformance option tests (ticket 12370)
\item QuantExt: 298 test functions (vs 272 in the previous release)
\item OREData: 264 test functions (vs. 257 in the previous release)
\item OREAnalytics: 79 test functions (vs. 78 in the previous release)
\end{itemize}

\bigskip
DOCUMENTATION
\begin{itemize}
\item add documentation of pricing engine configuration (ticket 12337)
\item add documentation for added instruments (ticket 12317)
\item add supported compiler and boost versions (ticket 11431) TODO update user guide
\item clean up ORE cmake build flags (ticket 12048) TODO and update user guide
\item update the ORE Design document (ticket 12375)
\item the user guide has grown significantly to about 820 pages, due to the migration of more instruments
  into ORE, additional examples, and especially the added documentation of pricing engine configurations.
\end{itemize}

\bigskip
LANGUAGE BINDINGS \& PYTHON MODULE
\begin{itemize}
\item add NPV Cube and joint cube to the swig interface (ticket 12421)
\item refactor OREApp to allow repeated run() calls (ticket 12317)
\item add Python wheels workflow on github (ticket 12436)
\end{itemize}

ORE SERVICE
\begin{itemize}
\item add a restful API to ORE, based on ORE-Python, see Example 0 (Examples/API)
\end{itemize}

\bigskip
GPU
\begin{itemize}
\item The external device interface based on the scripted trade and in particular its computation
  graph implementation is progressing. This release has several notable changes
  \begin{itemize}
    \item an option to use double precision in opencl (ticket 12386)
    \item a mersenne twister random number generator implementation for opencl (ticket 12387)
    \item a working example that parallelizes bump \& reval sensitivity analysis on a GPU, see Example 61
    \item smoke test are passing on several external devices 
  \end{itemize}
\item Next steps
  \begin{itemize}
    \item implementation of the conditional expectation operator on the GPU
    \item implementation of several coupon types to cover IR/FX products more broadly
    \item implementation of a computation graph version of the post processor
  \end{itemize}
\end{itemize}

\bigskip
NEW \& EXTENDED EXAMPLES
\begin{itemize}
\item  0: ORE Web Service
\item  4: American Swaption added
\item  5: Callable Swap instrument added  
\item 44: IM Schedule added
\item 54: Scripted Trade exposure with AMC: Bermudan Swaption and LPI Swap
\item 55: Scripted Trade exposure with AMC: TaRF
\item 56: Proof of Concept: CVA Sensitivities using AAD
\item 57: Base Scenario Analytic
\item 58: Historical Simulation VaR
\item 59: SABR model for Swaptions and Caps/Floors
\item 60: Exposure and XVA with Overlapping Close-Out Grid
\item 61: Fast Sensitivities using AAD and GPUs
\item 62: P\&L and P\&L Explain Analytic
\item 63: Stresstest with shifts in the par-rate domain
\item 64: Formual-based Coupon
\item 65: Flexi Swap
\item 66: Balance Guaranteed Swap 
\item 67: XVA Stress Testing
\item 68: XVA Bump \& Revalue Sensitivities
\end{itemize}

%========================================================
\section{ORE Data Flow}\label{sec:process}
%========================================================

The core processing steps followed in ORE to produce risk analytics results are sketched in Figure \ref{fig_process}.
All ORE calculations and outputs are generated in three fundamental process steps as indicated in the three boxes in the
upper part of the figure. In each of these steps appropriate data (described below) is loaded and results are generated,
either in the form of a human readable report, or in an intermediate step as pure data files (e.g. NPV data, exposure data).
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.6]{process.pdf}
\end{center}
\caption{Sketch of the ORE process, inputs and outputs. }
\label{fig_process}
\end{figure}

The overall ORE process needs to be parametrised using a set of configuration XML files which is the subject of section
\ref{sec:configuration}. The portfolio is provided in XML format which is explained in detail in sections
\ref{sec:portfolio_data} and \ref{sec:nettingsetinput}. Note that ORE comes with 'Schema' files for all supported
products so that any portfolio xml file can be validated before running through ORE. Market data is provided in a simple
three-column text file with unique human-readable labelling of market data points, as explained in section
\ref{sec:market_data}.  \\

The first processing step (upper left box) then comprises 
\begin{itemize}
\item loading the portfolio to be analysed, 
\item building any yield curves or other 'term structures' needed for pricing, 
\item calibration of pricing and simulation models.
\end{itemize}

The second processing step (upper middle box) is then 
\begin{itemize}
\item portfolio valuation, cash flow generation,
\item going forward - conventional risk analysis such as sensitivity analysis and stress testing, standard-rule capital
  calculations such as SA-CCR, etc,
\item and in particular, more time-consuming, the market simulation and portfolio valuation through time under Monte
  Carlo scenarios.
\end{itemize}
This process step produces several reports (NPV, cashflows etc) and in particular an {\bf NPV cube}, i.e. NPVs per
trade, scenario and future evaluation date. The cube is written to a file in both condensed binary and human-readable
text format.  \\

The third processing step (upper right box) performs more 'sophisticated' risk ana\-ly\-sis by post-processing the NPV
cube data:
\begin{itemize}
\item aggregating over trades per netting set, 
\item applying collateral rules to compute simulated variation margin as well as simulated (dynamic) initial margin
  posting,
\item computing various XVAs including CVA, DVA, FVA, MVA for all netting sets, with and without taking collateral
  (variation and initial margin) into account, on demand with allocation to the trade level.
\end{itemize}
The outputs of this process step are XVA reports and the 'net' NPV cube, i.e. after aggregation, netting and collateral. \\

The example section \ref{sec:examples} demonstrates for representative product types how the described processing steps
can be combined in a simple batch process which produces the mentioned reports, output files and exposure evolution
graphs in one 'go'.

Moreover, both NPV cubes can be further analysed interactively using a visualisation tool introduced in section
\ref{sec:jupyter}. And finally, sections \ref{sec:calc} and \ref{sec:excel} demonstrate how ORE processes can be
launched in spreadsheets and key results presented automatically within the same sheet.

%========================================================
\section{Getting and Building ORE}\label{sec:installation}
%========================================================

You can get ORE in two ways, either by downloading a release bundle as described in section \ref{sec:release} (easiest if you just want to use ORE) or by
checking out the source code from the github repository as described in section \ref{sec:build_ore} (easiest if you want to build and develop ORE).

\subsection{ORE Releases}\label{sec:release}

ORE releases are regularly provided in the form of source code archives, Windows exe\-cutables {\tt ore.exe}, example
cases and documentation. Release archives will be provided at \url{https://github.com/opensourcerisk/engine/releases}.

\subsubsection{Linux container images}

We build and release a container image, which includes the necessary libraries and binaries. \\

\medskip
Usage example: \\

\medskip
{\tt docker run ghcr.io/OpenSourceRisk/ore-engine:centos-stream9-latest [<ore\_argument> ...] } \\

\medskip
Other tags and versions are available, see \url{https://github.com/OpenSourceRisk/Engine/pkgs/container/ore-engine/versions?filters\%5Bversion_type\%5D=tagged} \\

\medskip
If you need to bring in files from your container host, then use a container volume to mount a host directory into the running container: \\

\medskip
{\tt docker run -v <host\_dir>:<container\_dir> ghcr.io/OpenSourceRisk/ore-engine:centos-stream9-latest [<ore\_argument> ...] } \\

\medskip
For example, this command can be used to run a single example from the {\tt Examples} directory: \\

\medskip
{\tt docker run -v <workspace>/Examples:/ore/Examples -i -t -w /ore/Examples/Example\_1 ghcr.io/OpenSourceRisk/ore-engine:centos-stream9-latest Input/ore.xml } \\

\subsubsection{Windows binaries}

The release contains the QuantLib source version that ORE depends on. This is the latest QuantLib release that precedes the ORE release including a small number of patches.

\medskip
The release consists of a single archive in zip format
\begin{itemize}
\item {\tt ORE-<VERSION>.zip}
\end{itemize}

When unpacked, it creates a directory {\tt ORE-<VERSION>} with the following files respectively subdirectories
\begin{enumerate}
%\item {\tt bin/win32/ore.exe}
%\item {\tt bin/x64/ore.exe}
\item {\tt App/}
\item {\tt Docs/}
\item {\tt Examples/}
\item {\tt FrontEnd/}
\item {\tt OREAnalytics/}
\item {\tt OREData/}
\item {\tt ORETest/}
\item {\tt QuantExt/}
\item {\tt QuantLib/}
\item {\tt ThirdPartyLibs/}
\item {\tt tools/}
\item {\tt xsd/}
\item {\tt userguide.pdf}
\end{enumerate} 

The first three items and {\tt userguide.pdf} are sufficient to run the compiled ORE application
on the list of examples described in the user guide (this works on Windows only). The Windows executables are located in {\tt App/bin/Win32/Release/} respectively {\tt App/bin/x64/Release/}. To continue with the compiled
executables:
\begin{itemize}
\item Ensure that the scripting language Python is installed on your computer, see also section \ref{sec:python}
  below;
\item Move on to the examples in section \ref{sec:examples}.
\end{itemize}

\medskip
The release bundle contains the ORE source code, which is sufficient to build ORE from sources as follows (if you build ORE for development purposes, we recommend using git though, see section \ref{sec:build_ore}):
\begin{itemize}
\item Set up Boost as described in section \ref{sec:boost}, unless already installed
\item Build QuantLib, QuantExt, OREData, OREAnalytics, App (in this order) as described in section \ref{sec:build}
\item Note that ThirdPartyLibs does not need to be built, it contains RapidXml, header only code for reading and
  writing XML files
\item Move on to section \ref{sec:python} and the examples in section \ref{sec:examples}.
\end{itemize}

Open {\tt Docs/html/index.html} to see the API documentation for QuantExt, OREData and OREAnalytics, generated by
doxygen.

\subsection{Building ORE}\label{sec:build_ore}

ORE's source code is hosted at \url{https://github.com/opensourcerisk/engine}.

\subsubsection{Git}

To access the current code base on GitHub, one needs to get {\tt git} installed first.
   
\begin{enumerate}
\item Install and setup Git on your machine following instructions at \cite{git-download}

\item Fetch ORE from github by running the following: 

{\tt\% git clone https://github.com/opensourcerisk/engine.git ore}      

This will create a folder 'ore' in your current directory that contains the codebase.

\item Initially, the QuantLib subdirectory under {\tt ore} is empty as it is a submodule pointing to the official
  QuantLib repository. To pull down locally, use the following commands:

{\tt
\% cd ore \\
\% git submodule init \\
\% git submodule update
}

\end{enumerate}

Note that one can also run 

{\footnotesize \tt\% git clone --recurse-submodules https://github.com/opensourcerisk/engine.git ore}

in step 2, which also performs the steps in 3.

\subsubsection{Boost}\label{sec:boost}

QuantLib and ORE depend on the boost C++ libraries. Hence these need to be installed before building QuantLib and
ORE. On all platforms the minimum required boost version is 1\_78.
%Other versions may work on some platforms and system configurations, but were not tested.

\subsubsection*{Windows}

\begin{enumerate}
\item Download the pre-compiled binaries for your MSVC version (e.g. MSVC-14.3 for MSVC2022) from \cite{boost-binaries}
%, any recent version should work
\begin{itemize}
\item 32-bit: \cite{boost-binaries}{\bs}VERSION{\bs}boost\_VERSION-msvc-14.3-32.exe{\bs}download 
\item 64-bit: \cite{boost-binaries}{\bs}VERSION{\bs}boost\_VERSION-msvc-14.3-64.exe{\bs}download
\end{itemize}
\item Start the installation file and choose an installation folder (the ``boost root directory''). Take a note of that folder as it will be needed later on.   
\item Finish the installation by clicking Next a couple of times.
\end{enumerate}
    
Alternatively, compile all Boost libraries directly from the source code:

\begin{enumerate}
\item Open a Visual Studio Tools Command Prompt
\begin{itemize}
\item 32-bit: VS2022 x86 Native Tools Command Prompt
\item 64-bit: VS2022 x64 Native Tools Command Prompt
\end{itemize}
\item Navigate to the boost root directory
\item Run bootstrap.bat
\item Build the libraries from the source code
\begin{itemize}
\item 32-bit: \\
  {\footnotesize\tt .{\bs}b2 --stagedir=.{\bs}lib{\bs}Win32{\bs}lib --build-type=complete toolset=msvc-14.3 \bs \\
    address-model=32 --with-test --with-system --with-filesystem  \bs \\
    --with-serialization --with-regex --with-date\_time stage}
\item 64-bit: \\
  {\footnotesize\tt .{\bs}b2 --stagedir=.{\bs}lib{\bs}x64{\bs}lib --build-type=complete toolset=msvc-14.3 \bs \\
    address-model=64 --with-test --with-system --with-filesystem \bs \\
    --with-serialization --with-regex --with-date\_time stage}
\end{itemize}
\end{enumerate}

\subsubsection*{Unix}

\begin{enumerate}
\item Download Boost from \cite{boost} and build following the instructions on the site
\item Define the environment variable BOOST that points to the boost directory
(so includes should be in BOOST and libs should be in BOOST/stage/lib)
\end{enumerate}

\subsubsection{ORE Libraries and Application}\label{sec:build}

\subsubsection*{Windows}

\begin{enumerate}

\item Download and install Visual Studio Community Edition (Version 2019 or later, 2022 is recommended). 
During the installation, make sure you install the Visual
C++ support under the Programming Languages features (disabled by default).

\item Configure boost paths: \\

Set environment variables, e.g.:
\begin{itemize}
  	\item  {\tt \%BOOST\%} pointing to your directory, e.g, {\tt C:{\bs}boost\_1\_72\_0} 
  	\item {\tt \%BOOST\_LIB32\%} pointing to your Win32 lib directory, e.g, {\tt C:{\bs}boost\_1\_72\_0{\bs}lib32\-msvc\-14.3} 
	\item  {\tt \%BOOST\_LIB64\%} pointing to your x64 lib directory, e.g, {\tt C:{\bs}boost\_1\_72\_0{\bs}lib64\-msvc\-14.3} 
 \end{itemize}
 
\item Download and install CMake for Windows (https://cmake.org/download/). Visual Studio Community Edition 2019 or later supports CMake and you can install the feature 'C++ CMake Tools for Windows' instead of installing CMake as standalone program.

\end{enumerate}

%\subsubsection*{Visual Studio with CMake}

Visual Studio 2019 and later supports CMake Projects.

\begin{enumerate}
\item Start Visual Studio 2019 or later.
\item Select "Open a local folder" from the start page or menu.
\item In the dialog window, select the ORE root directory.
\item Visual Studio will read the cmake presets from CMakePresets.json and the project file CMakeList.txt and configure the project.
\item Once the configuration is finished and one can build the project.
\item The executables are built in the subfolder {\tt /build/TARGET/CONFIGURATION/EXECUTABLE}, e.g. {\tt /build/App/Release/ore.exe}.
\end{enumerate}

ORE is shipped with configuration and build presets using Visual Studio 2022 and the Ninja build system. Those presets are configured in the CMakePreset.json which is read by Visual Studio by default when opening the CMake project. If you want to use Visual Studio 2019 instead, you would have to change the Generator in the CMakePreset.json from "Visual Studio 17 2022" to "Visual Studio 16 2019".

You can switch in the solution explorer from the file view to the projects view, where the CMake Targets View can be selected. In this view, the various target projects can be seen below "ORE Project" and be used in a similar manner as the usual VS projects.

%\subsubsection*{Generate Visual Studio Projects with CMake}
\medskip

Alternatively, Visual Studio project files can be auto-generated from the CMake project files or ORE can be built with the CMake command line tool, similar to UNIX / Mac systems.

\begin{enumerate}

\item Generate MSVC project files from CMake files:
\begin{itemize}
\item Open a Visual Studio Tools Command Prompt
\begin{itemize}
\item 32-bit: VS2022/x86 Native Tools Command Prompt for VS 2022
\item 64-bit: VS2022/x64 Native Tools Command Prompt for VS 2022
\end{itemize}
\item Navigate to the ORE root directory
\item Run CMake command:
\begin{itemize}
\item 64-bit: \\
{\tt cmake -G "Visual Studio 17 2022" -A x64 -DBOOST\_INCLUDEDIR=\%BOOST\% -DBOOST\_LIBRARYDIR=\%BOOST\_LIB64\% -DQL\_ENABLE\_SESSIONS=ON -DMSVC\_LINK\_DYNAMIC\_RUNTIME=true -B build}
\item 32-bit: \\
{\tt cmake -G "Visual Studio 17 2022" -A x32 -DBOOST\_INCLUDEDIR=\%BOOST\% -DBOOST\_LIBRARYDIR=\%BOOST\_LIB32\% -DQL\_ENABLE\_SESSIONS=ON -DMSVC\_LINK\_DYNAMIC\_RUNTIME=true -B build}
\end{itemize}
Replace the generator "Visual Studio 17 2022" with the actual installed version.
The solution and project files will be generated in the {\tt $\langle$ORE\_ROOT$\rangle${\bs}build} subdirectory.
\end{itemize}

\item build the cmake project with the command {\tt cmake --build build -v --config Release}, 

\item or open the MSVC solution file {\tt build{\bs}ORE.sln} and build the entire solution with Visual Studio (again, make sure to select the correct platform in the configuration manager first).
\end{enumerate}

\subsubsection*{Optional: Install optional dependencies with VCPKG}

VCPKG is an open source c++ library manager. ORE can be built optionally with ZLIB and Eigen library support. 

For both features the libraries needed to be installed on the system. On Windows one can use the VCPKG package manager to install those dependencies:

\begin{itemize}
\item Install vcpkg: https://vcpkg.io/en/getting-started.html
\item Install dependencies with invoking the command \\
\medskip
{\tt vcpkg install --triplet x64-windows zlib} \\
{\tt vcpkg install --triplet x64-windows eigen3} \\
\medskip
\end{itemize}

To make VCPKG visible to CMAKE, create an environment variable {\tt VCPKG\_ROOT} pointing to the root of the vcpkg directory and configure ORE with the flag {\tt -DCMAKE\_TOOLCHAIN\_FILE=\%VCPKG\_ROOT\%/scripts/buildsystems/vcpkg.cmake}. 

To use VCPKG with Visual Studio add the toolChainFile to the configurePresets in the CMakePresets.json:

{\tt "toolchainFile": "\$env\{VCPKG\_ROOT\}/scripts/buildsystems/vcpkg.cmake",}

\subsubsection*{Unix}

With the 5th release we have discontinued automake support so that ORE can only be built with CMake on Unix systems, as follows.

\begin{enumerate}
\item set environment variable to locate the boost include and boost library directories\\
\medskip
  {\tt export BOOST\_LIB=path/to/boost/lib}\\
  {\tt export BOOST\_INC=path/to/boost/include}
\medskip
\item Change to the ORE project directory that contains the {\tt QuantLib}, {\tt QuantExt}, etc, folders; create subdirectory {\tt build} and change to subdirectory {\tt build}
\item Configure CMake by invoking \\
\medskip
{\tt cmake -DBOOST\_ROOT=\${BOOST\_INC} -DBOOST\_LIBRARYDIR=\${BOOST\_LIB} -DQL\_ENABLE\_SESSIONS=ON ..} \\
\medskip
where the {\tt QL\_ENABLE\_SESSIONS} variable is set to ON in order to enable some multi-threading applications in ORE.

Alternatively, set environment variables {\tt BOOST\_ROOT} and {\tt BOOST\_LIBRARYDIR} directly and run \\
\medskip
{\tt cmake ..} \\
\medskip
\item Build all ORE libraries, QuantLib, as well as the doxygen API documentation for QuantExt, OREData and OREAnalytics, by invoking \\
\medskip
{\tt make -j4} \\
\medskip
using four threads in this example.
\medskip
\item Run all test suites by invoking \\
\medskip
{\tt ctest -j4}
\item Run Examples (see section \ref{sec:examples})
\end{enumerate}

Note: 
\begin{itemize}
\item If the boost libraries are not installed in a standard path they might not be found during runtime because of a missing rpath
tag in their path. Run the script {\tt rename\_libs.sh} to set the rpath tag in all libraries located in {\tt
  \${BOOST}/stage/lib}.
\item Unset {\tt LD\_LIBRARY\_PATH} respectively {\tt DYLD\_LIBRARY\_PATH} before running the ORE executable or the test suites, in order not to override the rpath information embedded into the libaries built with CMake
\item On Linux systems, the 'locale' settings can negatively affect the ORE process and output. To avoid this, we
recommend setting the environment variable {\tt LC\_NUMERIC} to {\tt C}, e.g. in a bash shell, do

{\tt\footnotesize
\% export LC\_NUMERIC=C
}

before running ORE or any of the examples below. This will suppress thousand separators in numbers when converted to
strings.

\item Generate {\tt CMakeLists.txt}:

The .cpp and .hpp files included in the build process need to be explicitly specified in the various {\tt CMakeLists.txt} 
files in the project directory. The python script (in {\tt Tools/update\_cmake\_files.py}) can be used to update all CMakeLists.txt files 
automatically. 

\end{itemize}

\subsubsection{Supported Compiler and Boost Versions}
\label{sec:compiler_boost_versions}

The following table \ref{tab:compiler_boost_versions} reflects the compiler / boost version combinations that
the users/developers at Acadia/LSEG can confirm as working combinations with the latest ORE v12.

\begin{table}[hbt]
  \begin{tabular}{|l|c|c|}
    \hline
    Compiler & Boost & ORE \\
    \hline
    AppleClang version 13.0.0 & 1.82.0 & 12\\
    AppleClang version 13.1.6 & 1.84.0 & 12\\
    AppleClang version 14.0.0 & 1.82.0 & 12\\
    AppleClang version 15.0.0 & 1.83.0 & 12\\
    gcc 10.2.1 & 1.74.0 & 12 \\
    gcc 11.4.0 & 1.74.0 & 12 \\
    VS2019 & 1.72.0 & 12 \\
    VS2022 & 1.78.0 & 12 \\
    VS2022 & 1.83.0 & 12 \\
    \hline
  \end{tabular}
  \caption{Supported compiler and boost versions for ORE v12.}
  \label{tab:compiler_boost_versions}
\end{table}

\subsubsection*{ZLIB support}

To enable zlib support configure CMake with the flag {\tt -DORE\_USE\_ZLIB=ON}. 

If zlib is not installed on the system, it can be installed on Windows with the package manager VCPKG.

\subsection{Python and Jupyter}\label{sec:python}

Python (version 3.5 or higher) is required to use the ORE Python language bindings in section \ref{sec:oreswig}, 
or to run the examples in section \ref{sec:examples} and plot exposure
evolutions. Moreover, we use Jupyter \cite{jupyter} in section \ref{sec:visualisation} to visualise simulation
results. Both are part of the 'Anaconda Open Data Science Analytics Platform' \cite{Anaconda}. Anaconda installation
instructions for Windows, OS X and Linux are available on the Anaconda site, with graphical installers for
Windows\footnote{With Windows, after a fresh installation of Python the user may have to run the {\tt python} command
  once in a command shell so that the Python executable will be found subsequently when running the example scripts in
  section \ref{sec:examples}.}, Linux and OS X.

With Linux and OS X, the following environment variable settings are required
\begin{itemize}
\item set {\tt LANG} and {\tt LC\_ALL } to {\tt en\_US.UTF-8} or {\tt en\_GB.UTF-8}
\item set {\tt LC\_NUMERIC} to {\tt C}. 
\end{itemize}
The former is required for both running the Python scripts in the examples section, as well as successful installation
of the following packages. \\

The full functionality of the Jupyter notebook introduced in section \ref{sec:jupyter} requires furthermore installing
\begin{itemize}
\item jupyter\_dashboards: \url{https://github.com/jupyter-incubator/dashboards}
\item ipywidgets: \url{https://github.com/ipython/ipywidgets}
\item pythreejs: \url{https://github.com/jovyan/pythreejs}
\item bqplot: \url{https://github.com/bloomberg/bqplot}
\end{itemize}
With Python and Anaconda already installed, this can be done by running these commands
\begin{itemize}
\item {\tt conda install -c conda-forge ipywidgets}
\item {\tt pip install jupyter\_dashboards}
\item {\tt jupyter dashboards quick-setup --sys-prefix}
\item {\tt conda install -c conda-forge bqplot}
\item {\tt conda install -c conda-forge pythreejs}
\end{itemize}
Note that the bqplot installation requires the environment settings mentioned above.

\subsection{Building ORE-SWIG and Python Wheels}\label{sec:oreswig}

Since release 4, ORE comes with Python and Java language bindings following the QuantLib-SWIG example.
The ORE bindings extend the QuantLib SWIG wrappers and allow calling ORE functionality in the 
QuantExt/OREData/OREAnalytics libraries alongside with functionality in QuantLib.  

\medskip
The ORE-SWIG source code is hosted in a separate git repository at \url{https://github.com/opensourcerisk/ore-swig}.
The {\tt README.md} in the top level directory of this git repository contains build instructions and refers to tutorials 
for installing and building Python wrappers and wheels.

\medskip
Typical usage of the Python wrapper is shown in ORE's {\tt Example\_42} and in ORE SWIG's {\tt OREAnalytics/Python/Examples} directory.

%========================================================
\section{Examples}\label{sec:examples}
%========================================================

The examples shown in table \ref{tab_0} are intended to help with getting started with ORE, and to serve as plausibility
checks for the simulation results generated with ORE.

\begin{table}[hbt]
\scriptsize
\begin{center}
\begin{tabular}{|c|l|}
\hline
Example & Description \\
\hline
\hline
0 & ORE Web Service \\
\hline
1 & Vanilla at-the-money Swap with flat yield curve \\
\hline
2 & Vanilla Swap with normal yield curve \\
\hline
3 & European Swaption \\
\hline
4 & Bermudan Swaption \\
\hline
5 & Callable Swap \\
\hline
6 & Cap/Floor \\
\hline
7 & FX Forward \\
  & European FX Option \\ 
\hline
8 & Cross Currency Swap without notional reset \\
\hline
9 & Cross Currency Swap with notional reset \\
\hline
10 & Three-Swap portfolio with netting and collateral \\
   & XVAs - CVA, DVA, FVA, MVA, COLVA \\
   & Exposure and XVA Allocation to trade level \\
\hline
11 & Basel exposure measures - EE, EPE, EEPE \\
\hline
12 & Long term simulation with horizon shift \\
\hline
13 & Dynamic Initial Margin and MVA \\
\hline
14 & Minimal Market Data Setup \\
\hline
15 & Sensitivity Analysis and Stress Testing \\
\hline
16 & Equity Derivatives Exposure \\
\hline
17 & Inflation Swap Exposure under Dodgson-Kainth\\
\hline
18 & Bonds and Amortisation Structures\\
\hline
19 & Swaption Pricing with Smile\\
\hline
20 & Credit Default Swap Pricing\\
\hline
21 & Constant Maturity Swap Pricing\\
\hline
22 & Option Sensitivity Analysis with Smile\\
\hline
23 & Forward Rate Agreement and Averaging OIS Exposure\\
\hline
24 & Commodity Forward and Option Pricing and Sensitivity\\
\hline
25 & CMS Spread with (Digital) Cap/Floor Pricing, Sensitivity and Exposures\\
\hline
26 & Bootstrap Consistency\\
\hline
27 & BMA Basis Swap Pricing and Sensitivity\\
\hline
28 & Discount Ratio Curves\\
\hline
29 & Curve Building using Fixed vs. Float Cross Currency Helpers\\
\hline
30 & USD-Prime Curve Building via Prime-LIBOR Basis Swap\\
\hline
31 & Exposure Simulation using a Close-out Grid\\
\hline
32 & Inflation Swap Exposure under Jarrow-Yildrim\\ 
\hline
33 & CDS Exposure Simulation \\
\hline
34 & Wrong Way Risk \\
\hline
35 & Flip View \\
\hline
36 & Choice of Measure \\
\hline
37 & Multifactor Hull-White scenario generation \\
\hline
38 & Cross Currency Exposure using Multifactor Hull-White Models \\
\hline
39 & Exposure Simulation using American Monte Carlo \\
\hline
40 & Par Sensitivity Analysis \\
\hline
41 & Multi-threaded Exposure Simulation \\
\hline
42 & ORE Python Module \\
\hline
43 & Credit Portfolio Model \\
\hline
44 & Initial Margin: ISDA SIMM and IM Schedule \\
\hline
45 & Collateralized Bond Obligation \\
\hline
46 & Generic Total Return Swap \\
\hline
47 & Composite Trade \\
\hline
48 & Convertible Bond and ASCOT \\
\hline
49 & Bond Yield Shifted \\
\hline
50 & Par Sensitivity Conversion of external "Raw" Sensis \\
\hline
51 & Custom Trade Fixings\\
\hline
52 & Scripted Trades \\
\hline
53 & Curve Building using FRAs tailored to Central Bank Meeting Dates\\
\hline
54 & Scripted Trade Exposure with AMC: Bermudan Swaption and LPI Swap \\
\hline
55 & Scripted Trade Exposure with AMC: Fx TaRF \\
\hline
56 & CVA Sensitivity using AAD \\
\hline
57 & Base Scenario Analytic \\
\hline
58 &  Historical Simulation VaR \\
\hline
59 & SABR model for Swaptions and Caps/Floors \\
\hline
60 &  Exposure and XVA with Overlapping Close-Out Grids \\
\hline
61 &  Fast Sensitivties using AAD and GPUs \\
\hline
62 &  P\&L and P\&L Explain Analytics \\
\hline
63 &  Stress Tests in the Par-Rate Domain \\
\hline
64 &  Formula-based Coupon \\
\hline
65 &  Flexi Swap \\
\hline
66 &  Balance Guaranteed Swap \\
\hline
67 &  XVA Stress Testing \\
\hline
68 &  XVA Bump \& Revalue Sensitivities \\
\hline
69 &  Convert Zero Rate Shifts into Par Rate Shifts \\
\hline
\end{tabular}
\caption{ORE examples.}
\label{tab_0}
\end{center}
\end{table}
\clearpage

All example results can be produced with the Python scripts {\tt run.py} in the ORE release's {\tt Examples/Example\_\#}
folders which work on both Windows and Unix platforms. In a nutshell, all scripts call ORE's command line application
with a single input XML file

\medskip
\centerline{\tt ore[.exe] ore.xml}
\medskip

They produce a number of standard reports and exposure graphs in PDF format. The structure of the input file and of the
portfolio, market and other configuration files referred to therein will be explained in section
\ref{sec:configuration}.

\medskip ORE is driven by a number of input files, listed in table \ref{tab_1} and explained in detail in sections
\ref{sec:configuration} to \ref{sec:fixings}. In all examples, these input files are either located in the example's sub
directory {\tt Examples/Example\_\#/Input} or the main input directory {\tt Examples/Input} if used across several
examples. The particular selection of input files is determined by the 'master' input file {\tt ore.xml}.

\begin{table}[h]
\scriptsize
\begin{center}
\begin{tabular}{|l|p{11cm}|}
  \hline
  File Name & Description \\
  \hline
  {\tt ore.xml}&   Master input file, selection of further inputs below and selection of analytics \\
  {\tt portfolio.xml} & Trade data \\
  {\tt netting.xml} &  Collateral (CSA) data \\
  {\tt simulation.xml} & Configuration of simulation model and market\\
  {\tt market.txt} &  Market data snapshot \\
  {\tt fixings.txt} &  Index fixing history \\
  {\tt dividends.txt} &  Dividends history \\
  {\tt curveconfig.xml} & Curve and term structure composition from individual market instruments\\
  {\tt conventions.xml} & Market conventions for all market data points\\
  {\tt todaysmarket.xml} &  Configuration of the market composition, relevant for the pricing of the given portfolio as
                           of today (yield curves, FX rates, volatility surfaces etc) \\
  {\tt pricingengines.xml} &  Configuration of pricing methods by product\\
  \hline
\end{tabular}
\end{center}
\caption{ORE input files}
\label{tab_1}
\end{table}

The typical list of output files and reports is shown in table \ref{tab_2}. The names of output files can be configured
through the master input file {\tt ore.xml}. Whether these reports are generated also depends on the setting in {\tt
  ore.xml}. For the examples, all output will be written to the directory {\tt Examples/Example\_\#/Output}.

\begin{table}[h]
\scriptsize
\begin{center}
\begin{tabular}{|l|p{11cm}|}
\hline
File Name & Description \\
\hline
{\tt npv.csv}&   NPV report \\
{\tt flows.csv} & Cashflow report \\
{\tt curves.csv} & Generated yield (discount) curves report \\
{\tt xva.csv} & XVA report, value adjustments at netting set and trade level \\
{\tt exposure\_trade\_*.csv} & Trade exposure evolution reports\\
{\tt exposure\_nettingset\_*.csv} &  Netting set exposure evolution reports\\
{\tt rawcube.csv} & NPV cube in readable text format \\
{\tt netcube.csv} & NPV cube after netting and colateral, in readable text format \\
{\tt *.csv.gz} & Intermediate storage of NPV cube and scenario data \\
{\tt *.pdf} &  Exposure graphics produced by the python script {\tt run.py} after ORE completed\\
\hline
\end{tabular}
\end{center}
\caption{ORE output files}
\label{tab_2}
\end{table}

Note: When building ORE from sources on Windows platforms, make sure that you copy your {\tt ore.exe} to the binary
directory {\tt App/bin/win32/} respectively {\tt App/bin/x64/}. Otherwise the examples may be run using the pre-compiled
executables which come with the ORE release.

% start with section 0
\setcounter{subsection}{-1}
\subsection{ORE Web Service}\label{sec:service}

Since release 12, ORE comes with a proof-of-concept implementation of a web service around ORE
that is written in Python, see folder {\tt Examples/API}.

The service is based on
\begin{itemize}
\item the flask web framework \url{https://flask.palletsprojects.com}, and
\item the ORE Python module which can be installed using \\
  {\tt pip install open-source-risk-engine} \\
  or built from sources following the instructions in the ORE-SWIG repository
  at \url{https://github.com/opensourcerisk/ore-swig}.
\end{itemize}

\medskip
Main files in the {\tt Examples/API} directory:
\begin{itemize}
\item {\bf restapi.py} runs a flask api as analytics service that takes a post request
  (e.g. from a server like postman, or from a python script): The request contains a json body
  which corresponds to the data usually contained in the master input file ore.xml. restapi.py
  calls the oreApi.py class (see below) to do the work. By default, the analytics service listens for
  requests on port 5001.
\item {\bf oreApi.py} reads the json body, compiles all ORE input parameters, calls into a
  data service (see below) to retrieve additional data. Then it kicks off an ORE run to process
  the request.  Finally it posts resulting reports through the data service.
\item {\bf simplefileserver.py} runs a flask api as a data service. It takes requests from the
  analytics service above in the form of urls of xml files that contain additional data required
  by ORE (market data, portfolio, configuration). By default, the data service
  listens on port 5000, reads from the Input directory in Examples/API and writes reports to the Output
  directory in Examples/API.
\end{itemize}

\medskip
Run a local example:
\begin{itemize}
\item start the data service: \\
  {\tt python3 simplefileserver.py \&} 
\item start the analytics service: \\
  {\tt python3 restapi.py \&}
\item send a request to run the equivalent of Example 1: \\
  {\tt python3 request.py} \\
  Note that the json equivalent of {\tt Example\_1/Input/ore.xml} is contained in request.py,
  and all other inputs are retrieved from folder Examples/API/Input via the data service.
\end{itemize}

%See {\tt Examples/API/ReadMe.txt} for further ways to utilise the service, e.g. to run all
%examples in this section, deploy via docker etc.

%--------------------------------------------------------
\subsection{Interest Rate Swap Exposure, Flat Market}\label{sec:example1}
%--------------------------------------------------------

We start with a vanilla single currency Swap (currency EUR, maturity 20y, notional 10m, receive fixed 2\% annual, pay
6M-Euribor flat). The market yield curves (for both discounting and forward projection) are set to be flat at 2\% for
all maturities, i.e. the Swap is at the money initially and remains at the money on average throughout its life. Running
ORE in directory {\tt Examples/Example\_1} with

\medskip
\centerline{\tt python run.py } 
\medskip

yields the exposure evolution in 

\medskip
\centerline{\tt Examples/Example\_1/Output/*.pdf } 
\medskip

and shown in figure \ref{fig_1}. 
\begin{figure}[h!]
\begin{center}
%\includegraphics[scale=0.45]{mpl_swap_1_1m_sbb_100k.pdf}
\includegraphics[scale=0.45]{mpl_swap_1_1m_sbb_10k_flat.pdf}
\end{center}
\caption{Vanilla ATM Swap expected exposure in a flat market environment from both parties' perspectives. The symbols are European Swaption prices. The simulation was run with monthly time steps and 10,000 Monte Carlo samples to demonstrate the convergence of EPE and ENE profiles. A similar
outcome can be obtained more quickly with 5,000 samples on a quarterly time grid which is the default setting of Example\_1. }
\label{fig_1}
\end{figure}
Both Swap simulation and Swaption pricing are run with calls to the ORE executable, essentially 

\medskip
\centerline{\tt ore[.exe] ore.xml} 

\centerline{\tt ore[.exe] ore\_swaption.xml} 
\medskip

which are wrapped into the script {\tt Examples/Example\_1/run.py} provided with the ORE release.
It is instructive to look into the input folder in Examples/Example\_1, the content of the main input file {\tt
  ore.xml}, together with the explanations in section \ref{sec:configuration}. \\

This simple example is an important test case which is also run similarly in one of the unit test suites of ORE. The
expected exposure can be seen as a European option on the underlying netting set, see also appendix
\ref{sec:app_exposure}. In this example, the expected exposure at some future point in time, say 10 years, is equal to
the European Swaption price for an option with expiry in 10 years, underlying Swap start in 10 years and underlying Swap
maturity in 20 years. We can easily compute such standard European Swaption prices for all future points in time where
both Swap legs reset, i.e. annually in this case\footnote{Using closed form expressions for standard European Swaption
  prices.}. And if the simulation model has been calibrated to the points on the Swaption surface which are used for
European Swaption pricing, then we can expect to see that the simulated exposure matches Swaption prices at these annual
points, as in figure \ref{fig_1}.  In Example\_1 we used co-terminal ATM Swaptions for both model calibration and
Swaption pricing. Moreover, as the yield curve is flat in this example, the exposures from both parties'
perspectives (EPE and ENE) match not only at the annual resets, but also for the period between annual reset of both
legs to the point in time when the floating leg resets. Thereafter, between floating leg (only) reset and next joint
fixed/floating leg reset, we see and expect a deviation of the two exposure profiles.

%--------------------------------------------------------
\subsection{Interest Rate Swap Exposure, Realistic Market}\label{sec:example2}
%--------------------------------------------------------

Moving to {\tt Examples/Example\_2}, we see what changes when using a realistic (non-flat) market
environment. Running the example with

\medskip
\centerline{\tt python run.py } 
\medskip

yields the exposure evolution in 

\medskip
\centerline{\tt Examples/Example\_2/Output/*.pdf } 
\medskip

shown in figure \ref{fig_2}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_swap_3.pdf}
\end{center}
\caption{Vanilla ATM Swap expected exposure in a realistic market environment as of 05/02/2016 from both parties'
  perspectives. The Swap is the same as in figure \ref{fig_1} but receiving fixed 1\%, roughly at the money. The symbols
  are the prices of European payer and receiver Swaptions. Simulation with 5000 paths and monthly time steps.}
\label{fig_2}
\end{figure}
In this case, where the curves (discount and forward) are upward sloping, the receiver Swap is at the money at inception
only and moves (on average) out of the money during its life. Similarly, the Swap moves into the money from the
counterparty's perspective. Hence the expected exposure evolutions from our perspective (EPE) and the counterparty's
perspective (ENE) 'detach' here, while both can still be be reconciled with payer or respectively receiver Swaption
prices.

%--------------------------------------------------------
\subsection{European Swaption Exposure}\label{sec:european_swaption}
%--------------------------------------------------------

This demo case in folder {\tt Examples/Example\_3} shows the exposure evolution of European Swaptions with cash and
physical delivery, respectively, see figure \ref{fig_3}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_swaption.pdf}
\end{center}
\caption{European Swaption exposure evolution, expiry in 10 years, final maturity in 20 years, for cash and physical
  delivery. Simulation with 1000 paths and quarterly time steps. }
\label{fig_3}
\end{figure}
The delivery type (cash vs physical) yields significantly different valuations as of today due to the steepness of the
relevant yield curves (EUR). The cash settled Swaption's exposure graph is truncated at the exercise date, whereas the
physically settled Swaption exposure turns into a Swap-like exposure after expiry. For comparison, the example also
provides the exposure evolution of the underlying forward starting Swap which yields a somewhat higher exposure after
the forward start date than the physically settled Swaption. This is due to scenarios with negative Swap NPV at expiry
(hence not exercised) and positive NPVs thereafter. Note the reduced EPE in case of a Swaption with settlement of the option premium on exercise date.

%--------------------------------------------------------
\subsection{Bermudan Swaption Exposure}
%--------------------------------------------------------

This demo case in folder {\tt Examples/Example\_4} shows the exposure evolution of Bermudan rather than European
Swaptions with cash and physical delivery, respectively, see figure \ref{fig_3b}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_bermudan_swaption.pdf}
\end{center}
\caption{Bermudan Swaption exposure evolution, 5 annual exercise dates starting in 10 years, final maturity in 20 years,
  for cash and physical delivery. Simulation with 1000 paths and quarterly time steps.}
\label{fig_3b}
\end{figure}
The underlying Swap is the same as in the European Swaption example in section \ref{sec:european_swaption}. Note in
particular the difference between the Bermudan and European Swaption exposures with cash settlement: The Bermudan shows
the typical step-wise decrease due to the series of exercise dates. Also note that we are using the same Bermudan option
pricing engines for both settlement types, in contrast to the European case, so that the Bermudan option cash and
physical exposures are identical up to the first exercise date. When running this example, you will notice the
significant difference in computation time compared to the European case (ballpark 30 minutes here for 2 Swaptions, 1000
samples, 90 time steps). The Bermudan example takes significantly more computation time because we use an LGM grid
engine for pricing under scenarios in this case. In a realistic context one would more likely resort to American Monte
Carlo simulation, feasible in ORE, but not provided in the current release. However, this implementation can be used to
benchmark any faster / more sophisticated approach to Bermudan Swaption exposure simulation.

%--------------------------------------------------------
\subsection{Callable Swap Exposure}
%--------------------------------------------------------

This demo case in folder {\tt Examples/Example\_5} shows the exposure evolution of a European callable Swap, represented
as two trades - the non-callable Swap and a Swaption with physical delivery. We have sold the call option, i.e. the
Swaption is a right for the counterparty to enter into an offsetting Swap which economically terminates all future flows
if exercised. The resulting exposure evolutions for the individual components (Swap, Swaption), as well as the callable
Swap are shown in figure \ref{fig_4}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_callable_swap.pdf}
\end{center}
\caption{European callable Swap represented as a package consisiting of non-callable Swap and Swaption. The Swaption has
  physical delivery and offsets all future Swap cash flows if exercised. The exposure evolution of the package is shown
  here as 'EPE Netting Set' (green line). This is covered by the pink line, the exposure evolution of the same Swap but
  with maturity on the exercise date. The graphs match perfectly here, because the example Swap is deep in the money and
  exercise probability is close to one. Simulation with 5000 paths and quarterly time steps.}
\label{fig_4}
\end{figure}
The example is an extreme case where the underlying Swap is deeply in the money (receiving fixed 5\%), and hence the
call exercise probability is close to one. Modify the Swap and Swaption fixed rates closer to the money ($\approx$ 1\%)
to see the deviation between net exposure of the callable Swap and the exposure of a 'short' Swap with maturity on
exercise.

We have added more recently the combined CallableSwap instrument representation, check {\tt portfolio.xml} and compare
the CallableSwap NPV in {\tt npv.csv} to the package NPV of Swap and Swaption.

%--------------------------------------------------------
\subsection{Cap/Floor Exposure}\label{sec:capfloor}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_6} generates exposure evolutions of several Swaps, caps and floors. The
example shown in figure \ref{fig_capfloor_1} ('portfolio 1') consists of a 20y Swap receiving 3\% fixed and paying
Euribor 6M plus a long 20y Collar
with both cap and floor at 4\% so that the net exposure corresponds to a Swap paying 1\% fixed. \\

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_capfloor_1.pdf}
\end{center}
\caption{Swap+Collar, portfolio 1. The Collar has identical cap and floor rates at 4\% so that it corresponds to a
  fixed leg which reduces the exposure of the Swap, which receives 3\% fixed. Simulation with 1000 paths and quarterly
  time steps.}
\label{fig_capfloor_1}
\end{figure}

The second example in this folder shown in figure \ref{fig_capfloor_2} ('portfolio 2') consists of a short Cap, long
Floor and a long Collar that exactly offsets the netted Cap and Floor.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_capfloor_2.pdf}
\end{center}
\caption{Short Cap and long Floor vs long Collar, portfolio 2. Simulation with 1000 paths and quarterly time steps.}
\label{fig_capfloor_2}
\end{figure}

Further three test portfolios are provided as part of this example. Run the example and inspect the respective output
directories {\tt Examples/Example\_6/Output/portfolio\_\#}. Note that these directories have to be present/created
before running the batch with {\tt python run.py}.

%--------------------------------------------------------
\subsection{FX Forward and FX Option Exposure}\label{sec:fxfwd}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_7} generates the exposure evolution for a EUR / USD FX Forward transaction
with value date in 10Y. This is a particularly simple show case because of the single cash flow in 10Y. On the other
hand it checks the cross currency model implementation by means of comparison to analytic limits - EPE and ENE at the
trade's value date must match corresponding Vanilla FX Option prices, as shown in figure \ref{fig_5}.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.45]{mpl_fxforward.pdf}
\end{center}
\caption{EUR/USD FX Forward expected exposure in a realistic market environment as of 26/02/2016 from both parties'
  perspectives. Value date is obviously in 10Y. The flat lines are FX Option prices which coincide with EPE and ENE,
  respectively, on the value date. Simulation with 5000 paths and quarterly time steps.}
\label{fig_5}
\end{figure}

%--------------------------------------------------------
\subsection*{FX Option Exposure}\label{sec:fxoption}
%--------------------------------------------------------

This example (in folder {\tt Examples/Example\_7}, as the FX Forward example) illustrates the exposure evolution for an
FX Option, see figure \ref{fig_7}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_fxoption.pdf}
\end{center}
\caption{EUR/USD FX Call and Put Option exposure evolution, same underlying and market data as in section
  \ref{sec:fxfwd}, compared to the call and put option price as of today (flat line). Simulation with 5000 paths and
  quarterly time steps.}
\label{fig_7}
\end{figure}
Recall that the FX Option value $NPV(t)$ as of time $0 \leq t \leq T$ satisfies
\begin{align*}
\frac{NPV(t)}{N(t)} &= \mbox{Nominal}\times\E_t\left[\frac{(X(T) - K)^+}{N(T)}\right]\\
NPV(0) &= \E\left[\frac{NPV(t)}{N(t)}\right] = \E\left[\frac{NPV^+(t)}{N(t)} \right]= \EPE(t) 
\end{align*}
where $N(t)$ denotes the numeraire asset.
One would therefore expect a flat exposure evolution up to option expiry. The deviation from this in ORE's simulation is
due to the pricing approach chosen here under scenarios. A Black FX option pricer is used with deterministic Black
volatility derived from today's volatility structure (pushed or rolled forward, see section \ref{sec:sim_market}). The
deviation can be removed by extending the volatility modelling, e.g. implying model consistent Black volatilities in
each simulation step on each path.  
%\todo[inline]{Add exposure evolution graph with 'simulated' FX vol}

%--------------------------------------------------------
\subsection{Cross Currency Swap Exposure, without FX Reset}
%--------------------------------------------------------

The case in {\tt Examples/Example\_8} is a vanilla cross currency Swap. It shows the typical blend of an Interest Rate
Swap's saw tooth exposure evolution with an FX Forward's exposure which increases monotonically to final maturity, see
figure \ref{fig_6}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_ccswap.pdf}
\end{center}
\caption{Cross Currency Swap exposure evolution without mark-to-market notional reset. Simulation with 1000 paths and
  quarterly time steps.}
\label{fig_6}
\end{figure}

%--------------------------------------------------------
\subsection{Cross Currency Swap Exposure, with FX Reset}
%--------------------------------------------------------

The effect of the FX resetting feature, common in Cross Currency Swaps nowadays, is shown in {\tt Examples/Example\_9}.
The example shows the exposure evolution of a EUR/USD cross currency basis Swap with FX reset at each interest period
start, see figure \ref{fig_6b}. As expected, the notional reset causes an exposure collapse at each period start when
the EUR leg's notional is reset to match the USD notional.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_xccy_reset.pdf}
\end{center}
\caption{Cross Currency Basis Swap exposure evolution with and without mark-to-market notional reset. Simulation with
  1000 paths and quarterly time steps.}
\label{fig_6b}
\end{figure}
  
%--------------------------------------------------------
\subsection{Netting Set, Collateral, XVAs, XVA Allocation}
%--------------------------------------------------------

In this example (see folder {\tt Examples/Example\_10}) we showcase a small netting set consisting of three Swaps in
different currencies, with different collateral choices
\begin{itemize}
\item no collateral - figure \ref{fig_8},
\item collateral with threshold (THR) 1m EUR, minimum transfer amount (MTA) 100k EUR, margin period of risk (MPOR) 2
  weeks - figure \ref{fig_9}
\item collateral with zero THR and MTA, and MPOR 2w - figure \ref{fig_10}
\end{itemize}
The exposure graphs with collateral and positive margin period of risk show typical spikes. What is causing these? As
sketched in appendix \ref{sec:app_collateral}, ORE uses a {\em classical collateral model} that applies collateral
amounts to offset exposure with a time delay that corresponds to the margin period of risk. The spikes are then caused
by instrument cash flows falling between exposure measurement dates $d_1$ and $d_2$ (an MPOR apart), so that a
collateral delivery amount determined at $d_1$ but settled at $d_2$ differs significantly from the closeout amount at
$d_2$ causing a significant residual exposure for a short period of time. See for example \cite{Andersen2016} for a
recent detailed discussion of collateral modelling. The approach currently implemented in ORE corresponds to {\em
  Classical+} in \cite{Andersen2016}, the more conservative approach of the classical methods. The less conservative
alternative, {\em Classical-}, would assume that both parties stop paying trade flows at the beginning of the MPOR, so
that the P\&L over the MPOR does not contain the cash flow effect, and exposure spikes are avoided. Note that the size
and position of the largest spike in figure \ref{fig_9} is consistent with a cash flow of the 40 million GBP Swap in the
example's portfolio that rolls over the 3rd of March and has a cash flow on 3 March 2020, a bit more than four years
from the evaluation date.
  
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_nocollateral_epe.pdf}
\end{center}
\caption{Three Swaps netting set, no collateral. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_8}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.45]{mpl_threshold_break_epe.pdf}
\end{center}
\caption{Three Swaps netting set, THR=1m EUR, MTA=100k EUR, MPOR=2w. The red evolution assumes that the each trade is
  terminated at the next break date. The blue evolution ignores break dates. Simulation with 5000 paths and bi-weekly
  time steps.}
\label{fig_9}
\end{figure}

%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=1.0]{example_mta_epe.pdf}
%\end{center}
%\caption{Three swaps, threshold = 0, mta > 0.}
%\label{fig_7}
%\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_mpor_epe.pdf}
\end{center}
\caption{Three Swaps, THR=MTA=0, MPOR=2w. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_10}
\end{figure}

%--------------------------------------------------------
\subsection*{CVA, DVA, FVA, COLVA, MVA, Collateral Floor}
%--------------------------------------------------------

We use one of the cases in {\tt Examples/Example\_10} to demonstrate the
XVA outputs, see folder {\tt Examples/Example\_10/Output/collateral\_threshold\_dim}.

\medskip The summary of all value adjustments (CVA, DVA, FVA, COLVA, MVA, as well as the Collateral Floor) is provided
in file {\tt xva.csv}.  The file includes the allocated CVA and DVA numbers to individual trades as introduced in the
next section. The following table illustrates the file's layout, omitting the three columns containing allocated data.

\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|r|r|r|r|r|r|r|r|r|}
\hline
TradeId & NettingSetId & CVA & DVA & FBA & FCA & COLVA & MVA & CollateralFloor & BaselEPE & BaselEEPE \\
\hline
 & CPTY\_A &  6,521  &  151,193  & -946  &  72,103  &  2,769  & -14,203  &  189,936  &  113,260  &  1,211,770 \\
Swap\_1 & CPTY\_A &  127,688  &  211,936  & -19,624  &  100,584  &  n/a  &  n/a  &  n/a   &  2,022,590  &  2,727,010 \\
Swap\_3 & CPTY\_A &  71,315  &  91,222  & -11,270  &  43,370  &  n/a  &  n/a  &  n/a   &  1,403,320  &  2,183,860 \\
Swap\_2 & CPTY\_A &  68,763  &  100,347  & -10,755  &  47,311  &  n/a  &  n/a  &  n/a   &  1,126,520  &  1,839,590 \\
\hline
\end{tabular}
}
\end{center}

The line(s) with empty TradeId column contain values at netting set level, the others contain uncollateralised
single-trade VAs.  Note that COLVA, MVA and Collateral Floor are only available at netting set level at which collateral
is posted.

\medskip
Detailed output is written for COLVA and Collateral Floor to file {\tt colva\_nettingset\_*.csv} which shows the 
incremental contributions to these two VAs through time.


%--------------------------------------------------------
\subsection*{Exposure Reports \& XVA Allocation to Trades}
%--------------------------------------------------------
Using the example in folder {\tt Examples/Example\_10} we illustrate here the layout of an exposure report produced by
ORE. The report shows the exposure evolution of Swap\_1 without collateral which - after running Example\_10 - is found
in folder \\
{\tt Examples/Example\_10/Output/collateral\_none/exposure\_trade\_Swap\_1.csv}:

\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|r|r|r|r|r|r|r|r|}
\hline
TradeId & Date & Time & EPE & ENE & AllocEPE & AllocENE & PFE & BaselEE & BaselEEE \\
\hline
Swap\_1 & 05/02/16 & 0.0000 & 0  & 1,711,748  & 0  & 0  & 0  & 0  & 0 \\
Swap\_1 & 19/02/16 & 0.0383 & 38,203   & 1,749,913  & -1,200,677 & 511,033 & 239,504 & 38,202 & 38,202 \\
Swap\_1 & 04/03/16 & 0.0765 & 132,862  & 1,843,837 & -927,499 & 783,476 & 1,021,715 & 132,845 & 132,845 \\
%Swap\_1 & 18/03/16 & 0.1148 & 299,155  & 1,742,450  & -650,225  & 793,067  & 1,914,150  & 299,091  & 299,091 \\
%Swap\_1 & 01/04/16 & 0.1530 & 390,178  & 1,834,810  & -552,029  & 892,604  & 2,373,560  & 390,058  & 390,058 \\
%Swap\_1 & 15/04/16 & 0.1913 & 471,849  & 1,918,600  & -465,580  & 981,171  & 2,765,710  & 471,659  & 471,659 \\
%Swap\_1 & 29/04/16 & 0.2295 & 550,301  & 2,000,640  & -330,578  & 1,119,760  & 3,106,810  & 550,016  & 550,016 \\
%Swap\_1 & 13/05/16 & 0.2678 & 620,279  & 2,074,880  & -266,042  & 1,188,560  & 3,427,080  & 619,888  & 619,888 \\
%Swap\_1 & 27/05/16 & 0.3060 & 690,018  & 2,140,320  & -190,419  & 1,259,880  & 3,778,570  & 689,509  & 689,509 \\
%Swap\_1 & 10/06/16 & 0.3443 & 763,207  & 2,206,020  & -137,681  & 1,305,130  & 4,052,870  & 762,560  & 762,560 \\
Swap\_1 & ... & ...& ... & ... & ... & ... & ... & ... & ... \\
\hline
\end{tabular}
}
\end{center}

The exposure measures EPE, ENE and PFE, and the Basel exposure measures $EE_B$ and $EEE_B$, are defined in appendix
\ref{sec:app_exposure}. Allocated exposures are defined in appendix \ref{sec:app_allocation}. The PFE quantile and
allocation method are chosen as described in section \ref{sec:analytics}. \\

In addition to single trade exposure files, ORE produces an exposure file per netting set. The example from the same
folder as above is:

\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|r|r|r|r|r|r|r|}
\hline
NettingSet & Date & Time & EPE & ENE & PFE & ExpectedCollateral & BaselEE & BaselEEE \\
\hline
CPTY\_A & 05/02/16 & 0.0000 & 1,203,836 & 0 & 1,203,836 & 0 & 1,203,836 & 1,203,836 \\%1,211,770 & 0 & 1,211,770 & 0 & 1,211,770 & 1,211,770\\
CPTY\_A & 19/02/16 & 0.0383 & 1,337,713 & 137,326 & 3,403,460 & 0 & 1,337,651 & 1,337,651 \\ %0.0383 & 1,344,220 & 137,776 & 3,414,000 & 0 & 1,344,160 & 1,344,160\\
%CPTY\_A & 04/03/16 & 0.0765 & 1,518,610 & 308,381 & 4,354,060 & 0 & 1,518,410 & 1,518,410\\
%CPTY\_A & 18/03/16 & 0.1148 & 1,846,900 & 382,068 & 5,200,730 & 0 & 1,846,500 & 1,846,500\\
%CPTY\_A & 01/04/16 & 0.1530 & 1,961,290 & 494,416 & 5,869,470 & 0 & 1,960,690 & 1,960,690\\
%CPTY\_A & 15/04/16 & 0.1913 & 2,067,240 & 598,283 & 6,384,140 & 0 & 2,066,400 & 2,066,400\\
%CPTY\_A & 29/04/16 & 0.2295 & 2,053,670 & 745,960 & 6,740,070 & 0 & 2,052,610 & 2,066,400\\
%CPTY\_A & 13/05/16 & 0.2678 & 2,149,190 & 845,507 & 6,930,230 & 0 & 2,147,840 & 2,147,840\\
%CPTY\_A & 27/05/16 & 0.3060 & 2,235,630 & 930,218 & 7,295,440 & 0 & 2,233,980 & 2,233,980\\
%CPTY\_A & 10/06/16 & 0.3443 & 2,314,470 & 1,014,690 & 7,753,190 & 0 & 2,312,510 & 2,312,510\\
CPTY\_A & ... & ...& ... & ... & ... & ... & ... & ...\\
%CPTY\_A & 07/07/17 & 1.4167 & 3,320,430 & 2,423,890 & 12,787,900 & 0 & 3,304,650 & 3,304,650\\
%CPTY\_A & 21/07/17 & 1.4551 & 3,351,780 & 2,452,640 & 12,964,200 & 0 & 3,335,420 & 3,335,420\\
%CPTY\_A & 04/08/17 & 1.4934 & 3,302,820 & 2,511,500 & 12,796,100 & 0 & 3,286,260 & 3,335,420\\
%CPTY\_A & 18/08/17 & 1.5318 & 3,339,840 & 2,545,850 & 13,120,000 & 0 & 3,322,640 & 3,335,420\\
%CPTY\_A & 01/09/17 & 1.5701 & 3,371,300 & 2,576,100 & 13,238,700 & 0 & 3,353,480 & 3,353,480\\
%CPTY\_A & 15/09/17 & 1.6085 & 3,279,670 & 2,555,370 & 13,041,300 & 0 & 3,261,880 & 3,353,480\\
%CPTY\_A & 29/09/17 & 1.6468 & 3,305,060 & 2,579,200 & 13,072,800 & 0 & 3,286,680 & 3,353,480\\
%CPTY\_A & 13/10/17 & 1.6852 & 3,332,830 & 2,604,200 & 13,225,600 & 0 & 3,313,850 & 3,353,480\\
%CPTY\_A & 27/10/17 & 1.7236 & 3,280,280 & 2,661,770 & 13,034,600 & 0 & 3,261,150 & 3,353,480\\
%CPTY\_A & 13/11/17 & 1.7701 & 3,316,800 & 2,701,060 & 13,331,600 & 0 & 3,296,880 & 3,353,480\\
%CPTY\_A & 24/11/17 & 1.8003 & 3,337,760 & 2,720,870 & 13,402,400 & 0 & 3,317,280 & 3,353,480\\
%CPTY\_A & ... & ...& ... & ... & ... & ... & ... & ...\\
\hline
\end{tabular}
}
\end{center}

Allocated exposures are missing here, as they make sense at the trade level only, and the expected collateral balance is
added for information (in this case zero as collateralisation is deactivated in this example).

\medskip The allocation of netting set exposure and XVA to the trade level is frequently required by finance
departments. This allocation is also featured in {\tt Examples/Example\_10}. We start again with the uncollateralised
case in figure \ref{fig_12}, followed by the case with threshold 1m EUR in figure \ref{fig_13}.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_nocollateral_allocated_epe.pdf}
\end{center}
\caption{Exposure allocation without collateral. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_12}
\end{figure}
In both cases we apply the {\em marginal} (Euler) allocation method as published by Pykhtin and Rosen in 2010, hence we
see the typical negative EPE for one of the trades at times when it reduces the netting set exposure. The case with
collateral moreover shows the typical spikes in the allocated exposures.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_threshold_allocated_epe.pdf}
\end{center}
\caption{Exposure allocation with collateral and threshold 1m EUR. Simulation with 5000 paths and bi-weekly time steps.}
\label{fig_13}
\end{figure}
The analytics results also feature allocated XVAs in file {\tt xva.csv} which are derived from the allocated exposure
profiles. Note that ORE also offers alternative allocation methods to the marginal method by Pykhtin/Rosen, which can be
explored with {\tt Examples/Example\_10}.

%--------------------------------------------------------
\subsection{Basel Exposure Measures}\label{sec:basel}
%--------------------------------------------------------

Example {\tt Example\_11} demonstrates the relation between the evolution of the expected exposure (EPE in our notation)
to the `Basel' exposure measures EE\_B, EEE\_B, EPE\_B and EEPE\_B as defined in appendix \ref{sec:app_exposure}. In
particular the latter is used in internal model methods for counterparty credit risk as a measure for the exposure at
default. It is a `derivative' of the expected exposure evolution and defined as a time average over the running maximum
of EE\_B up to the horizon of one year.
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_basel_exposures.pdf}
\end{center}
\caption{Evolution of the expected exposure of Vanilla Swap, comparison to the `Basel' exposure measures EEE\_B, EPE\_B and EEPE\_B. Note that the latter two are indistinguishable in this case, because the expected exposure is increasing for the first year.}
\label{fig_14}
\end{figure}

%--------------------------------------------------------
\subsection{Long Term Simulation with Horizon Shift}\label{sec:longterm}
%--------------------------------------------------------

The example in folder {\tt Example\_12} finally demonstrates an effect that, at first glance, seems to cause a serious
issue with long term simulations. Fortunately this can be avoided quite easily in the Linear Gauss Markov model setting
that is used here. \\

In the example we consider a Swap with maturity in 50 years in a flat yield curve environment. If we simulate this
naively as in all previous cases, we obtain a particularly noisy EPE profile that does not nearly reconcile with the
known exposure (analytical Swaption prices). This is shown in figure \ref{fig_15} (`no horizon shift'). The origin of
this issue is the width of the risk-neutral NPV distribution at long time horizons which can turn out to be quite small
so that the Monte Carlo simulation with finite number of samples does not reach far enough into the positive or negative
NPV range to adequately sample the distribution, and estimate both EPE and ENE in a single run.  Increasing the number
of samples may not solve the problem, and may not even be feasible in a realistic setting. \\

The way out is applying a `shift transformation' to the Linear Gauss Markov model, see {\tt
  Example\_12/Input/simulation2.xml} in lines 92-95:
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
        <ParameterTransformation>
          <ShiftHorizon>30.0</ShiftHorizon>
          <Scaling>1.0</Scaling>
        </ParameterTransformation>
\end{minted}
%\hrule
%\caption{LGM Shift transformation}
%\label{lst:shift_transformation}
\end{listing}

The effect of the 'ShiftHorizon' parameter $T$ is to apply a shift to the Linear Gauss Markov model's $H(t)$ parameter
(see appendix \ref{sec:app_rfe}) {\em after} the model has been calibrated, i.e. to replace:
$$ 
H(t) \rightarrow H(t) - H(T) 
$$ 
It can be shown that this leaves all expectations computed in the model (such as EPE and ENE) invariant. As explained in
\cite{Lichters}, subtracting an $H$ shift effectively means performing a change of measure from the `native' LGM measure
to a T-Forward measure with horizon $T$, here 30 years. Both negative and positive shifts are permissible, but only
negative shifts are connected with a T-Forward measure and improve numerical stability. \\

In our experience it is helpful to place the horizon in the middle of the portfolio duration to significantly improve
the quality of long term expectations. The effect of this change (only) is shown in the same figure \ref{fig_15}
(`shifted horizon').
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_longterm.pdf}
\end{center}
\caption{Long term Swap exposure simulation with and without horizon shift.}
\label{fig_15}
\end{figure}
Figure \ref{fig_15b} further illustrates the origin of the problem and its resolution: The rate distribution's mean
(without horizon shift or change of measure) drifts upwards due to convexity effects (note that the yield curve is flat
in this example), and the distribution's width is then too narrow at long horizons to yield a sufficient number of low
rate scenarios with contributions to the Swap's $\EPE$ (it is a floating rate payer). With the horizon shift (change of
measure), the distribution's mean is pulled 'back' at long horizons, because the convexity effect is effectively wiped
out at the chosen horizon, and the expected rate matches the forward rate.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_rates.pdf}
\end{center}
\caption{Evolution of rate distributions with and without horizon shift (change of measure). Thick lines indicate mean
  values, thin lines are contours of the rate distribution at $\pm$ one standard deviation.}
\label{fig_15b}
\end{figure}

%--------------------------------------------------------
\subsection{Dynamic Initial Margin and MVA}\label{sec:dim}
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_13} demonstrates Dynamic Initial Margin calculations (see also appendix
\ref{sec:app_dim}) for a number of elementary products:
\begin{itemize}
\item A single currency Swap in EUR (case A), 
\item a European Swaption in EUR with physical delivery (case B), 
\item a single currency Swap in USD (case C), and 
\item a EUR/USD cross currency Swap (case D).
\end{itemize}

The examples can be run as before with 

\medskip
\centerline{\tt python run\_A.py} 

\medskip
and likewise for cases B, C and D. The essential results of each run are are visualised in the form of 
\begin{itemize}
\item evolution of expected DIM
\item regression plots at selected future times 
\end{itemize}
illustrated for cases A and B in figures \ref{fig_ex13a_evolution} - \ref{fig_ex13b_regression}. 
In the three swap cases, the regression orders do make a noticeable difference in the respective expected DIM evolution. In the Swaption case B, first and second order polynomial choice makes a difference before option expiry. More details on this DIM model and its performance can be found in \cite{Anfuso2016,LichtersEtAl}.
 
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_evolution_A_swap_eur.pdf}
\end{center}
\caption{Evolution of expected Dynamic Initial Margin (DIM) for the EUR Swap of Example 13 A. DIM is evaluated using
  regression of NPV change variances versus the simulated 3M Euribor fixing; regression polynomials are zero, first and
  second order (first and second order curves are not distinguishable here). The simulation uses 1000 samples and a time
  grid with bi-weekly steps in line with the Margin Period of Risk.}
\label{fig_ex13a_evolution}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_regression_A_swap_eur.pdf}
\end{center}
\caption{Regression snapshot at time step 100 for the EUR Swap of Example 13 A.}
\label{fig_ex13a_regression}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_evolution_B_swaption_eur.pdf}
\end{center}
\caption{Evolution of expected Dynamic Initial Margin (DIM) for the EUR Swaption of Example 13 B with expiry in 10Y
  around time step 100. DIM is evaluated using regression of NPV change variances versus the simulated 3M Euribor
  fixing; regression polynomials are zero, first and second order. The simulation uses 1000 samples and a time grid with
  bi-weekly steps in line with the Margin Period of Risk.}
\label{fig_ex13b_evolution}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_dim_regression_B_swaption_eur_t100.pdf}
\end{center}
\caption{Regression snapshot at time step 100 (before expiry) for the EUR Swaption of Example 13 B.}
\label{fig_ex13b_regression}
\end{figure}

%--------------------------------------------------------
\subsection{Minimal Market Data Setup}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_14} demonstrates using a minimal market data setup in order to rerun the vanilla Swap exposure simulation shown in {\tt Examples/Example\_1}. The minimal market data uses single points per curve where possible.

%--------------------------------------------------------
\subsection{Sensitivity Analysis, Stress Testing and Parametric Value-at-Risk}\label{ex:sensitivity_stress}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_15} demonstrates the calculation of sensitivities and stress scenarios. The
portfolio used in this example consists of

\begin{itemize}
\item a vanilla swap in EUR
\item a cross currency swap EUR-USD
\item a resettable cross currency swap EUR-USD
\item a FX forward EUR-USD
\item a FX call option on USD/GBP % commented out?
\item a FX put option on USD/EUR
\item an European swaption
\item a Bermudan swaption 
\item a cap and a floor in USD
\item a cap and a floor in EUR
\item a fixed rate bond
\item a floating rate bond with floor
\item an Equity call option, put option and forward on S\&P500
\item an Equity call option, put option and forward on Lufthansa
\item a CPI Swap referencing UKRPI
\item a Year-on-Year inflation swap referencing EUHICPXT
\item a USD CDS.
\end{itemize}

The sensitivity configuration in {\tt sensitivity.xml} aims at computing the following sensitivities

\begin{itemize}
\item discount curve sensitivities in EUR, USD; GBP, CHF, JPY, on pillars 6M, 1Y, 2Y, 3Y, 5Y, 7Y, 10Y, 15Y, 20Y (absolute shift of 0.0001)
\item forward curve sensitivities for EUR-EURIBOR 6M and 3M indices, EUR-EONIA, USD-LIBOR 3M and 6M, GBP-LIBOR 3M and
  6M, CHF-LIBOR-6M and JPY-LIBOR-6M indices (absolute shift of 0.0001)
\item yield curve shifts for a bond benchmark curve in EUR (absolute shift of 0.0001)
\item FX spot sensitivities for USD, GBP, CHF, JPY against EUR as the base currency (relative shift of 0.01)
\item FX vegas for USDEUR, GBPEUR, JPYEUR volatility surfaces (relative shift of 0.01)
\item swaption vegas for the EUR surface on expiries 1Y, 5Y, 7Y, 10Y and underlying terms 1Y, 5Y, 10Y (relative shift of 0.01)
\item caplet vegas for EUR and USD on an expiry grid 1Y, 2Y, 3Y, 5Y, 7Y, 10Y and strikes 0.01, 0.02, 0.03, 0.04,
  0.05. (absolute shift of 0.0001)
\item credit curve sensitivities on tenors 6M, 1Y, 2Y, 5Y, 10Y (absolute shift of 0.0001).
\item Equity spots for S\&P500 and Lufthansa
\item Equity vegas for S\&P500 and Lufthansa at expiries 6M, 1Y, 2Y, 3Y, 5Y
\item Zero inflation curve deltas for UKRPI and EUHICPXT at tenors 6M, 1Y, 2Y, 3Y, 5Y, 7Y, 10Y, 15Y, 20Y
\item Year on year inflation curve deltas for EUHICPXT at tenors 6M, 1Y, 2Y, 3Y, 5Y, 7Y, 10Y, 15Y, 20Y
\end{itemize}

Furthermore, mixed second order derivatives (``cross gammas'') are computed for discount-discount, discount-forward and
forward-forward curves in EUR.

By definition the sensitivities are zero rate sensitivities and optionlet sensitivities, no par sensitivities are
provided. The sensitivity analysis produces three output files.

The first, {\tt scenario.csv}, contains the shift
direction ({\tt UP}, {\tt DOWN}, {\tt CROSS}), the base NPV, the scenario NPV and the difference of these two for each
trade and sensitivity key. For an overview over the possible scenario keys see \ref{sec:sensitivity}.

The second file, {\tt sensitivity.csv}, contains the shift size (in absolute terms always) and first (``Delta'') and second
(``Gamma'') order finite differences computed from the scenario results. Note that the Delta and Gamma results are pure
differences, i.e. they are not divided by the shift size.

The second file also contains second order mixed differences according to the specified cross gamma filter, along with the shift sizes for the two factors involved.

The stress scenario definition in {\tt stresstest.xml} defines two stress tests:

\begin{itemize}
\item {\tt parallel\_rates}: Rates are shifted in parallel by 0.01 (absolute). The EUR bond benchmark curve is shifted by
  increasing amounts 0.001, ..., 0.009 on the pillars 6M, ..., 20Y. FX Spots are shifted by 0.01 (relative), FX vols by
  0.1 (relative), swaption and cap floor vols by 0.0010 (absolute).
  Credit curves are not yet shifted.
\item {\tt twist}: The EUR bond benchmark curve is shifted by amounts -0.0050, -0.0040, -0.0030, -0.0020, 0.0020,
  0.0040, 0.0060, 0.0080, 0.0100 on pillars 6M, 1Y, 2Y, 3Y, 5Y, 7Y, 10Y, 15Y, 20Y.
\end{itemize}

The corresponding output file {\tt stresstest.csv} contains the base NPV, the NPV under the scenario shifts and the
difference of the two for each trade and scenario label.

%\todo[inline]{Update after CDS has been added to the example.}
\medskip
Finally, this example demonstrates a parametric VaR calculation based on the sensitivity and cross gamma output from the sensitivity analysis (deltas, vegas, gammas, cross gammas) and an external covariance matrix input. The result in {\tt var.csv} shows a breakdown by portfolio, risk class (All, Interest Rate, FX, Inflation, Equity, Credit) and risk type (All, Delta \& Gamma, Vega). The results shown are Delta Gamma Normal VaRs for the 95\% and 99\% quantile, the holding period is incorporated into the input covariances. Alternatively, one can choose a Monte Carlo VaR which means that the sensitivity based P\&L distribution is evaluated with MC simulation assuming normal respectively log-normal risk factor distribution. 
 
%--------------------------------------------------------
\subsection{Equity Derivatives Exposure}\label{ex:equityderivatives}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_16} demonstrates the computation of NPV, sensitivities, exposures and XVA for a portfolio 
of OTC equity derivatives. The portfolio used in this example consists of:

\begin{itemize}
	\item an equity call option denominated in EUR (``Luft'')
	\item an equity put option denominated in EUR (``Luft'')
	\item an equity forward denominated in EUR (``Luft'')
	\item an equity call option denominated in USD (``SP5'')
	\item an equity put option denominated in USD (``SP5'')
	\item an equity forward denominated in USD (``SP5'')
	\item an equity Swap in USD with return type  ``price'' (``SP5'')
	\item an equity Swap in USD with return type ``total'' (``SP5'')
\end{itemize}

The step-by-step procedure for running ORE is identical for equities as for other asset classes; the same market and 
portfolio data files are used to store the equity market data and trade details, respectively. For the exposure 
simulation, the calibration parameters for the equity risk factors can be set in the usual {\tt simulation.xml} file.

Looking at the MtM results in the output file {\tt npv.csv} we observe that put-call parity ($V_{Fwd} = V_{Call} - 
V_{Put}$) is observed as expected. Looking at Figure \ref{fig_eq_call} we observe that the Expected Exposure profile of 
the equity call option trade is relatively smooth over time, while for the equity forward trade the Expected Exposure 
tends to increase as we approach maturity. This behaviour is similar to what we observe in sections \ref{sec:fxfwd} 
and \ref{sec:fxoption}. 

\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{mpl_eq_call.pdf}
	\end{center}
	\caption{Equity (``Luft'') call option and OTC forward exposure evolution, maturity in approximately 2.5 years. 
	Simulation with 
	10000 paths and quarterly time steps.}
	\label{fig_eq_call}
\end{figure}

%--------------------------------------------------------
\subsection{Inflation Swap Exposure under Dodgson-Kainth}% Example 17
\label{example:17}
%--------------------------------------------------------

The example portfolio in folder {\tt Examples/Example\_17} contains two CPI Swaps and one Year-on-Year Inflation Swap.
The terms of the three trades are as follows:

\begin{itemize}
\item CPI Swap 1: Exchanges on 2036-02-05 a fixed amount of 20m GBP for a 10m GBP notional inflated with UKRPI with base CPI 210
\item CPI Swap 2: Notional 10m GBP, maturity 2021-07-18, exchanging GBP Libor for GBP Libor 6M vs. $2\%$ x CPI-Factor (Act/Act), inflated with index UKRPI with base CPI 210
\item YOY Swap: Notional 10m EUR, maturity 2021-02-05, exchanging fixed coupons for EUHICPXT year-on-year inflation coupons
\item YOY Swap with capped/floored YOY leg: Notional 10m EUR, maturity 2021-02-05, exchanging fixed coupons for EUHICPXT year-on-year inflation coupons, YOY leg capped with 0.03 and floored with 0.005
\item YOY Swap with scheduled capped/floored YOY leg: Notional 10m EUR, maturity 2021-02-05, exchanging fixed coupons for EUHICPXT year-on-year inflation coupons, YOY leg capped with cap schedule and floored with floor schedule
\end{itemize}

The example generates cash flows, NPVs, exposure evolutions, XVAs, as well as two exposure graphs for CPI Swap 1 respectively the YOY Swap. For the YOY Swap and the both YOY Swaps with capped/floored YOY leg, the example generates their cash flows, NPVs, exposure evolutions, XVAs and sensitivities. Figure \ref{fig_cpi_swap} shows the CPI Swap exposure evolution.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{mpl_cpi_swap.pdf}
	\end{center}
	\caption{CPI Swap 1 exposure evolution. Simulation with 1000 paths and quarterly time steps.}
	\label{fig_cpi_swap}
\end{figure}

Figure \ref{fig_yoy_swap} shows the evolution of the 5Y maturity Year-on-Year inflation swap for comparison. Note that the inflation simulation model (Dodgson-Kainth, see appendix \ref{sec:app_rfe}) yields the evolution of inflation indices and inflation zero bonds which allows spanning future inflation zero curves and the pricing of CPI swaps. To price Year-on-Year inflation Swaps under future scenarios, we imply Year-on-Year inflation curves from zero inflation curves\footnote{Currently we discard the required (small) convexity adjustment. This will be supplemented in a subsequent release.}. Note that for pricing Year-on-Year Swaps as of today we use a separate inflation curve bootstrapped from quoted Year-on-Year inflation Swaps.
 
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{mpl_yoy_swap.pdf}
	\end{center}
	\caption{Year-on-Year Inflation Swap exposure evolution. Simulation with 1000 paths and quarterly time steps.}
	\label{fig_yoy_swap}
\end{figure}

%--------------------------------------------------------
\subsection{Bonds and Amortisation Structures}% Example 18
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_18} computes NPVs and cash flow projections for a vanilla bond portfolio
consisting of a range of bond products, in particular demonstrating amortisation features. Also, pricing using a benchmark bond price curve (bond definitions in referencedata.xml) is demonstrated:
\begin{itemize}
\item fixed rate bond
\item floating rate bond linked to Euribor 6M
\item bond switching from fixed to floating
\item bond with 'fixed amount' amortisation
\item bond with percentage amortisation relative to the initial notional
\item bond with percentage amortisation relative to the previous notional
\item bond with fixed annuity amortisation
\item bond with floating annuity amortisation (this example needs QuantLib 1.10 or higher to work, in particular the amount() method in the Coupon class needs to be virtual)
\item bond with fixed amount amortisation followed by percentage amortisation relative to previous notional
\item fixed rate bond using a benchmark bond price curve instead of the benchmark yield curve
\end{itemize}

After running the example, the results of the computation can be found in the output files {\tt npv.csv} and {\tt
  flows.csv}, respectively.

\medskip
Note that the amortisation features used here are linked to the LegData structure, hence not limited to the Bond instrument, see section \ref{ss:amortisationdata}.

%--------------------------------------------------------
\subsection{Swaption Pricing with Smile}% Example 19
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_19} demonstrates European Swaption pricing with and without smile. Calling

\medskip
\centerline{\tt python run.py}

\medskip
will launch two ORE runs using config files {\tt ore\_flat.xml} and {\tt ore\_smile.xml}, respectively. The only difference in these is referencing alternative market configurations {\tt todaymarket\_flat.xml} and {\tt todaysmarket\_smile.xml} using an ATM Swaption volatility matrix and a Swaption cube, respectively. NPV results are written to {\tt npv\_flat.cvs} and {\tt npv\_smile.csv}.

%--------------------------------------------------------
\subsection{Credit Default Swap Pricing}% Example 20
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_20} demonstrates Credit Default Swap pricing via ORE. Calling

\medskip
\centerline{\tt python run.py}

\medskip
will launch a single ORE run to process a single name CDS example and to generate NPV and cash flows in the usual result files. 

\medskip
CDS can be included in sensitivity analysis and stress testing. Exposure simulation for credit derivatives will follow in the next ORE release.

%--------------------------------------------------------
\subsection{CMS and CMS Cap/Floor Pricing}% Example 21
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_21} demonstrates the pricing of CMS and CMS Cap/Floor using a portfolio consisting of a CMS Swap (CMS leg vs. fixed leg) and a CMS Cap. Calling

\medskip
\centerline{\tt python run.py}

\medskip
will launch a single ORE run to process the portfolio and generate NPV and cash flows in the usual result files. 

\medskip
CMS structures can be included in sensitivity analysis, stress testing and exposure simulation. 

%--------------------------------------------------------
\subsection{Option Sensitivity Analysis with Smile}% Example 22
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_22} demonstrates the current state of sensitivity calculation for European options where the volatility surface has a smile. 

\medskip
The portfolio used in this example consists of
\begin{itemize}
	\item an equity call option denominated in USD (``SP5'')
	\item an equity put option denominated in USD (``SP5'')
	\item a receiver swaption in EUR
	\item an FX call option on EUR/USD
\end{itemize}

\medskip
Refer to appendix \ref{sec:app_sensi} for the current status of sensitivity implementation with smile. In this example the setup is as follows
\begin{itemize}
\item today's market is configured with volatility smile for all three products above
\item simulation market has two configurations, to simulate ``ATM only'' or the ``full surface''; ``ATM only'' means that only ATM volatilities are to be simulated and shifts to ATM vols are propagated to the respective smile section (see appendix \ref{sec:app_sensi});  
\item the sensitivity analysis has two corresponding configurations as well, ``ATM only'' and ``full surface''; note that the ``full surface'' configuration leads to explicit sensitivities by strike only in the case of Swaption volatilities, for FX and Equity volatilities only ATM sensitivity can be specified at the moment and sensitivity output is currently aggregated to the ATM bucket (to be extended in subsequent releases).
\end{itemize}

The respective output files end with ``{\tt\_fullSurface.csv}'' respectively ``{\tt\_atmOnly.csv}''.

%ORE supports two methods of simulating equity volatility smile. The first method simulates the entire surface using specific moneyness levels configured in simulation.xml. The second method simulates only the ATM equity volatilities, the other strikes are shifted relative to this new ATM using the $t_{0}$ smile.  This example compares both methods using the same sensitivity configuration as in {\tt Examples/Example\_15}. For the first method {\tt simulation\_fullSurface.xml} is used and all output files are appended with ``\_fullSurface'', for the second method {\tt simulation\_atmOnly.xml} is used and all output files are appended with ``\_atmOnly''.


%--------------------------------------------------------
\subsection{FRA and Average OIS Exposure}% Example 23
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_23} demonstrates pricing, cash flow projection and exposure simulation for two additional products
\begin{itemize}
\item Forward Rate Agreements
\item Averaging Overnight Index Swaps
\end{itemize}
using a minimal portfolio of four trades, one FRA and three OIS. The essential results are in {\tt npv.csv}, {\tt flows.csv} and 
four {\tt exposure\_trade\_*.csv} files.

%--------------------------------------------------------
\subsection{Commodity Derivatives, Pricing, Sensitivity, Exposure}% Example 24
\label{example:24}
%--------------------------------------------------------

Calling

\medskip
\centerline{\tt python run.py}

\medskip
in folder {\tt Examples/Example\_24} will launch two ORE runs. The first one determined by {\tt ore.xml} demonstrates pricing and sensitivity analysis for
\begin{itemize}
\item Commodity Forwards
\item European Commodity Options
\end{itemize}
using a minimal portfolio of four forwards and two options referencing WTI and Gold. 
The essential results are in {\tt npv.csv} and {\tt sensitivity.csv}.

The second run determined by {\tt ore\_wti.xml} demonstrates Commodity exposure simulation for a portfolio including a
\begin{itemize}
\item Commodity Forward
\item Commodity Swap
\item European Commodity Option
\item Commodity Average Price Option
\item Commodity Swaption
\end{itemize}
with the usual results, exposure reports and graphs. 

%--------------------------------------------------------
\subsection{CMS Spread with (Digital) Cap/Floor}% Example 25
\label{example:25}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_25}  demonstrates pricing, sensitivity analysis 
and exposure simulation for 
\begin{itemize}
\item Capped/Floored CMS Spreads
\item CMS Spreads with Digital Caps/Floors
\end{itemize}

The example can be run with

\medskip
\centerline{\tt python run.py}

\medskip
and results are in {\tt npv.csv}, {\tt sensitivity.csv}, {\tt exposure\_*.csv} 
and the exposure graphs in {\tt mpl\_cmsspread.pdf}.

%--------------------------------------------------------
\subsection{Bootstrap Consistency}% Example 26
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_26} confirms that bootstrapped curves 
correctly reprice the bootstrap instruments (FRAs, Interest Rate Swaps, FX Forwards, Cross
Currency Basis Swaps) using three pricing setups with
\begin{itemize}
\item EUR collateral discounting (configuration xois\_eur)
\item USD collateral discounting (configuration xois\_usd)
\item in-currency OIS discounting (configuration collateral\_inccy)
\end{itemize}
all defined in {\tt Examples/Input/todaysmarket.xml}.

\medskip
The required portfolio files need to be generated from market data and conventions in
{\tt Examples/Input} and trade templates in 
{\tt Examples/Example\_26/Helpers}, calling

\medskip
\centerline{\tt python TradeGenerator.py}

\medskip
This will place three portfolio files {\tt *\_portfolio.xml} in the input folder.
Thereafter, the three consistency checks can be run calling

\medskip
\centerline{\tt python run.py}

\medskip
Results are in three files {\tt *\_npv.csv} and should show zero NPVs for all benchmark instruments.

%--------------------------------------------------------
\subsection{BMA Basis Swap}% Example 27
\label{example:27}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_27} demonstrates pricing 
and sensitivity analysis for a series of USD Libor 3M vs. Averaged BMA (SIFMA) 
Swaps that correspond to the instruments used to bootstrap the BMA curve. 

The example can be run with

\medskip
\centerline{\tt python run.py}

\medskip
and results are in {\tt npv.csv} and {\tt sensitivity.csv}.

%--------------------------------------------------------
\subsection{Discount Ratio Curves}% Example 28
\label{example:28}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_28} shows how to use a yield curve 
built from a DiscountRatio segment. 
In particular, it builds a GBP collateralized in EUR discount curve by referencing 
three other discount curves:
\begin{itemize}
\item a GBP collateralised in USD curve
\item a EUR collateralised in USD curve
\item a EUR OIS curve i.e. a EUR collateralised in EUR curve
\end{itemize}

The implicit assumption in building the curve this way is that EUR/GBP FX 
forwards collateralised in EUR have the same fair market rate as EUR/GBP 
FX forwards collateralised in USD. This assumption is illustrated in the 
example by the NPV of the two forward instruments in the portfolio returning 
exactly 0 under both discounting regimes i.e. under USD collateralization with 
direct curve building and under EUR collateralization with the discount ratio 
modified ``GBP-IN-EUR'' curve.

Also, in this example, an assumption is made that there are no direct GBP/EUR FX 
forward or cross currency quotes available which in general is false. The example 
s merely for illustration.

Both collateralizaton scenarios can be run calling {\tt python run.py}.

%--------------------------------------------------------
\subsection{Curve Building using Fixed vs. Float Cross Currency Helpers}% Example 29
\label{example:29}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_29} demonstrates using fixed vs. float 
cross currency swap helpers. In particular, it builds a TRY collateralised in USD 
discount curve using TRY annual fixed vs USD 3M Libor swap quotes.

The portfolio contains an at-market fixed vs. float cross currency swap that is 
included in the curve building. The NPV of this swap should be zero when the example is run,
using {\tt python run.py} or ``directly'' calling {\tt ore[.exe] ore.xml}.

%--------------------------------------------------------
\subsection{USD-Prime Curve Building via Prime-LIBOR Basis Swap}% Example 30
\label{example:30}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_30} demonstrates the implementation of the USD-Prime index in the ORE.
The USD-Prime yield curve is built from USD-Prime vs USD 3M Libor basis swap quotes.
The portfolio consists of two fair basis swaps (NPVs equal to 0):
\begin{itemize}
\item US Dollar Prime Rate vs 3 Month LIBOR
\item US Dollar 3 Month LIBOR vs Fed Funds + 0.027
\end{itemize}

In particular, it is confirmed that the bootstrapped curves USD-FedFunds and USD-Prime follow
the 3\% rule observed on the market: {\tt U.S. Prime Rate = (The Fed Funds Target Rate + 3\%)}.
(See \url{http://www.fedprimerate.com/}.)

Running ORE in directory {\tt Examples/Example\_30} with {\tt python run.py }
yields the USD-Prime curve in {\tt Examples/Example\_30/Output/curves.csv.}

%--------------------------------------------------------
\subsection{Exposure Simulation using a Close-Out Grid}% Example 31
\label{example:31}
%--------------------------------------------------------

In the previous examples we have used a ``lagged'' approach, described at the end of appendix \ref{sec:app_collateral}, to take the Margin Period of Risk into account in exposure modelling. This has the disadvantage in ORE that we need to use equally-spaced time grids with time steps that match the MPoR, e.g. 2W, out to final portfolio maturity. 

In this example we demonstrate an alternative approach supported by ORE since release 6. In this approach we use two nested grids: The (almost) arbitrary main simulation grid is used to compute ``default values'' which feed into the collateral balance $C(t)$ filtered by MTA and Threshold etc; an auxiliary ``close-out'' grid, offset from the main grid by the MPoR, is used to compute the delayed close-out values $V(t)$ associated with time default time $t$. The difference between $V(t)$ and $C(t)$ causes a residual exposure $[V(t)-C(t)]^+$ even if minimum transfer amounts and thresholds are zero.

The close-out date value can be computed in two ways in ORE
\begin{itemize}
\item as of default date, by just evolving the market from default date to close-out date
   (``sticky date''), or 
\item  as of close-out date, by evolving both valuation date and market over the 
   close-out period (``actual date''), i.e., the portfolio ages and cash flows might occur
   in the close-out period causing spikes in the evolution of exposures. 
\end{itemize}

We are reusing one case from Example 10 here, perfect CSA with zero threshold and
minimum transfer amount, so that the remaining exposure is solely due to the MPoR
effect. The portfolio consists of a single at-the-money Swap in GBP. 
The relevant configuration changes that trigger this modelling are in the Parameters section of {\tt simulation.xml} as shown in Listing \ref{lst:close_out_grid}

\begin{listing}[H]
\begin{minted}[fontsize=\footnotesize]{xml}
  <Parameters>
    <Grid> ... </Grid>
    <Calendar> ... </Calendar>
    <Sequence> ... </Sequence>
    <Scenario> ... </Scenario>
    <Seed> ... </Seed>
    <Samples> ... </Samples>
    <CloseOutLag> 2W </CloseOutLag>
    <MporMode> StickyDate </MporMode><!-- Alternative: ActualDate -->
  </Parameters>
\end{minted}
\caption{Close-out grid specification}
\label{lst:close_out_grid}
\end{listing}

and moreover in the XVA analytics section of {\tt ore\_mpor.xml} as shown in Listing \ref{lst:calctype_nolag}.

\begin{listing}[H]
\begin{minted}[fontsize=\footnotesize]{xml}
  <Analytic type="xva">
    ...
    <Parameter name="calculationType"> NoLag </Parameter>
    ...
  </Parameters>
\end{minted}
\caption{Close-out grid specification}
\label{lst:calctype_nolag}
\end{listing}

Run as usual calling {\tt python run.py}.

%--------------------------------------------------------------------
\subsection{Inflation Swap Exposure under Jarrow-Yildrim}% Example 32
\label{example:32}
%--------------------------------------------------------------------

The example here is similar to that in Section \ref{example:17} in that we are generating exposures for inflation swaps. The example in Section \ref{example:17} uses the Dodgson-Kainth model whereas this example uses the Jarrow-Yildrim model. The valuation date is 2 Nov 2020 and the portfolio contains four spot starting inflation swaps:

\begin{itemize}
\item trade\_01: 20Y standard UKRPI ZCIIS struck at the fair market rate of 3.1925\% giving an NPV of 0.0. 
\item trade\_02: 20Y standard EUHICPXT ZCIIS struck at the fair market rate of 1.16875\% giving an NPV of 0.0.
\item trade\_03: 20Y year on year EUHICPXT swap.
\item trade\_04: 20Y year on year UKRPI swap.
\end{itemize}

The example generates cash flows, NPVs, exposure evolutions and XVAs.

%--------------------------------------------------------------------
\subsection{CDS Exposure Simulation}% Example 33
\label{example:33}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_33} is the credit variant of the example in
\ref{sec:example1}. Running ORE in directory {\tt Examples/Example\_33} with

\medskip
\centerline{\tt python run.py } 
\medskip

yields the exposure evolution in 

\medskip
\centerline{\tt Examples/Example\_33/Output/*.pdf } 
\medskip

and shown in figure \ref{fig_33}. 
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_cds_33_2w_10k.pdf}
\end{center}
\caption{Credit Default Swap expected exposure in a flat market environment from both parties' perspectives. The symbols are CDS Option prices. The simulation was run with bi-weekly time steps and 10,000 Monte Carlo samples to demonstrate the convergence of EPE and ENE profiles. A similar
outcome can be obtained more quickly with 5,000 samples on a monthly time grid which is the default setting of Example\_33. }
\label{fig_33}
\end{figure}
Both CDS simulation and CDS Option pricing are run with calls to the ORE executable, essentially 

\medskip
\centerline{\tt ore[.exe] ore.xml} 

\centerline{\tt ore[.exe] ore\_cds\_option.xml} 
\medskip

which are wrapped into the script {\tt Examples/Example\_33/run.py} provided with the ORE release.

This example demonstrates credit simulation using the LGM model and the calculation of Wrong Way Risk due to credit
correlation between the underlying entity of the CDS and the counterparty of the CDS trade via dynamic credit.
Positive correlation between the two names weakens the protection of the CDS whilst
negative correlation strengthens the protection.

The following table lists the XVA result from the example at different levels of correlation.


\begin{table}[hbt]
\scriptsize
\begin{center}
\begin{tabular}{|r|l|r|r|r|r|}
\hline
Correlation & NettingSetId & CVA & DVA & FBA & FCA \\
\hline
-100\%  &  CPTY\_B  &  -2,638  &  2,906  &  486  &  -1,057 \\
 -90\%  &  CPTY\_B  &  -2,204  &  2,906  &  488  &  -1,053 \\
 -50\%  &  CPTY\_B  &    -485  &  2,906  &  493  &  -1,040 \\
 -40\%  &  CPTY\_B  &     -60  &  2,906  &  495  &  -1,037 \\
 -30\%  &  CPTY\_B  &     363  &  2,906  &  496  &  -1,033 \\
 -20\%  &  CPTY\_B  &     784  &  2,906  &  498  &  -1,030 \\
 -10\%  &  CPTY\_B  &   1,204  &  2,906  &  500  &  -1,027 \\
   0\%  &  CPTY\_B  &   1,621  &  2,906  &  501  &  -1,023 \\
  10\%  &  CPTY\_B  &   2,036  &  2,906  &  503  &  -1,020 \\
  20\%  &  CPTY\_B  &   2,450  &  2,906  &  504  &  -1,017 \\
  30\%  &  CPTY\_B  &   2,861  &  2,906  &  506  &  -1,013 \\
  40\%  &  CPTY\_B  &   3,271  &  2,906  &  507  &  -1,010 \\
  50\%  &  CPTY\_B  &   3,679  &  2,906  &  509  &  -1,017 \\
  90\%  &  CPTY\_B  &   5,290  &  2,906  &  515  &    -994 \\
 100\%  &  CPTY\_B  &   5,689  &  2,906  &  517  &    -991 \\
\hline
\end{tabular}
\caption{CDS XVA results with LGM model}
\end{center}
\end{table}

%--------------------------------------------------------------------
\subsection{Wrong Way Risk}% Example 34
\label{example:34}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_34} is an extension of the example in
\ref{sec:example1} with dynamic credit and IR-CR correlation. As we are paying
float, negative correlation implies that we pay more when the counterparty's credit
worsens, leading to a surge of CVA.

The following table lists the XVA result from the example at different levels of correlation.

\begin{table}[hbt]
\scriptsize
\begin{center}
\begin{tabular}{|r|l|r|r|r|r|}
\hline
Correlation & NettingSetId & CVA & DVA & FBA & FCA \\
\hline
 -30\%  &  CPTY\_A  & 105,146  &  68,061  &  31,519  &  -4,127 \\
 -20\%  &  CPTY\_A  &  88,442  &  68,061  &  30,976  &  -4,219 \\
 -10\%  &  CPTY\_A  &  71,059  &  68,061  &  30,439  &  -4,314 \\
   0\%  &  CPTY\_A  &  52,983  &  68,061  &  29,909  &  -4,411 \\
  10\%  &  CPTY\_A  &  34,199  &  68,061  &  29,386  &  -4,511 \\
  20\%  &  CPTY\_A  &  14,691  &  68,061  &  28,869  &  -4,614 \\
  30\%  &  CPTY\_A  &  -5,554  &  68,061  &  28,360  &  -4,719 \\
\hline
\end{tabular}
\caption{IR Swap XVA results with LGM model}
\end{center}
\end{table}

%--------------------------------------------------------------------
\subsection{Flip View}% Example 35
\label{example:35}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_35} demonstrates how ORE can be used to quickly switch perspectives in XVA calculations with minimal changes in the {\tt ore.xml} file only. In particular it does not involve manipulating the portfolio input or the netting set.

%--------------------------------------------------------------------
\subsection{Choice of Measure}% Example 36
\label{example:36}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_36} illustrates the effect of measure changes on simulated expected and peak exposures. For that purpose we reuse Example 1 (un-collateralized vanilla swap exposure) and run the simulation three times with different risk-neutral measures,
\begin{itemize}
\item in the LGM measure as in Example 1 (note {\tt <Measure>LGM</Measure>} in {\tt simulation\_lgm.xml}, this is the default also if the Measure tag is omitted)  
\item in the more common Bank Account measure (note {\tt <Measure>BA</Measure>} in {\tt simulation\_ba.xml})  
\item in the T-Forward measure with horizon T=20 at the Swap maturity (note {\tt <Measure>LGM</Measure>}  and {\tt <ShiftHorizon>20.0</ShiftHorizon>} in {\tt simulation\_fwd.xml})
\end{itemize}

The results are summarized in the exposure evolution graphs in figure \ref{fig:36}. As expected, the expected exposures evolutions match across measures, as these are expected discounted NPVs and hence measure independent.
However, peak exposures are dependent on the measure choice as confirmed graphically here. Many more measures are accessible with ORE, by way of varying the T-Forward horizon which was chosen arbitrarily here to match the Swap's maturity.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.45]{mpl_exposures_measures.pdf}
\end{center}
\caption{Evolution of expected exposures (EPE) and peak exposures (PFE at the 95\% quantile) in three measures, LGM, Bank Account, T-Forward with T=20, with 10k Monte Carlo samples.}
\label{fig:36}
\end{figure}

%--------------------------------------------------------------------
\subsection{Multifactor Hull-White Scenario Generation}% Example 37
\label{example:37}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_37} illustrates the scenario generation under a Hull-White multifactor
model. The model is driven by two independent Brownian motions and has four states. The diffusion matrix sigma is
therefore 2 x 4. The reversion matrix is a 4 x 4 diagonal matrix and entered as an array. Both diffusion and reversion
are constant in time. Their values are not calibrated to the option market, but hardcoded in simulation.xml.

The values for the diffusion and reversion matrices were fitted to the first two principal components of a
(hypothetical) analyis of absolute rate curve movements. These input principal components can be found in
inputeigenvectors.csv in the input folder. The tenor is given in years, and the two components are given as column
vectors, see table \ref{tab:ex37_1}.

\begin{table}[hbt]
\begin{center}
\begin{tabular}{r|r|r}
tenor & eigenvector 1  & eigenvector 2   \\
\hline      
1     & 0.353553390593 & -0.537955502871 \\
2     & 0.353553390593 & -0.374924478795 \\
3     & 0.353553390593 & -0.252916811525 \\
5     & 0.353553390593 & -0.087587539893 \\
10    & 0.353553390593 & 0.12267800393   \\
15    & 0.353553390593 & 0.240659435416  \\
20    & 0.353553390593 & 0.339148675322  \\
30    & 0.353553390593 & 0.552478951238
\end{tabular}
\caption{Input principal components}
\label{tab:ex37_1}
\end{center}
\end{table}

The first eigenvector represent perfectly parallel movements. The second eigenvector represent a rotation around the 7y
point of the curve. Furthermore we prescribe an annual volatility of 0.0070 for the first components and 0.0030 for the
second one. The values can be compared to normal (bp) volatilities.

We follow \cite{Andersen_Piterbarg_2010} chapter 12.1.5 ``Multi-Factor Statistical Gaussian Model'' to calibrate the
diffusion and reversion matrices to the prescribed components and volatilities. We do not detail the procedure here and
refer the interested reader to the given reference.

The example generates a single monte carlo path with 5000 daily steps and outputs the generated scenarios in
scenariodump.csv. The python script pca.py performs a principal component analysis on this output. The model implied
eigenvalues are given in table \ref{tab:ex37_2}.

\begin{table}[hbt]
\begin{center}
\begin{tabular}{r|r}
number & value                  \\
\hline      
1      & 4.9144936649319346e-05 \\
2      & 8.846877641067412e-06  \\
3      & 5.82566039467854e-10   \\
4      & 2.1298948225571415e-10 \\
5      & 9.254913949332787e-11  \\
6      & 1.0861256211767673e-11 \\
7      & 8.478795662698618e-14  \\
8      & 9.74468069377584e-13   \\
\end{tabular}
\caption{Input principal components}
\label{tab:ex37_2}
\end{center}
\end{table}

Only the first two values are relevant, the following are all close to zero. The square root of the first two
eigenvalues is given in table \ref{tab:ex37_3}.

\begin{table}[hbt]
\begin{center}
\begin{tabular}{r|r}
number & sqrt(value)                \\
\hline      
1      & 0.007010344973631422       \\
2      & 0.0029743701250966414      \\
\end{tabular}
\caption{Input principal components}
\label{tab:ex37_3}
\end{center}
\end{table}

matching the prescribed input values of 0.0070 and 0.0030 quite well. The correpsonding eigenvectors are given in etable
\ref{tab:ex37_4}.

\begin{table}[hbt]
\begin{center}
\begin{tabular}{r|r|r}
tenor & eigenvector 1       & eigenvector 2       \\
\hline      
1     & 0.34688826736335926 & 0.5441204725042812  \\
2     & 0.3489303472083185  & 0.380259707350115   \\
3     & 0.350362134519783   & 0.2581408080614405  \\
5     & 0.3523983915961889  & 0.09230899007104967 \\
10    & 0.3550169593982022  & -0.11856777284904292\\
15    & 0.35647835947136625 & -0.23676104168229614\\
20    & 0.3577146190751303  & -0.3354486339442275 \\
30    & 0.36042236352102563 & -0.549124709243042  \\
\end{tabular}
\caption{Input principal components}
\label{tab:ex37_4}
\end{center}
\end{table}

again matching the input principal components quite well. The second eigenvector is the negative of the input vector
here (the principal compoennt analysis can not distinguish these of course).

The example also produces a plot comparing the input eigenvectors and the model implied eigenvectors as shown in figure \ref{fig:ex37}.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.50]{mpl_eigenvectors_ex37.pdf}
\end{center}
\caption{Input and model implied eigenvectors for a Hull-White 4-factor model calibrated to 2 principal components of
  rate curve movements (parallel + rotation). Notice that the model implied 2nd eigenvector is the negative of the input
  vector.}
\label{fig:ex37}
\end{figure}

%--------------------------------------------------------------------
\subsection{Cross Currency Swap Exposure using Multifactor Hull-White Models}% Example 38
\label{example:38}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_38} is similar to Example 8 (EPE, ENE for xccy swap), but uses a
multifactor HW model for EUR and USD to generate scenarios. The parametrization of the HW models is taken from Example
37.

Each of the two factors of each HW model is correlated with each of the two factors of the other currency's HW model and
with the FX factors. Remember that the factors represent principal components of interest rate movements and so the
correlations can be interpreted as correlations of these principal components with each other and the fx rate processes.

%--------------------------------------------------------------------
\subsection{Exposure Simulation using American Monte Carlo}% Example 39
\label{example:39}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_39} demonstrates how to use American Monte Carlo simulation (AMC) to generate exposures in ORE.
For a sketch of the methodology and comments on its implementation in ORE see appendix \ref{sec:app_amc}.

Calling 

\medskip
\centerline {\tt python run.py} 

\medskip
performs two ORE runs, a 'classical' exposure simulation and an American Monte Carlo simulation, both on a quarterly simulation grid and for the same portfolio consisting of four trades:

\begin{itemize}
\item Bermudan swaption
\item Single Currency Swap
\item Cross Currency Swap
\item FX Option
\end{itemize}

We use a 'flat' market here (yield curve and Swaption volatility surface). The number of simulation paths is 2k in the classic simulations. If not stated otherwise below, the number of training paths and simulation paths is 10k in the AMC simulations. 

In the following we compare the AMC exposure profiles to those produced by the 'classic' valuation engine for each trade and the netting set. 

Figure \ref{epe_swaption} shows the EPE and ENE for a Bermudan Swaption 10y into 10y in (base ccy) EUR with physical settlement. The classic run uses
the LGM grid engine for valuation. We observe close agreement between the two runs. To achieve the observed agreement, it is essential to set the LGM model's mean reversion speed to zero in both
\begin{itemize}
\item the Bermudan Swaption LGM pricing model (see Input/pricingengine.xml), and
\item the Cross Asset Model's IR model components (see Input/simulation.xml and Input/simulation\_amc.xml) 
\end{itemize}
and to use a high order 6 of the regression polynomials (see Input/pricingengine\_amc.xml).
 
\begin{figure}
  \includegraphics[width=0.8\textwidth]{mpl_amc_bermudanswaption.pdf}
  \caption{EPE of a EUR Bermudan Swaption computed with the classic and AMC valuation engines, using 50k training paths for the AMC simulation.}
  \label{epe_swaption}
\end{figure}

Figure \ref{epe_swap} shows the EPE and ENE for a 20y vanilla Swap in USD. The currency of
the amc calculator is USD in this case, i.e. it is different from the base ccy of the simulation (EUR). The consistency
of the classic and amc runs in particular demonstrates the correct application of the currency conversion factor
\ref{currency_conversion_factor}. To get a better accuracy for purposes of the plot in this document we increased the
number of training paths for this example to 50k and the order of the basis functions to 6.

\begin{figure}
  \includegraphics[width=0.8\textwidth]{mpl_amc_vanillaswap_usd.pdf}
  \caption{EPE of a USD swap computed with the classic and AMC valuation engines}
  \label{epe_swap}
\end{figure}

Figure \ref{epe_ccyswap} shows the EPE and ENE for a 20y cross currency Swap EUR-USD. 

\begin{figure}
  \includegraphics[width=0.8\textwidth]{mpl_amc_xccyswap.pdf}
  \caption{EPE of a EUR-USD cross currency swap computed with the classic and AMC valuation engines}
  \label{epe_ccyswap}
\end{figure}

Figure \ref{epe_fxoption} shows the EPE and ENE for a vanilla FX Option EUR-USD with 10y1m expiry. 
For the classic run the FX volatility surface is not implied by the cross asset model but kept flat, which
yields a slight hump in the profile. The AMC profile is flat on the other hand which demonstrates the consistency of the
FX Option pricing with the risk factor evolution model.

\begin{figure}
  \includegraphics[width=0.8\textwidth]{mpl_amc_fxoption.pdf}
  \caption{EPE of a EUR-USD FX option computed with the classic and AMC valuation engines}
  \label{epe_fxoption}
\end{figure}

\subsubsection*{Analytic Configuration}
\label{sec:amc_applicationconfig}

To use the AMC engine for an XVA simulation the following needs to be added to the {\tt simulation} analytic in {\tt ore.xml}:

\begin{minted}[fontsize=\scriptsize]{xml}
<Analytic type="simulation">
  ...
  <Parameter name="amc">Y</Parameter>
  <Parameter name="amcPricingEnginesFile">pricingengine_amc.xml</Parameter>
  <Parameter name="amcTradeTypes">Swaption</Parameter>
  ...
</Analytic>
\end{minted}

The trades which have a trade type matching one of the types in the \verb+amcTradeTypes+ list, will be built against the
pricing engine config provided and processed in the AMC engine. As a naming convention, pricing engines with engine type
AMC provide the required functionality to be processed by the AMC engine, for technical details cf. \ref{sec:app_amc}.

All other trades are processed by the classic simulation engine in ORE. The resulting cubes from the classic and AMC
simulation are joined and passed to the post processor in the usual way.

Note that since sometimes the AMC pricing engines have a different base ccy than the risk factor evolution model (see
below), a horizon shift parameter in the simulation set up should be set for all currencies, so that the shift also
applies to these reduced models.

\subsubsection*{Pricing Engine Configuration}
\label{sec:amc_pricingengineconfig}

At this point we assume that the reader is generally familiar with the configuration section 
\ref{sec:configuration}, in particular pricing engine configuration in section \ref{sec:configuration_pricingengines}.

The pricing engine configuration is similar for all AMC enabled products, e.g. for Bermudan Swaptions:

\begin{minted}[fontsize=\scriptsize]{xml}
<Product type="BermudanSwaption">
  <Model>LGM</Model>
  <ModelParameters/>
  <Engine>AMC</Engine>
  <EngineParameters>
    <Parameter name="Training.Sequence">MersenneTwisterAntithetic</Parameter>
    <Parameter name="Training.Seed">42</Parameter>
    <Parameter name="Training.Samples">50000</Parameter>
    <Parameter name="Training.BasisFunction">Monomial</Parameter>
    <Parameter name="Training.BasisFunctionOrder">6</Parameter>
    <Parameter name="Pricing.Sequence">SobolBrownianBridge</Parameter>
    <Parameter name="Pricing.Seed">17</Parameter>
    <Parameter name="Pricing.Samples">0</Parameter>
    <Parameter name="BrownianBridgeOrdering">Steps</Parameter>
    <Parameter name="SobolDirectionIntegers">JoeKuoD7</Parameter>
    <Parameter name="MinObsDate">true</Parameter>
    <Parameter name="RegressionOnExerciseOnly">false</Parameter>
  </EngineParameters>
</Product>
\end{minted}

The \verb+Model+ differs by product type, table \ref{tbl:amcconfig} summarises the supported product types and model and
engine types. The engine parameters are the same for all products:

\begin{enumerate}
\item \verb+Training.Sequence+: The sequence type for the traning phase, can be \verb+MersenneTwister+,
  \verb+MersenneTwisterAntithetc+, \verb+Sobol+, \verb+Burley2020Sobol+, \verb+SobolBrownianBridge+,
  \verb+Burley2020SobolBrownianBridge+
\item \verb+Training.Seed+: The seed for the random number generation in the training phase
\item \verb+Training.Samples+: The number of samples to be used for the training phase
\item \verb+Pricing.Sequence+: The sequence type for the pricing phase, same values allowed as for training
\item \verb+Training.BasisFunction+: The type of basis function system to be used for the regression analysis, can be
  \verb+Monomial+, \verb+Laguerre+, \verb+Hermite+, \verb+Hyperbolic+, \verb+Legendre+, \verb+Chbyshev+,
  \verb+Chebyshev2nd+
\item \verb+BasisFunctionOrder+: The order of the basis function system to be used
\item \verb+Pricing.Seed+: The seed for the random number generation in the pricing
\item \verb+Pricing.Samples+: The number of samples to be used for the pricing phase. If this number is zero, no pricing
  run is performed, instead the (T0) NPV is estimated from the training phase (this result is used to fill the T0 slice
  of the NPV cube)
\item \verb+BrownianBridgeOrdering+: variate ordering for Brownian bridges, can be \verb+Steps+, \verb+Factors+,
  \verb+Diagonal+
\item \verb+SobolDirectionIntegers+: direction integers for Sobol generator, can be \verb+Unit+, \verb+Jaeckel+,
  \verb+SobolLevitan+, \verb+SobolLevitanLemieux+, \verb+JoeKuoD5+, \verb+JoeKuoD6+, \verb+JoeKuoD7+,
  \verb+Kuo+, \verb+Kuo2+, \verb+Kuo3+
\item \verb+MinObsDate+: if true the conditional expectation of each cashflow is taken from the minimum possible
  observation date (i.e. the latest exercise or simulation date before the cashflow's event date); recommended setting
  is \verb+true+
\item \verb+RegressionOnExerciseOnly+: if true, regression coefficients are computed only on exercise dates and
  extrapolated (flat) to earlier exercise dates; only for backwards compatibility to older versions of the AMC module,
  recommended setting is \verb+false+
\end{enumerate}

\begin{table}[hbt]
  \begin{tabular}{l|l|l}
    Product Type & Model & Engine \\ \hline
    Swap & CrossAssetModel & AMC \\
    CrossCurrencySwap & CrossAssetModel & AMC \\
    FxOption & CrossAssetModel & AMC \\
    BermudanSwaption & LGM & AMC \\
    MultiLegOption & CrossAssetModel & AMC \\
  \end{tabular}
  \caption{AMC enabled products with engine and model types}
  \label{tbl:amcconfig}
\end{table}

\subsubsection*{Additional Features}
\label{sec:amc_sideproducts}

As a side product the AMC module provides plain MC pricing engines for Bermudan Swaptions and a new trade type
\verb+MultiLegOption+ with a corresponding MC pricing engine.

\subsubsection*{MC pricing engine for Bermudan swaptions}\label{sec:mc_bermudan_engine}

The following listing shows a sample configuration for the MC Bermudan Swaption engine. The model parameters are
identical to the LGM Grid engine configuration. The engine parameters on the other hand are the same as for the AMC
engine, see \ref{sec:amc_pricingengineconfig}.

\begin{minted}[fontsize=\scriptsize]{xml}
<Product type="BermudanSwaption">
  <Model>LGM</Model>
  <ModelParameters>
    <Parameter name="Calibration">Bootstrap</Parameter>
    <Parameter name="CalibrationStrategy">CoterminalDealStrike</Parameter>
    <Parameter name="Reversion_EUR">0.0050</Parameter>
    <Parameter name="Reversion_USD">0.0030</Parameter>
    <Parameter name="ReversionType">HullWhite</Parameter>
    <Parameter name="VolatilityType">HullWhite</Parameter>
    <Parameter name="Volatility">0.01</Parameter>
    <Parameter name="ShiftHorizon">0.5</Parameter>
    <Parameter name="Tolerance">1.0</Parameter>
  </ModelParameters>
  <Engine>MC</Engine>
  <EngineParameters>
    <Parameter name="Training.Sequence">MersenneTwisterAntithetic</Parameter>
    <Parameter name="Training.Seed">42</Parameter>
    <Parameter name="Training.Samples">10000</Parameter>
    <Parameter name="Training.BasisFunction">Monomial</Parameter>
    <Parameter name="Training.BasisFunctionOrder">6</Parameter>
    <Parameter name="Pricing.Sequence">SobolBrownianBridge</Parameter>
    <Parameter name="Pricing.Seed">17</Parameter>
    <Parameter name="Pricing.Samples">25000</Parameter>
    <Parameter name="BrownianBridgeOrdering">Steps</Parameter>
    <Parameter name="SobolDirectionIntegers">JoeKuoD7</Parameter>
  </EngineParameters>
</Product>
\end{minted}

\subsubsection*{Multi Leg Options / MC pricing engine}

The following listing shows a sample MultiLegOption trade. It consists of

\begin{enumerate}
\item an option data block; this is optional, see below
\item a number of legs; in principle all leg types are supported, the number of legs is arbitrary and they can be in
  different currencies; if the payment currency of a leg is different from a floating index currency, this is
  interpreted as a quanto payoff
\end{enumerate}

If the option block is given, the trade represents a Bermudan swaption on the underlying legs. If the option block is
missing, the legs themselves represent the trade.

See \ref{sec:amc_limitations} for limitations of the multileg option pricing engine.

\begin{minted}[fontsize=\scriptsize]{xml}
<Trade id="Sample_MultiLegOption">
  <TradeType>MultiLegOption</TradeType>
  <Envelope>...</Envelope>
  <MultiLegOptionData>
    <OptionData>
      <LongShort>Long</LongShort>
      <OptionType>Call</OptionType>
      <Style>Bermudan</Style>
      <Settlement>Physical</Settlement>
      <PayOffAtExpiry>false</PayOffAtExpiry>
      <ExerciseDates>
        <ExerciseDate>2026-02-25</ExerciseDate>
        <ExerciseDate>2027-02-25</ExerciseDate>
        <ExerciseDate>2028-02-25</ExerciseDate>
      </ExerciseDates>
    </OptionData>
    <LegData>
      <LegType>Floating</LegType>
      <Payer>false</Payer>
      <Currency>USD</Currency>
      <Notionals>
        <Notional>100000000</Notional>
      </Notionals>
      ...
    </LegData>
    <LegData>
      <LegType>Floating</LegType>
      <Payer>true</Payer>
      <Currency>EUR</Currency>
      <Notionals>
        <Notional>100000000</Notional>
      </Notionals>
      ...
    </LegData>
  </MultiLegOptionData>
</Trade>
\end{minted}

The pricing engine configuration is similar to that of the MC Bermudan swaption engine, cf.
\ref{sec:mc_bermudan_engine}, also see the following listing.

\begin{minted}[fontsize=\scriptsize]{xml}
  <Product type="MultiLegOption">
  <Model>CrossAssetModel</Model>
  <ModelParameters>
    <Parameter name="Tolerance">0.0001</Parameter>
    <!-- IR -->
    <Parameter name="IrCalibration">Bootstrap</Parameter>
    <Parameter name="IrCalibrationStrategy">CoterminalATM</Parameter>
    <Parameter name="ShiftHorizon">1.0</Parameter>
    <Parameter name="IrReversion_EUR">0.0050</Parameter>
    <Parameter name="IrReversion_GBP">0.0070</Parameter>
    <Parameter name="IrReversion_USD">0.0080</Parameter>
    <Parameter name="IrReversion">0.0030</Parameter>
    <Parameter name="IrReversionType">HullWhite</Parameter>
    <Parameter name="IrVolatilityType">HullWhite</Parameter>
    <Parameter name="IrVolatility">0.0050</Parameter>
    <!-- FX -->
    <Parameter name="FxCalibration">Bootstrap</Parameter>
    <Parameter name="FxVolatility_EURUSD">0.10</Parameter>
    <Parameter name="FxVolatility">0.08</Parameter>
    <Parameter name="ExtrapolateFxVolatility_EURUSD">false</Parameter>
    <Parameter name="ExtrapolateFxVolatility">true</Parameter>
    <!-- Correlations IR-IR, IR-FX, FX-FX -->
    <Parameter name="Corr_IR:EUR_IR:GBP">0.80</Parameter>
    <Parameter name="Corr_IR:EUR_FX:GBPEUR">-0.50</Parameter>
    <Parameter name="Corr_IR:GBP_FX:GBPEUR">-0.15</Parameter>
  </ModelParameters>
  <Engine>MC</Engine>
  <EngineParameters>
    <Parameter name="Training.Sequence">MersenneTwisterAntithetic</Parameter>
    <Parameter name="Training.Seed">42</Parameter>
    <Parameter name="Training.Samples">10000</Parameter>
    <Parameter name="Pricing.Sequence">SobolBrownianBridge</Parameter>
    <Parameter name="Pricing.Seed">17</Parameter>
    <Parameter name="Pricing.Samples">25000</Parameter>
    <Parameter name="Training.BasisFunction">Monomial</Parameter>
    <Parameter name="Training.BasisFunctionOrder">4</Parameter>
    <Parameter name="BrownianBridgeOrdering">Steps</Parameter>
    <Parameter name="SobolDirectionIntegers">JoeKuoD7</Parameter>
  </EngineParameters>
</Product>
\end{minted}

Model Parameters special to that product are

\begin{enumerate}
\item \verb+IrCalibrationStrategy+ can be \verb+None+, \verb+CoterminalATM+, \verb+UnderlyingATM+
\item \verb+FXCalibration+ can be \verb+None+ or \verb+Bootstrap+
\item \verb+ExtrapolateFxVolatility+ can be \verb+true+ or \verb+false+; if false, no calibration instruments are used
  that require extrapolation of the market fx volatilty surface in option expiry direction
\item \verb+Corr_Key1_Key2+: These entries describe the cross asset model correlations to be used; the syntax for
  \verb+Key1+ and \verb+Key2+ is the same as in the simulation configuration for the cross asset model
\end{enumerate}

%--------------------------------------------------------------------
\subsection{Par Sensitivity Analysis}% Example 40
\label{example:40}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_40}  demonstrates ORE's par sensitivity analysis (e.g. to Swap rates) 
that is implemented  by means of a Jacobi transformation of the "raw" sensitivities (e.g. to zero rates), see a sketch of the 
methodology in appendix \ref{app:par_sensi} and section \ref{sec:sensitivity} for configuration details.

To perform a par sensitivity analysis, the following required change in {\tt ore.xml} is required

\begin{minted}[fontsize=\scriptsize]{xml}
    <Analytic type="sensitivity">
      <Parameter name="active">Y</Parameter>
      <Parameter name="marketConfigFile">simulation.xml</Parameter>
      <Parameter name="sensitivityConfigFile">sensitivity.xml</Parameter>
      <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
      <Parameter name="scenarioOutputFile">sensi_scenarios.csv</Parameter>
      <Parameter name="sensitivityOutputFile">sensitivity.csv</Parameter>
      <Parameter name="outputSensitivityThreshold">0.000001</Parameter>
      <!-- Additional parametrisation for par sensitivity analysis -->
      <Parameter name="parSensitivity">Y</Parameter>
      <Parameter name="parSensitivityOutputFile">parsensitivity.csv</Parameter>
      <Parameter name="outputJacobi">Y</Parameter>
      <Parameter name="jacobiOutputFile">jacobi.csv</Parameter>
      <Parameter name="jacobiInverseOutputFile">jacobi_inverse.csv</Parameter>
    </Analytic>
\end{minted}

The portfolio used in this example includes products sensitive to 
\begin{itemize}
\item Discount and index curves
\item Credit curves
\item Inflation curves
\item CapFloor volatilities
\end{itemize}

The usual sensitivity analysis is performed by bumping the "raw" rates (zero rates, hazard rates, inflation zero rates, optionlet vols).
This is followed by the Jacobi transformation that turns "raw" sensitivities  into sensitivities in the par domain (Deposit/FRA/Swap rates, FX Forwards, CC Basis Swap spreads, 
CDS spreads, ZC and YOY Inflation Swap rates, flat Cap/Floor vols). The conversion is controlled by the additional {\tt ParConversion} data blocks 
in {\tt sensitivity.xml} where the assumed par instruments and corresponding conventions are coded, as shown below for three types of discount curves.

\begin{minted}[fontsize=\scriptsize]{xml}
  <DiscountCurves>
  
    <DiscountCurve ccy="EUR">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>2W,1M,3M,6M,9M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y</ShiftTenors>
      <ParConversion>
        <!--DEP, FRA, IRS, OIS, FXF, XBS -->
	<Instruments>OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS</Instruments>
	<SingleCurve>true</SingleCurve>
	<Conventions>
	  <Convention id="OIS">EUR-OIS-CONVENTIONS</Convention>
	</Conventions>
      </ParConversion>
    </DiscountCurve>   
    
    <DiscountCurve ccy="USD">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>2W,1M,3M,6M,9M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y</ShiftTenors>
      <ParConversion>
	<Instruments>FXF,FXF,FXF,FXF,FXF,XBS,XBS,XBS,XBS,XBS,XBS,XBS,XBS,XBS,XBS,XBS</Instruments>
	<SingleCurve>true</SingleCurve>
	<Conventions>
	  <Convention id="XBS">EUR-USD-XCCY-BASIS-CONVENTIONS</Convention>
	  <Convention id="FXF">EUR-USD-FX-CONVENTIONS</Convention>
	</Conventions>
      </ParConversion>

    <DiscountCurve ccy="GBP">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>2W,1M,3M,6M,9M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y</ShiftTenors>
      <ParConversion>
	<Instruments>DEP,DEP,DEP,DEP,DEP,IRS,IRS,IRS,IRS,IRS,IRS,IRS,IRS,IRS,IRS,IRS</Instruments>
	<SingleCurve>true</SingleCurve>
	<Conventions>
	  <Convention id="DEP">GBP-DEPOSIT</Convention>
	  <Convention id="IRS">GBP-6M-SWAP-CONVENTIONS</Convention>
	</Conventions>
      </ParConversion>
    </DiscountCurve>
  
  </DiscountCurves>
\end{minted}

Finally note that par sensitivity analysis requires that the shift tenor grid in the sensitivity data above matches the corresponding grid in the simulation (market) configuration.
See also section \ref{sec:sensitivity}.

%--------------------------------------------------------------------
\subsection{Multi-threaded Exposure Simultion}% Example 41
\label{example:41}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_41} demonstrates the multithreaded valuation engine to generate the exposure for a
portfolio of 8 copies of the vanilla swap in {\tt Example\_1}.

%--------------------------------------------------------------------
\subsection{ORE Python Module}% Example 42
\label{example:42}
%--------------------------------------------------------------------

Since release 9 (March 2023) we provide easy access to ORE via a pre-compiled Python module. Some example scripts using this ORE module are provided in this example, so change to this directory first

\medskip
{\tt cd Example\_42} 

\medskip
The examples require Python 3. The ORE Python module is then installed with a one-liner, see step 3 below. However, to separate ORE from any other Python environments on your machine, we recommend creating a virtual environment first. In that case the steps are as follows. 

\begin{enumerate}
\item To create a virtual environment: {\tt python -m venv env1} 
\item To activate this environment on Windows: {\tt .{\bs}env1{\bs}Scripts{\bs}activate.bat}  \\
or on macOS/Linux: {\tt ./env1/bin/activate }  
\item Then install the latest release of ORE:\\
{\tt pip install open-source-risk-engine } 
\item Try examples:\\
	\begin{itemize} 
	\item {\tt python ore.py} \\
	This demonstrates the Python-wrapped version of the ORE application that is also used in the command line application {\tt ore.exe}. We use it here to re-run the Swap exposure of {\tt Example\_1}. 
	\item {\tt python ore2.py} \\
	This extends the previous example and shows how to access and post-process ORE in-memory results in the Python framework without reading files. 
	\item {\tt python commodityforward.py} \\
	The ORE Python module also allows lower-level access to the QuantLib and QuantExt libraries, demonstrated here for a CommodityForward instrument defined in QuantExt. 
	Note that the ORE Python module contains the entire QuantLib Python functionality.
	\end{itemize}
	More use cases of the ORE Python module including Jupyter notebooks can be found in the ORE SWIG repository, in particular in folder OREAnalytics-SWIG/Python/Examples. 
\item You can deactivate the environment with {\tt deactivate} \\
or even fully remove the environment again by removing the {\tt env1} folder.
\end{enumerate}

Finally, you can build the Python module and installable packages yourself following the instructions in sections \ref{sec:oreswig} based on your local ORE code. 

%--------------------------------------------------------------------
\subsection{Credit Portfolio Model}% Example 43
\label{example:43}
%--------------------------------------------------------------------

The purpose of the credit portfolio model in ORE is to generate an integrated portfolio gain/loss distribution at a given future horizon which is driven by 
\begin{itemize}
\item credit defaults and rating migrations in Bonds and CDS, and 
\item the PnL of a portfolio of derivatives over the specified time horizon.
\end{itemize}
The model integrates Credit and Market Risk by jointly evolving systemic credit risk drivers alongside the usual risk factors in ORE's Cross Asset Model.
See also the separate documentation in Docs/UserGuide/creditmodel.tex.

By running \\
\medskip
\centerline{{\tt python run.py}} 

\medskip
this example demonstrates the model's outcome for seven demo portfolios

\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Case & Credit Mode & Exposure Mode & Evaluation \\
\hline
\hline
Single Bond & Migration & Value & Analytic \\
\hline
Bond and Swap & Migration & Value & Analytic \\
\hline
3 Bonds & Migration & Value & Analytic \\
\hline
10 Bonds & Migration & Value & Analytic \\
\hline
10 Bonds & Migration & Value & Terminal Simulation \\
\hline
Bonds and CDS & Migration & Notional & Analytic \\
\hline
100 Bonds & Default & Notional & Analytic \\
\hline
\end{tabular}
\end{center}
The last demo case in this table can be activated by uncommenting the corresponding section at the end of the {\tt run.py} script.
 
%--------------------------------------------------------------------
\subsection{Initial Margin: ISDA SIMM and IM Schedule}% Example 44
\label{example:44}
%--------------------------------------------------------------------

This example demonstrates the calculation of initial margin using ISDA's Standard Initial Margin Model (SIMM) based on a provided 
sensitivity file in ISDA's Common Risk Interchange Format (CRIF). In addition, we show how to use the standard "IM Schdule" method to compute 
initial margin.

ORE covers all SIMM versions since inception to date, i.e.\ 1.0, 1.1, 1.2, 1.3, 1.3.38, 2.0, 2.1, 2.2, 2.3, 2.4 (=2.3.8), 2.5, 2.5A, 2.6 (=2.5.6).
All versions have been tested against the respective ISDA SIMM model unit test suites and pass these tests.
Any new SIMM versions will be added with each ORE release.

For SIMM versions >= 2.2 we support SIMM calculation for both MPoR horizons, 1d and 10d.
 
Note that you need to purchase a SIMM model license from ISDA if you want to use the model in production, and the unit test
suites mentioned above are provided to licensed vendors only. Therefore we unfortunately cannot share our ORE SIMM model 
test suite here either. 

By running \\
\medskip
\centerline{{\tt python run.py}} 

\medskip
ORE will pick up the small example CRIF file in {\tt Input/crif.csv} (i.e.\ par sensitivities rebucketed and reformatted to match the ISDA CRIF template) and generate the resulting SIMM report in a {\tt simm.csv} file.
This report shows ISDA SIMM results with the usual breakdown by product class, risk class, margin type, bucket and SIMM ``side'' (IM to call or post).
The SIMM calculation in this example is done for SIMM version 2.4 and 2.6, with MPoR 1d and 10d:

\begin{itemize}
  \item SIMM 2.4, 1-day MPoR
  \item SIMM 2.4, 10-day MPoR
  \item SIMM 2.6, 1-day MPoR
  \item SIMM 2.6, 10-day MPoR
\end{itemize}

\medskip
There are four SIMM-related input files -- {\tt ore\_SIMM2.4\_1D.xml}, {\tt ore\_SIMM2.4\_10D.xml}, {\tt ore\_SIMM2.6\_1D.xml}, {\tt ore\_SIMM2.6\_10D.xml} -- with corresponding folders in the {\tt Output/} directory.
The relevant inputs in the files are:

\begin{itemize}
\item SIMM version
\item name of the CRIF file to be loaded
\item calculation currency - this determines which Risk\_FX entries of the CRIF will be ignored in the SIMM calculation
\item result currency (optional) - currency of the resulting SIMM amounts in the report, by default equal to the calculation currency
\item MPoR horizon, in terms of days
\end{itemize}

The market data input and todays's market configuration required here is minimal - limited to FX rates for conversions from base/calculation currency into USD and into the result currency.

\bigskip
If the ORE Python module is installed, as shown in Example 42, then you can also run the SIMM example using

\medskip
\centerline{\tt python ore.py} 

\subsubsection*{IM Schedule}

As an additonal case in this example we demonstrate how to use the IM Schedule method to compute initial margin.
The related input file is {\tt Input/ore\_schedule.xml}. It is also run when calling {\tt python run.py}, and results are written to folder 
{\tt Output/IM\_SCHEDULE}.
The basic input is provided in CRIF file format where ORE expects two lines per trade, one with RiskClass = PV and one with RiskClass = Notional, 
so that the  amounts in these CRIF lines are interpeted as NPV respectively notional. 
Further required columns are product class and end date, as shown in the example {\tt Input/crif\_schedule.csv}. Note that the product class has to be in
\begin{itemize}
  \item Rates
  \item FX
  \item Equity
  \item Credit
  \item Commodity
\end{itemize}
in contrast to SIMM where we use the combined RatesFX.

To run the IM Schedule analytic, the following minimal addition to {\tt Input/ore\_schedule.xml} is required.
\begin{minted}[fontsize=\scriptsize]{xml}
  <Analytics>
    <Analytic type="imschedule">
      <Parameter name="active">Y</Parameter>
      <Parameter name="crif">crif_schedule.csv</Parameter>
      <Parameter name="calculationCurrency">USD</Parameter>
    </Analytic>
  </Analytics>
\end{minted}

%--------------------------------------------------------
\subsection{Collateralized Bond Obligation}% Example 45
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_45} demonstrates a Cashflow CDO or Collateralized Bond Obligation (CBO) via ORE. Calling

\medskip
\centerline{\tt python run.py}

\medskip
will launch a single ORE run to process a CBO example, referencing underyling bond portfolio of 20 trades. 
The CBO is represented by a CBO reference datum specified in the reference data file. 
NPV results are calculated for the investment in the junior tranche. 

%--------------------------------------------------------
\subsection{Generic Total Return Swap}% Example 46
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_46} demonstrates ORE's generic Total Return Swap referencing a CBO. 
Calling

\medskip
\centerline{\tt python run.py}

\medskip
will launch a single ORE run to process a TRS example and to generate NPV and cash flows in the usual result files.
As opposed to example 45, the CBO and its bondbasket are represented explicitly in the CBO node.

%--------------------------------------------------------
\subsection{Composite Trade}% Example 47
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_47} demonstrates the input of ORE's Composite Trade that can consist on any number 
and type of products covered by ORE. In this case the composite consists of two Equity Swaps.
Calling

\medskip
\centerline{\tt python run.py}

\medskip
runs ORE and generates an NPV report.

%--------------------------------------------------------
\subsection{Convertible Bond and ASCOT}% Example 48
%--------------------------------------------------------

This example in folder {\tt Examples/Example\_48} demonstrates the input of 
\begin{itemize}
\item a ConvertibleBond  trade
\item a related Asset Swapped Convertible Option Transaction (ASCOT)
\item a vanilla Swap that represents the package of Convertible Bond position and ASCOT
\end{itemize}

Calling
\medskip
\centerline{\tt python run.py}

\medskip
runs ORE and generates an NPV report.

%--------------------------------------------------------
\subsection{Bond Yield Shifted}% Example 49
\label{example:49}
%--------------------------------------------------------

The example in folder {\tt Examples/Example\_49} shows how to use a yield curve
built from a BondYieldShifted segment, as described in section \ref{sec:bond_yield_shifted}.

In particular, it builds the curve {\tt USD.BMK.GVN.CURVE\_SHIFTED} shifted by three liquid Bonds:

\begin{itemize}
\item Fixed rate USD Bond maturing in August 2023 with id {\tt EJ7706660}.
\item Fixed rate USD Bond maturing in September 2049 with id {\tt ZR5330686}.
\item Floating Rate Bond maturing in May 2025 with id {\tt AS064441}.
\end{itemize}

The resulting curve is exhibited in the {\tt curves.csv} output file.
Moreover, the results can be crosschecked against the NPVs, i.e. prices, of the ZeroBonds comprised in the portfolio.
\begin{itemize}
\item {\tt ZeroBond\_long}, maturing 2052-03-01 shows a price of 0.2080 akin to the 0.2080 in the curves output at the same date.
\item {\tt ZeroBond\_short}, maturing 2032-06-01 shows a price of 0.5808 aktin to the 0.808 in the curves output at the same date.
\end{itemize}

The example can be run calling {\tt python run.py}.


%--------------------------------------------------------------------
\subsection{Zero to Par sensitivity Conversion Analysis}% Example 50
\label{example:50}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_50} demonstrates ORE's capability to convert external computed zero sensitivities (e.g Zero rates) to par sensitivities (e.g. to Swap rates) 
that is implemented  by means of a Jacobi transformation of the "raw" sensitivities (e.g. to zero rates), see a sketch of the 
methodology in appendix \ref{app:par_sensi} and section \ref{sec:sensitivity} for configuration details.

To perform a par sensitivity analysis, the following required change in {\tt ore.xml} is required

\begin{minted}[fontsize=\scriptsize]{xml}
    <Analytic type="zeroToParSensiConversion">
      <Parameter name="active">Y</Parameter>
      <Parameter name="marketConfigFile">simulation.xml</Parameter>
      <Parameter name="sensitivityConfigFile">sensitivity.xml</Parameter>
      <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
	  <!-- Input file with the raw sensitivities -->
      <Parameter name="sensitivityInputFile">sensitivity.csv</Parameter>
      <Parameter name="idColumn">TradeId</Parameter>
      <Parameter name="riskFactorColumn">Factor_1</Parameter>
      <Parameter name="deltaColumn">Delta</Parameter>
	  <Parameter name="currencyColumn">Currency</Parameter>
	  <Parameter name="baseNpvColumn">Base NPV</Parameter>
 	  <Parameter name="shiftSizeColumn">ShiftSize_1</Parameter>
      <Parameter name="outputThreshold">0.000001</Parameter>
      <Parameter name="outputFile">parconversion_sensitivity.csv</Parameter>
      <Parameter name="outputJacobi">Y</Parameter>
      <Parameter name="jacobiOutputFile">jacobi.csv</Parameter>
      <Parameter name="jacobiInverseOutputFile">jacobi_inverse.csv</Parameter>
    </Analytic>
\end{minted}

The portfolio used in this example includes zero sensitivities of 
\begin{itemize}
\item Discount and index curves
\item Credit curves
\item Inflation curves
\item CapFloor volatilities
\end{itemize}

ORE reads the raw sensitivites from the csv input file *sensitivityInputFile*. The input file needs to have six  columns, the column names can be user configured. Here is a description of each of the columns:

\begin{enumerate}
\item idColumn : Column with a unique identifier for the trade / nettingset / portfolio.
\item riskFactorColumn: Column with the identifier of the zero/raw sensitiviy. The risk factor name needs to follow the ORE naming convention, e.g. DiscountCurve/EUR/5/1Y (the 6th bucket in EUR discount curve as specified in the sensitivity.xml)\
\item deltaColumn: The raw sensitivity of the trade/nettingset / portfolio with respect to the risk factor
\item currencyColumn: The currency in which the raw sensitivity is expressed, need to be the same as the BaseCurrency in the simulation settings.
\item shiftSizeColumn: The shift size applied to compute the raw sensitivity, need to be consistent to the sensitivity configuration.
\item baseNpvColumn: The base npv of the trade / nettingset / portfolio in currency.
\end{enumerate}

This is followed by the Jacobi transformation that turns "raw" sensitivities  into sensitivities in the par domain (Deposit/FRA/Swap rates, FX Forwards, CC Basis Swap spreads, 
CDS spreads, ZC and YOY Inflation Swap rates, flat Cap/Floor vols). The conversion is controlled by the additional {\tt ParConversion} data blocks 
in {\tt sensitivity.xml} where the assumed par instruments and corresponding conventions are coded, as shown below for three types of discount curves.

\begin{minted}[fontsize=\scriptsize]{xml}
  <DiscountCurves>
  
    <DiscountCurve ccy="EUR">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>2W,1M,3M,6M,9M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y</ShiftTenors>
      <ParConversion>
        <!--DEP, FRA, IRS, OIS, FXF, XBS -->
	<Instruments>OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS</Instruments>
	<SingleCurve>true</SingleCurve>
	<Conventions>
	  <Convention id="OIS">EUR-OIS-CONVENTIONS</Convention>
	</Conventions>
      </ParConversion>
    </DiscountCurve>   
    
    <DiscountCurve ccy="USD">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>2W,1M,3M,6M,9M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y</ShiftTenors>
      <ParConversion>
	<Instruments>FXF,FXF,FXF,FXF,FXF,XBS,XBS,XBS,XBS,XBS,XBS,XBS,XBS,XBS,XBS,XBS</Instruments>
	<SingleCurve>true</SingleCurve>
	<Conventions>
	  <Convention id="XBS">EUR-USD-XCCY-BASIS-CONVENTIONS</Convention>
	  <Convention id="FXF">EUR-USD-FX-CONVENTIONS</Convention>
	</Conventions>
      </ParConversion>

    <DiscountCurve ccy="GBP">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>2W,1M,3M,6M,9M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y</ShiftTenors>
      <ParConversion>
	<Instruments>DEP,DEP,DEP,DEP,DEP,IRS,IRS,IRS,IRS,IRS,IRS,IRS,IRS,IRS,IRS,IRS</Instruments>
	<SingleCurve>true</SingleCurve>
	<Conventions>
	  <Convention id="DEP">GBP-DEPOSIT</Convention>
	  <Convention id="IRS">GBP-6M-SWAP-CONVENTIONS</Convention>
	</Conventions>
      </ParConversion>
    </DiscountCurve>
  
  </DiscountCurves>
\end{minted}

Finally note that par sensitivity analysis requires that the shift tenor grid in the sensitivity data above matches the corresponding grid in the simulation (market) configuration. 
See also section \ref{sec:sensitivity}.


%--------------------------------------------------------------------
\subsection{Custom Trade Fixings}% Example 51
\label{example:51}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_51} demonstrates ORE's capability to use custom trade specific fixings. For OIS and Ibor floating legs one can specify historical fixing on a trade level, see \ref{ss:floatingleg_data}. Those trade level fixings will be only use for the specific trade, all other trades will use the global fixings.

%--------------------------------------------------------------------
\subsection{Scripted Trade}% Example 52
\label{example:52}
%--------------------------------------------------------------------

The scripted trade was added to ORE to gain more flexibility in representing exotic products, with hyprid payoffs across
asset classes, path-dependence, multiple kinds of early termination options. The scripted trade module uses Monte Carlo and
Finite Difference pricing approaches, it is an evolving interface to implement parallel processing with GPUs and a central
interface to implement AD methods in ORE. See the separate documentation in folder Docs/ScriptedTrade for an introduction to trade
representation, scripting language, model and pricing engine configuration. 

\medskip
The example in this folder {\tt Examples/Example\_52} is a basic demonstration of ORE's scripted trade functionality.
In this example we provide a self-contained case that can be run as usual calling

\medskip
\centerline{\tt python run.py}

\medskip

This generates an NPV and cash flow report for the following portfolio
\begin{itemize}
\item Trade 1: Vanilla European Equity Option, represented as standard ORE XML with analytical pricing
\item Trade 2: Same Option as above, represented as ``generic'' scripted trade with scripted payoff embedded into the trade XML,
  pricing via Monte Carlo
\item Trade 3: Same Option as above, same representation, pricing via Finite Differences triggered by a {\tt ProductTag} assigned
  to the script and used in {\tt pricingengine.xml} 
\item Trade 4: Same Option as above, the scripted trade now refers to an ``external'' script in {\tt scriptlibrary.xml},
  MC pricing
\item Trade 4b: Same as trade 4, but ``compact'' scripted trade representation (uncomment trade 4b in {\tt portfolio.xml})
\item Trade 5: Barrier Option with single continuously observed Up \& Out barrier, represented as standard ORE XML with
  analytical pricing
\item Trade 6: Same Barrier Option as above, approximated as generic scripted trade with daily barrier observation
\item Trade 6b: Same Barrier Option as above, approximated as ``compact'' scripted trade with daily barrier observation
  (uncomment trade 6b in {\tt portfolio.xml})
\item Trade 7: Same Barrier Option as above, represented as generic scripted trade with continuously observed barrier,
  i.e. adjusting for the probability of knock-out between daily observations
\item Trade 7b: Same Barrier Option as of above, represented as ``compact'' scripted trade
  (uncomment trade 7b in {\tt portfolio.xml})
\item Trade 8: Equity Accumulator, represented as generic scripted trade with external payoff script
\item Trade 8b: Same Equity Accumulator as above, represented as compact scripted trade with external payoff script
  (uncomment trade 8b in {\tt portfolio.xml})
\end{itemize}

Note:
\begin{itemize}
\item In all cases we use the Black-Scholes model to drive the Equity process.
\item The Barrier Option pricing using the scripted trade deviates noticeably from the analytical pricing when we use daily
  observations (trade 6 and 6b), but matches quite closely when we adjust for the probability of knock-out between observation
  dates (trade 7 and 7b)
\item We are not aware of analytical pricing for the Accumulator product in trade 8 to benchmark against; trade 8 is priced with MC,
  FD pricing of the Accumulator is possible as well but requires a separate payoff script, only in the vanilla European option case
  we can utilize the same script for both MC and FD pricing
\end{itemize}

Though this initial Example\_52 shows only single-asset Equity cases, the scripted trade in its current version is
  significantly more versatile, more examples and scripts to follow.

%--------------------------------------------------------------------
\subsection{Curve Building around Central Bank Meeting Dates}% Example 53
\label{example:53}
%--------------------------------------------------------------------

This example demonstrates the build of a GBP OIS curve using MPC Swaps at the short end.

%--------------------------------------------------------------------
\subsection{Scripted Trade Exposure with AMC: Bermudan Swaption and LPI Swap}% Example 54
\label{example:54}
%--------------------------------------------------------------------

This example demonstrates exposure simulation using AMC for selected scripted trade types
\begin{itemize}
\item Bermudan Swaption
\item LPI Swap
\end{itemize}
Both payoffs are defined in {\tt scriptlibrary.xml} which is referenced in {\tt portfolio.xml}. \\

To enable the AMC processing requires the following highlighted settings in {\tt ore.xml}.

\begin{minted}[fontsize=\scriptsize]{xml}
    <Analytic type="simulation">
      <Parameter name="active">Y</Parameter>
      <!-- Set to Y to trigger AMC processing -->
      <Parameter name="amc">Y</Parameter>
      <Parameter name="simulationConfigFile">simulation.xml</Parameter>
      <Parameter name="pricingEnginesFile">pricingengine.xml</Parameter>
      <!-- Specify a separate pricing engine file for AMC engines -->
      <Parameter name="amcPricingEnginesFile">pricingengine\_amc.xml</Parameter>
      <!-- Specify trade types to be covered by the AMC processing -->
      <Parameter name="amcTradeTypes">ScriptedTrade</Parameter>
      <Parameter name="baseCurrency">EUR</Parameter>
      <Parameter name="storeScenarios">N</Parameter>
      <Parameter name="cubeFile">cube.csv.gz</Parameter>
      <Parameter name="aggregationScenarioDataFileName">scenariodata.csv.gz</Parameter>
      <Parameter name="aggregationScenarioDataDump">scenariodata.csv</Parameter>
    </Analytic>
\end{minted}

Note that ORE can handle a mix of trades covered by AMC simulation and covered by ``classic'' simulation.
The respective NPV cubes are combined before generating results such as exposures or XVAs.

%--------------------------------------------------------------------
\subsection{Scripted Trade Exposure with AMC: Target Redemption Forward}% Example 55
\label{example:55}
%--------------------------------------------------------------------

This example in folder {\tt Examples/Example\_55} demonstrates exposure simulation and XVA for another scripted product, an
FX Target Redemption Forward (TaRF). In contrast to the cases presented above, you won't see
the payoff script library in the Input folder, nor is the script embedded into the trade XML file.
The trade type in this case is {\tt FxTARF} which has its own implementation in OREData/ored/portfolio/tarf.xpp
and a separate trade schema. However, the scipted trade framework is used under the hood, and the payoff
script is embedded into the C++ code in OREData/ored/portfolio/tarf.cpp.

%--------------------------------------------------------------------
\subsection{CVA Sensitivities using AAD}% Example 56
\label{example:56}
%--------------------------------------------------------------------

This example in folder {\tt Examples/Example\_56} demonstrates a prototype CVA sensitivity
calculation applying Adjoint Algorithmic Differentiation (AAD)
to a Swap instrument represented as scripted trade. 

%--------------------------------------------------------------------
\subsection{Base Scenario Analytic}% Example 57
\label{example:57}
%--------------------------------------------------------------------

Demonstration of the {\tt Scenario} analytic which has been added to export the simulation market's base scenario
as a file.

%--------------------------------------------------------------------
\subsection{Historical Simlation VaR Analytic}% Example 58
\label{example:58}
%--------------------------------------------------------------------

This example in folder {\tt Examples/Example\_58} demonstrates a historical simulation VaR calculation
given a portfolio and externally provided ``market scenarios'' covering
one or several historical observation period(s).
The analytic is specified as usual in {\tt ore.xml} with the following parameters:
\begin{itemize}
\item outputFile: csv file name of the resulting VaR report 
%\item breakdown: boolean, if true the VaR report will contain a breakdown by risk class and risk type, otherwise the report shows the portfolio-lvel VaR only.
\item quantiles: comma searated list of quantiles to be reported
\item portfolioFilter (optional): Only trades with {\tt portfolioId} equal to the provided filter name are processed, see {\tt portfolio.xml}; the entire portfolio is processed, if omitted
\item historicalPeriod: comma-separated date list, an even number of ordered dates is required (d1, d2, d3, d4, ...), where each pair (d1-d2, d3-d4, ...) defines the start and end of historical observation periods used
\item mporDays: Number of calendar days between historical scenarios taken from the observation periods in order to compute P\&L effects (typically 1 or 10) 
\item mporCalendar: Calendar applied in the scenario date calculation
\item mporOverlappingPeriods: Boolean, if true we use overlapping periods of length mporDays (t to t + 10 calendate days, t+1 to t+11, t+2 to t+12, ...), otherwise consecutive periods (t to t+10, t+10 to t+20, ...)
\item simulationConfigFile: defines the structure of the simulation market applied in the P\&L calculation, e.g. discount and index curves, yield curve tenor points used, FX pairs etc.
\item historicalScenarioFile: csv file containing the market scenarios for each date in the observation periods defined below; the granularity of the scenarios (e.g. discount and index curves, number of yield curve tenors) needs to match the simulation market definition above; each yield curve tenor scenario is represented as a discount factor 
\end{itemize}

The example is run as usual by calling {\tt python run.py}

%--------------------------------------------------------------------
\subsection{SABR Model for Swaptions and Caps/Floors}% Example 59
\label{example:59}
%--------------------------------------------------------------------

This example in folder {\tt Examples/Example\_59} demonstrates the pricing of a Swaption and a Cap on
volatility surfaces that are interpolated in smile direction using a SABR
model flavour. As usual the example is run by calling {\tt python run.py}

The essential configuration is in {\tt curveconfig.xml} where the
Interpolation (Swaption) resp. StrikeInterpolation (Caps/Floors) allows
the following new SABR types
\begin{itemize}
\item Hagan2002Lognormal
\item Hagan2002Normal
\item Hagan2002NormalZeroBeta
\item Antonov2015FreeBoundaryNormal
\item KienitzLawsonSwaynePde
\item FlochKennedy
\end{itemize}
SABR parameters can be calibrated or have fixed externally provided values
per option tenor and Swap tenor (Swaptions) resp. optionlet (Caps/Floors).

%--------------------------------------------------------------------
\subsection{Overlapping Close-Out Grids}% Example 60
\label{example:60}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_60} demonstrates ORE's capability to handle xVA simulations using American Monte-Carlo with overlapping close-out grids.

%--------------------------------------------------------------------
\subsection{Fast Sensitivities using AAD and GPUs}% Example 61
\label{example:61}
%--------------------------------------------------------------------

This example in folder {\tt Examples/Example\_61} demonstrates alternative ways of speeding up sensitivity
calculations - using AAD or an external compute device.
The test portfolio consists of 
\begin{itemize}
\item Vanilla Equity Option
\item Equity Barrier Option
\item Equity Accumulator
\item Asian Basket Option
\item FX TaRFs
\end{itemize}
The sensitivity analysis is run in four ways, see {\tt run.py},
\begin{itemize}
\item with ``classic'' bump and revalue
\item as above but using the Computation Graph, see {\tt UseCG=true} in {\tt pricingengine\_cg.xml}, which
  is the basis for the following two approaches
\item using AAD, see {\tt pricingengine\_ad.xml} (as in Example 
\item using the external device if available, see {\tt pricingengine\_gpu.xml}
\end{itemize}
to compare sensitivities and performance. In the latter case we have set the external device in
{\tt pricingengine\_gpu.xml} to ``BasicCpu/Default/Default'' which mimics an external device on the CPU.
On a macbook pro (2023) with M2 Max processor, we can also choose  
``OpenCL/Apple/Apple M2 Max'' here (a 38 core GPU).
The Jupyter notebook {\tt ore.ipynb} in this Example\_61 folder also kicks
off these four runs, but adds further commentary and visualises results.
To run this notebook you need to build the Python bindings for release 12
or ``pip install'' ORE v12.

%--------------------------------------------------------------------
\subsection{P\&L and P\&L Explain Analytics}% Example 62
\label{example:62}
%--------------------------------------------------------------------

This example in folder {\tt Examples/Example\_62} demonstrates the P\&L analytic
type on a very simple test portfolio that consists of two
single-leg swaps. The example can be run as usual by calling {\tt python run.py}.
Main output is the P\&L report in {\tt Output/Pnl/pnl.csv} with the following columns
\begin{itemize}
\item TradeId
\item Maturity and MaturityTime
\item StartDate and EndDate of the P\&L period, referred to as t0 and t1 below
\item NPV(t0)
\item NPV(asof=t0; mkt=t1)
\item NPV(asof=t1; mkt=t0)
\item NPV(t1)
\item PeriodCashFlow: Aggregate of trade flows in the period, converted into the P\&L currency below
\item Theta: NPV(asof=t1; mkt=t0) - NPV(t0) + PeriodCashFLow
\item HypotheticalCleanPnL: NPV(asof=t0; mkt=t1) - NPV(t0)
\item CleanPnL: NPV(t1) - NPV(t0) + PeriodCashFlow 
\item DirtyPnL: NPV(t1) - NPV(t0)
\item Currency
\end{itemize}
Moreover we write
\begin{itemize}
\item Four ``flavours'' of NPV reports used here
\item Four related additional results reports
\item Two reports for the market scenarios used in the two ``lagged'' NPV calculations
\end{itemize}

This analytic is work in progress since it is based on the portfolio at t0 only: The t1 portfolio needs
to be taken into account in order to work out the effects of portfolio changes (maturities, new business, trade
changes) in the observation period.

Furthermore, this example runs a second batch (see {\tt Input/ore\_explain.xml}) to explain the
P\&L above in terms of portfolio sensitivities and changes in related market moves. The main output of this
is in {\tt Output/PnlExplain/pnl\_explain.csv}. The PnlExplain analytic contains the Pnl analytic as dependent
analytic, i.e. the PnlExplain analytic is self-sufficient kicking off Pnl calculation internally. 
The only additional piece of input for the explainer run is {\tt sensitivity.xml}.

%--------------------------------------------------------------------
\subsection{Stress Tests in the Par-Rate Domain}% Example 63
\label{example:63}
%--------------------------------------------------------------------

The stress testing framework in ORE has been operated so far in the ``raw'' domain
of zero rate shifts, hazard rate shifts, optionlet volatility shifts etc.
To analyse the impact of market rate shifts (Swap rates, CDS spreads, flat vols), one had to
manipulate the market data input into ORE and re-run the entire ORE process multiple times.

This example in folder {\tt Examples/Example\_63} demonstrates the extended
stress testing framework that can be operated in the ``par'' rate domain instead.

%--------------------------------------------------------------------
\subsection{Formula-based Coupon}% Example 64
\label{example:64}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_64} demonstrates the
formula based leg in the context of Swaps with CMS Spread and
Digital CMS Spread legs, as well as a Bond with a CMS Spread leg.

The formula-based leg can be seen as a predecessor of the more versatile
scripted trade framework. However, the formula leg may be used to
apply a multi-factor Log-normal Swap Rate model instead of the Gaussian
interest rate models currently applied in the scripted trade framework.

%--------------------------------------------------------------------
\subsection{Flexi Swap}% Example 65
\label{example:65}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_65} demonstrates the
FlexiSwap instrument in ORE - in a nutshell an amortizing swap in which one party
has the option to reduce the notional in each period to any value between the current
notional and a specified lower bound for that period.

%--------------------------------------------------------------------
\subsection{Balance Guaranteed Swap}% Example 66
\label{example:66}
%--------------------------------------------------------------------

The example in folder {\tt Examples/Example\_66} demonstrates the
Balance Guaranteed Swap (BGS) instrument, an amortizing Swap with prepayments
that match prepayments in an underlying reference security. In ORE this
BGS is approximated by a FlexiSwap instrument with amorization bounds
given by two ``conditional prepayment rate (CPR)'' levels, e.g. determined by
current and past CPRs or expert judgement.

%--------------------------------------------------------------------
\subsection{XVA Stress Testing}% Example 67
\label{example:67}
%--------------------------------------------------------------------

The example in folder {\\tt Examples/Example\_67} demonstrates the XVA stresstesting with the classical and AMC XVA engine.
The new analytic type \\emph{XVA\_STRESS} utilizes the existing stresstest framework and supports stresstests in both zero and par domain. 
The Stresstest scenarios are given in the same input format as for the regular stresstest. 

To analyse the impact of market rate shifts (Swap rates, CDS spreads, flat vols), one had to
manipulate the market data input into ORE and re-run the entire ORE process multiple times.

The generated outputs are the xva and exposure reports under each scenario.

%--------------------------------------------------------------------
\subsection{XVA Bump \& Revalue Sensitivities}% Example 68
\label{example:68}
%--------------------------------------------------------------------

The example in folder {\\tt Examples/Example\_68} demonstrates the XVA sensitivity analysis with the AMC XVA engine.
The new analytic type \emph{XVA\_SENSITIVITY} applies zero shifts as specified in the sensitivity.xml and 
computes the xva and exposure measures under each shifted market condition.

The aggregation of the results to sensitivites need to handled outside of ORE. 
These external computed sensitivites can be converted to par sensitivities with the 
zero-to-par conversion analytic (see \ref{example:50}).
%--------------------------------------------------------------------
\subsection{Zero Rate Shifts To Par Shifts}% Example 69
\label{example:69}
%--------------------------------------------------------------------

The example in folder {\\tt Examples/Example\_69} demonstrates the conversion of zero shifts to par rate shifts.
ORE applies the zero rate shifts to the zero curves and computes the resulting shifts in the implied fair rate of a given set of par instruments. 
The zero rate shifts are defined as stresstests and the par instruments are defined in the usual sensitivity configuration.

\clearpage
%========================================================
\section{Launchers and Visualisation}\label{sec:visualisation}
%========================================================


\subsection{Jupyter}\label{sec:jupyter}

ORE comes with an experimental Jupyter notebook for launching ORE batches and in particular for drilling into NPV cube
data.  The notebook is located in directory {\tt FrontEnd/Python/Visualization/npvcube}. To launch the notebook, change
to this directory and follow instructions in the {\tt Readme.txt}. In a nutshell, type\footnote{With Mac OS X, you may
  need to set the environment variable {\tt LANG} to {\tt en\_US.UTF-8} before running jupyter, as mentioned in the
  installation section \ref{sec:python}.}

\medskip
\centerline{\tt jupyter notebook}
\medskip

to start the ipython console and open a browser window. From the list of files displayed in the browser then click

\medskip
\centerline{\tt ore\_jupyter\_dashboard.ipynb} 
\medskip

to open the ORE notebook. The notebook offers
\begin{itemize}
\item launching an ORE job
\item selecting an NPV cube file and netting sets or trades therein
\item plotting a 3d exposure probability density surface
\item viewing exposure probability density function at a selected future time
\item viewing expected exposure evolution through time  
\end{itemize}

The cube file loaded here by default when processing all cells of the notebook (without changing it or launching a ORE
batch) is taken from {\tt Example\_7} (FX Forwards and FX Options).

%\todo[inline]{Add Jupyter section}

\subsection{Calc}\label{sec:calc}

ORE comes with a simple LibreOffice Calc \cite{LO} sheet as an ORE launcher and basic result viewer. This is
demonstrated on the example in section \ref{sec:example1}. It is currently based on the stable LibreOffice version 5.0.6
and tested on OS X. \\

To launch Calc, open a terminal, change to directory {\tt Examples/Example\_1}, and run

\medskip
{\centerline{\tt ./launchCalc.sh} }
\medskip

%This will show the blank sheet in figure \ref{fig_14}.
%\begin{figure}[h]
%\begin{center}
%\includegraphics[scale=0.4]{demo_calc_1}
%\end{center}
%\caption{Calc sheet after launching.}
%\label{fig_14}
%\end{figure}
The user can choose a configuration (one of the {\tt ore*.xml} files in Example\_1's subfolder Input) by hitting the
'Select' button. Initially Input/ore.xml is pre-selected. The ORE process is then kicked off by hitting 'Run'. Once
completed, standard ORE reports (NPV, Cashflow, XVA) are loaded into several sheets. Moreover, exposure evolutions can
then be viewed by hitting 'View' which shows the result in figure \ref{fig_16}.  \\
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{demo_calc_2}
\end{center}
\caption{Calc sheet after hitting 'Run'.}
\label{fig_16}
\end{figure}

This demo uses simple Libre Office Basic macros which call Python scripts to execute ORE. The Libre Office Python uno
module (which comes with Libre Office) is used to communicate between Python and Calc to upload results into the sheets.

%\todo[inline]{Remove hard-coded file names from Python scripts}
%\todo[inline]{Calc example on Windows and Linux} 
%\todo[inline]{Harmonise layout with Excel launcher} 

\subsection{Excel}\label{sec:excel}

ORE also comes with a basic Excel sheet to demonstrate launching ORE and presenting results in Excel. This demo is more
self-contained than the Calc demo in the previous section, as it uses VBA only rather than calls to external Python
scripts. The Excel demo is available in Example\_1. Launch {\tt Example\_1.xlsm}. Then modify the paths on the first
sheet, and kick off the ORE process.


%========================================================
\section{Parameterisation}\label{sec:configuration}
%========================================================

A run of ORE is kicked off with a single command line parameter 

\medskip
\centerline{\tt ore[.exe] ore.xml}
\medskip

which points to the 'master input file' referred to  as {\tt ore.xml} subsequently. 
This file is the starting point of the engine's configuration explained in the following sub section.
An overview of all input configuration files respectively all output files is shown in Table \ref{tab_1} respectively Table \ref{tab_2}.
To set up your own ORE configuration, it might be not be necessary to start from scratch, but instead use any of the examples discussed in section \ref{sec:examples} as a boilerplate and just change the folders, see section \ref{sec:master_input}, and the trade data, see section \ref{sec:portfolio_data}, together with the netting definitions, see section \ref{sec:nettingsetinput}.

\subsection{Master Input File: {\tt ore.xml}}\label{sec:master_input}

The master input file contains general setup information (paths to configuration, trade data and market data), as well
as the selection and configuration of analytics. The file has an opening and closing root element {\tt <ORE>}, {\tt
  </ORE>} with three sections
\begin{itemize}
\item Setup
\item Logging
\item Markets
\item Analytics
\end{itemize}
which we will explain in the following.

\subsubsection{Setup}

This subset of data is easiest explained using an example, see listing \ref{lst:ore_setup}.
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Setup>
  <Parameter name="asofDate">2016-02-05</Parameter>
  <Parameter name="inputPath">Input</Parameter>
  <Parameter name="outputPath">Output</Parameter>
  <Parameter name="logFile">log.txt</Parameter>
  <Parameter name="logMask">255</Parameter>
  <Parameter name="marketDataFile">../../Input/market_20160205.txt</Parameter>
  <Parameter name="fixingDataFile">../../Input/fixings_20160205.txt</Parameter>
  <Parameter name="dividendDataFile">../../Input/dividends_20160205.txt</Parameter> <!-- Optional -->
  <Parameter name="implyTodaysFixings">Y</Parameter>
  <Parameter name="curveConfigFile">../../Input/curveconfig.xml</Parameter>
  <Parameter name="conventionsFile">../../Input/conventions.xml</Parameter>
  <Parameter name="marketConfigFile">../../Input/todaysmarket.xml</Parameter>
  <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
  <Parameter name="portfolioFile">portfolio.xml</Parameter>
  <Parameter name="calendarAdjustment">../../Input/calendaradjustment.xml</Parameter>
  <Parameter name="currencyConfiguration">../../Input/currencies.xml</Parameter>
  <Parameter name="referenceDataFile">../../Input/referencedata.xml</Parameter>
  <Parameter name="iborFallbackConfig">../../Input/iborFallbackConfig.xml</Parameter>
  <!-- None, Unregister, Defer or Disable -->
  <Parameter name="observationModel">Disable</Parameter>
  <Parameter name="lazyMarketBuilding">false</Parameter>
  <Parameter name="continueOnError">false</Parameter>
  <Parameter name="buildFailedTrades">true</Parameter>
  <Parameter name="nThreads">4</Parameter>
</Setup>
\end{minted}
%\hrule
\caption{ORE setup example}
\label{lst:ore_setup}
\end{listing}

Parameter names are self explanatory: Input and output path are interpreted relative from the directory where the ORE
executable is executed, but can also be specified using absolute paths. All file names are then interpreted relative to the
'inputPath' and 'outputPath', respectively. The files starting with {\tt ../../Input/} then point to files in the global
Example input directory {\tt Example/Input/*}, whereas files such as {\tt portfolio.xml} are local inputs in {\tt 
Example/Example\_\#/Input/}. 

Parameter {\tt logMask} determines the verbosity of log file output. Log messages are 
internally labelled as Alert, Critical, Error, Warning, Notice, Debug, associated with logMask values 1, 2, 4, 8, ..., 64. 
The logMask allows filtering subsets of these categories and controlling the verbosity of log file output\footnote{by bitwise comparison of the external logMask value with each message's log level}. LogMask 255 ensures maximum verbosity. \\

When ORE starts, it will initialise today's market, i.e. load market data, fixings and dividends, and build all term
structures as specified in {\tt todaysmarket.xml}.  Moreover, ORE will load the trades in {\tt portfolio.xml} and link
them with pricing engines as specified in {\tt pricingengine.xml}. When parameter {\tt implyTodaysFixings} is set to Y,
today's fixings would not be loaded but implied, relevant when pricing/bootstrapping off hypothetical market data as
e.g. in scenario analysis and stress testing. The curveConfigFile {\tt curveconfig.xml}, the conventionsFile {\tt
  conventions.xml}, the referenceDataFile {\tt referencedata.xml}, the iborFallbackConfig, the marketDataFile and the
fixingDataFile are explained in the sections below.

\medskip Parameter {\tt calendarAdjustment} includes the {\tt calendarAdjustment.xml} which lists out additional holidays and
business days to be added to specified calendars.

\medskip The optional parameter {\tt currencyConfiguration} points to a configuration file that contains additional currencies
to be added to ORE's setup, see {\tt Examples/Input/currencies.xml} for a full list of ISO currencies and a few unofficial currency
codes that can thus be made available in ORE. Note that the external configuration does not override any currencies that are
hard-coded in the QuantLib/QuantExt libraries, only currencies not present already are added from the external configuration file.

\medskip The last parameter {\tt observationModel} can be used to control ORE performance during simulation. The choices
{\em Disable } and {\em Unregister } yield similarly improved performance relative to choice {\em None}. For users
familiar with the QuantLib design - the parameter controls to which extent {\em QuantLib observer notifications} are
used when market and fixing data is updated and the evaluation date is shifted:
\begin{itemize}
\item The 'Unregister' option limits the amount of notifications by unregistering floating rate coupons from indices;
\item Option 'Defer' disables all notifications during market data and fixing updates with
{\tt ObservableSettings::instance().disableUpdates(true)}
and kicks off updates afterwards when enabled again
\item The 'Disable' option goes one step further and disables all notifications during market data and fixing updates,
  and in particular when the evaluation date is changed along a path, with \\
  {\tt ObservableSettings::instance().disableUpdates(false)} \\
  Updates are not deferred here. Required term structure and instrument recalculations are triggered explicitly.
\end{itemize}
%\todo[inline]{Expand the technical description of observationModel}

\medskip If the parameter {\tt lazyMarketBuilding} is set to true, the build of the curves in the TodaysMarket is
delayed until they are actually requested. This can speed up the processing when some curves configured in TodaysMarket
are not used. If not given, the parameter defaults to {\tt true}.

\medskip If the parameter {\tt continueOnError} is set to true, the application will not exit on an error, but try to
continue the processing. If not given, the parameter defaults to {\tt false}.

\medskip If the parameter {\tt buildFailedTrades} is set to true, the application will build a dummy trade if loading or
building the original trade fails. The dummy trade has trade type ``Failed'', zero notional and NPV.
If not given, the parameter defaults to {\tt false}.

\medskip If the parameter {\tt nThreads} is given, multiple threads will be used for valuation engine runs where
applicable (Sensitivity, Exposure Classic, Exposure AMC). If not given, the parameter defaults to $1$.

\subsubsection{Logging}\label{sec:master_input_logging}

The {\tt Logging} section (see listing \ref{lst:ore_logging}) is used to configure some ORE logging options.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Logging>
  <Parameter name="logFile">log.txt</Parameter>
  <Parameter name="logMask">31</Parameter>
  <Parameter name="progressLogFile">my_log_progress_%N.json</Parameter>
  <Parameter name="progressLogRotationSize">102400</Parameter>
  <Parameter name="progressLogToConsole">false</Parameter>
  <Parameter name="structuredLogFile">my_structured_logs_%N.txt</Parameter>
  <Parameter name="structuredLogRotationSize">102400</Parameter>
</Logging>
\end{minted}
%\hrule
\caption{ORE logging}
\label{lst:ore_logging}
\end{listing}

Parameter {\tt logFile} and {\tt logMask} will override the same parameters in the {\tt Setup} section.

Parameters {\tt progressLogFile} and {\tt structuredLogFile} are the filename where progress log messages
and structured log messages are written out to, respectively, which supports Boost string patterns.This defaults to ``log\_progress\_\%N.json'' and ``log\_structured\_\%N.json'', respectively, where {\tt N} will be an integer (beginning at 0) used for log file rotation.

Parameters {\tt progressLogRotationSize} and {\tt structuredLogRotationSize} are the size limit (in bytes)
of each log file before applying log file rotation to the progress log file and structured log message file,
respectively.. For example, $10 * 1024 * 1024 = 10 \text{MiB}$. Defaults to 100 MiB.

If the parameter {\tt progressLogToConsole} is set to true, then progress logs will be written to std::cout.
This can be used simultaneously with {\tt progressLogFile}, i.e.\ progress logs can be written out
to both file and std::cout.

\subsubsection{Markets}\label{sec:master_input_markets}

The {\tt Markets} section (see listing \ref{lst:ore_markets}) is used to choose market configurations for calibrating
the IR, FX and EQ simulation model components, pricing and simulation, respectively. These configurations have to be 
defined
in {\tt todaysmarket.xml} (see section \ref{sec:market}).

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Markets>
  <Parameter name="lgmcalibration">collateral_inccy</Parameter>
  <Parameter name="fxcalibration">collateral_eur</Parameter>
  <Parameter name="eqcalibration">collateral_inccy</Parameter>
  <Parameter name="pricing">collateral_eur</Parameter>
  <Parameter name="simulation">collateral_eur</Parameter>
</Markets>
\end{minted}
%\hrule
\caption{ORE markets}
\label{lst:ore_markets}
\end{listing}

For example, the calibration of the simulation model's interest rate components requires local OIS discounting whereas
the simulation phase requires cross currency adjusted discount curves to get FX product pricing right. So far, the
market configurations are used only to distinguish discount curve sets, but the market configuration concept in ORE
applies to all term structure types.

\subsubsection{Analytics}\label{sec:analytics}

The {\tt Analytics} section lists all permissible analytics using tags {\tt <Analytic type="..."> ... </Analytic>} where
type can be (so far) in
\begin{itemize}
\item npv
\item cashflow
\item curves
\item simulation
\item xva
\item sensitivity
\item stress
\item parStressConversion
\item parametricVar
\item historicalSimulationVar
\item pnl
\item pnlExplain
\item simm
\item imschedule
\item scenario
\end{itemize}

Each {\tt Analytic} section contains a list of key/value pairs to parameterise the analysis of the form {\tt <Parameter
  name="key">value</Parameter>}. Each analysis must have one key {\tt active} set to Y or N to activate/deactivate this
analysis.  The following listing \ref{lst:ore_analytics} shows the parametrisation of the first four basic analytics in
the list above.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>    
  <Analytic type="npv">
    <Parameter name="active">Y</Parameter>
    <Parameter name="baseCurrency">EUR</Parameter>
    <Parameter name="outputFileName">npv.csv</Parameter>
    <Parameter name="additionalResults">Y</Parameter>
    <Parameter name="additionalResultsReportPrecision">6</Parameter>
  </Analytic>      
  <Analytic type="cashflow">
    <Parameter name="active">Y</Parameter>
    <Parameter name="outputFileName">flows.csv</Parameter>
    <Parameter name="includePastCashflows">N</Parameter>
  </Analytic>      
  <Analytic type="curves">
    <Parameter name="active">Y</Parameter>
    <Parameter name="configuration">default</Parameter>
    <Parameter name="grid">240,1M</Parameter>
    <Parameter name="outputFileName">curves.csv</Parameter>
    <Parameter name="outputTodaysMarketCalibration">N</Parameter>
  </Analytic>
  <Analytic type="...">
    <!-- ... -->
  </Analytic>      
</Analytics>      
\end{minted}
\caption{ORE analytics: npv, cashflow, curves, additional results, todays market calibration}
\label{lst:ore_analytics}
\end{listing}

The cashflow analytic writes a report containing all future cashflows of the portfolio. Table \ref{cashflowreport} shows
a typical output for a vanilla swap.

\begin{table}[hbt]
\scriptsize
\begin{center}
  \begin{tabular}{l|l|l|l|r|l|r|r|l|r|r}
\hline
\#ID & Type & LegNo & PayDate & Amount & Currency & Coupon & Accrual & fixingDate & fixingValue & Notional \\
\hline
\hline
tr123 & Swap & 0 & 13/03/17 & -111273.76 & EUR & -0.00201 & 0.50556 & 08/09/16 & -0.00201 & 100000000.00 \\
tr123 & Swap & 0 & 12/09/17 & -120931.71 & EUR & -0.002379 & 0.50833 & 09/03/17 & -0.002381 & 100000000.00 \\
\ldots
\end{tabular}
\caption{Cashflow Report}
\label{cashflowreport}
\end{center}
\end{table}

The amount column contains the projected amount including embedded caps and floors and convexity (if applicable), the
coupon column displays the corresponding rate estimation. The fixing value on the other hand is the plain fixing
projection as given by the forward value, i.e. without embedded caps and floors or convexity.

Note that the fixing value might deviate from the coupon value even for a vanilla coupon, if the QuantLib library was
compiled {\em without} the flag \verb+QL_USE_INDEXED_COUPON+ (which is the default case). In this case the coupon value
uses a par approximation for the forward rate assuming the index estimation period to be identical to the accrual
period, while the fixing value is the actual forward rate for the index estimation period, i.e. whenever the index estimation
period differs from the accrual period the values will be slightly different.

The Notional column contains the underlying notional used to compute the amount of each coupon. It contains \verb+#NA+
if a payment is not a coupon payment.

The curves analytic exports all yield curves that have been built according to the specification in {\tt
  todaysmarket.xml}. Key {\tt configuration} selects the curve set to be used (see explanation in the previous Markets
section).  Key {\tt grid} defines the time grid on which the yield curves are evaluated, in the example above a grid of
240 monthly time steps from today. The discount factors for all curves with configuration default will be exported on
this monthly grid into the csv file specified by key {\tt outputFileName}. The grid can also be specified explicitly by
a comma separated list of tenor points such as {\tt 1W, 1M, 2M, 3M, \dots}.

The additionalResults analytic writes a report containing any additional results generated for the portfolio. The results are pricing engine specific but Table \ref{additionalreport} shows the output for a vanilla swaption.

\begin{table}[hbt]
\scriptsize
\begin{center}
  \begin{tabular}{l|l|l|l}
\hline
\#TradeId & ResultId & ResultType & ResultValue \\
example\_swaption & annuity & double & 2123720984 \\
example\_swaption & atmForward & double & 0.01664135 \\
example\_swaption & spreadCorrection & double & 0 \\
example\_swaption & stdDev & double & 0.00546015 \\
example\_swaption & strike & double & 0.024 \\
example\_swaption & swapLength & double & 4 \\
example\_swaption & vega & double & 309237709.5 \\
\hline
\hline
\ldots
\end{tabular}
\caption{AdditionalResults Report}
\label{additionalreport}
\end{center}
\end{table}

The todaysMarketCalibration analytic writes a report containing information on the build of the t0 market.

\medskip The purpose of the {\tt simulation} 'analytics' is to run a Monte Carlo simulation which evolves the market as
specified in the simulation config file. The primary result is an NPV cube file, i.e. valuations of all trades in the
portfolio file (see section Setup), for all future points in time on the simulation grid and for all paths. Apart from
the NPV cube, additional scenario data (such as simulated overnight rates etc) are stored in this process which are
needed for subsequent XVA analytics.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
  <Analytic type="simulation">
    <Parameter name="active">Y</Parameter>
    <Parameter name="amc">Y</Parameter>
    <Parameter name="amcCg">Y</Parameter>
    <Parameter name="xvaCgSensitivityConfigFile">xvasensiconfig.xml</Parameter>
    <Parameter name="amcTradeTypes">Swap</Parameter>
    <Parameter name="amcPricingEnginesFile">pricingengine_amc.xml</Parameter> -> not documented
    <Parameter name="simulationConfigFile">simulation.xml</Parameter>
    <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
    <Parameter name="baseCurrency">EUR</Parameter>
    <Parameter name="storeFlows">Y</Parameter>
    <Parameter name="storeSurvivalProbabilities">Y</Parameter>
    <Parameter name="salvageCorrelationMatrix">true</Parameter>
    <Parameter name="scenariodump">scenariodump.csv</Parameter>
    <Parameter name="aggregationScenarioDataFileName">scenariodata.csv.gz</Parameter>
    <Parameter name="storeCreditStateNPVs">8</Parameter>
    <Parameter name="cubeFile">cube_A.csv.gz</Parameter>
  </Analytic>
</Analytics>      
\end{minted}
\caption{ORE analytic: simulation}
\label{lst:ore_simulation}
\end{listing}

The pricing engines file specifies how trades are priced under future scenarios which can differ from pricing as of
today (specified in section Setup).  Key base currency determines into which currency all NPVs will be converted. Key
store scenarios (Y or N) determines whether the market scenarios are written to a file for later reuse. Key
`store flows' (Y or N) controls whether cumulative cash flows between simulation dates are stored in the (hyper-)
cube for post processing in the context of Dynamic Initial Margin and Variation Margin calculations. And finally, the
key `store survival probabilities' (Y or N) controls whether survival probabilities on simulation dates are stored in the
cube for post processing in the context of Dynamic Credit XVA calculation. The additional
scenario data (written to the specified file here) is likewise required in the post processor step. These data comprise
simulated index fixing e.g. for collateral compounding and simulated FX rates for cash collateral conversion into base
currency. The scenario dump file, if specified here, causes ORE to write simulated market data to a human-readable csv
file. Only those currencies or indices are written here that are stated in the AggregationScenarioDataCurrencies and 
AggregationScenarioDataIndices subsections of the simulation files market section, see also section
\ref{sec:sim_market}.
 
\medskip The XVA analytic section offers CVA, DVA, FVA and COLVA calculations which can be selected/deselected here
individually. All XVA calculations depend on a previously generated NPV cube (see above) which is referenced here via
the {\tt cubeFile} parameter. This means one can re-run the XVA analytics without regenerating the cube each time. The
XVA reports depend in particular on the settings in the {\tt csaFile} which determines CSA details such as margining
frequency, collateral thresholds, minimum transfer amounts, margin period of risk. By splitting the processing into
pre-processing (cube generation) and post-processing (aggregation and XVA analysis) it is possible to vary these CSA
details and analyse their impact on XVAs quickly without re-generating the NPV cube. The cube file is usually a
compressed csv file (using gzip compression, with file ending .csv.gz), except when the file extension is set explicitly
to txt or csv in which case an uncompressed version of the file is written to disk.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
  <Analytic type="xva">
    <Parameter name="active">Y</Parameter>
    <Parameter name="csaFile">netting.xml</Parameter>
    <Parameter name="cubeFile">cube.csv.gz</Parameter>
    <Parameter name="scenarioFile">scenariodata.csv.gz</Parameter>
    <Parameter name="collateralBalancesFile">collateralbalances.xml</Parameter>
    <Parameter name="baseCurrency">EUR</Parameter>
    <Parameter name="exposureProfiles">Y</Parameter>
    <Parameter name="exposureProfilesByTrade">Y</Parameter>
    <Parameter name="quantile">0.95</Parameter>
    <Parameter name="calculationType">NoLag</Parameter>      
    <Parameter name="allocationMethod">None</Parameter>    
    <Parameter name="marginalAllocationLimit">1.0</Parameter>
    <Parameter name="exerciseNextBreak">N</Parameter>
    <Parameter name="cva">Y</Parameter>
    <Parameter name="dva">N</Parameter>
    <Parameter name="dvaName">BANK</Parameter>
    <Parameter name="fva">N</Parameter>
    <Parameter name="fvaBorrowingCurve">BANK_EUR_BORROW</Parameter>
    <Parameter name="fvaLendingCurve">BANK_EUR_LEND</Parameter>
    <Parameter name="colva">Y</Parameter>
    <Parameter name="collateralFloor">Y</Parameter>
    <Parameter name="dynamicCredit">N</Parameter>
    <Parameter name="kva">Y</Parameter>
    <Parameter name="kvaCapitalDiscountRate">0.10</Parameter>
    <Parameter name="kvaAlpha">1.4</Parameter>
    <Parameter name="kvaRegAdjustment">12.5</Parameter>
    <Parameter name="kvaCapitalHurdle">0.012</Parameter>
    <Parameter name="kvaOurPdFloor">0.03</Parameter>
    <Parameter name="kvaTheirPdFloor">0.03</Parameter>
    <Parameter name="kvaOurCvaRiskWeight">0.005</Parameter>
    <Parameter name="kvaTheirCvaRiskWeight">0.05</Parameter>
    <Parameter name="dim">Y</Parameter>
    <Parameter name="dimModel">Regression</Parameter>
    <Parameter name="mva">Y</Parameter>
    <Parameter name="dimQuantile">0.99</Parameter>
    <Parameter name="dimHorizonCalendarDays">14</Parameter>
    <Parameter name="dimRegressionOrder">1</Parameter>
    <Parameter name="dimRegressors">EUR-EURIBOR-3M,USD-LIBOR-3M,USD</Parameter>
    <Parameter name="dimLocalRegressionEvaluations">100</Parameter>
    <Parameter name="dimLocalRegressionBandwidth">0.25</Parameter>
    <Parameter name="dimScaling">1.0</Parameter>
    <Parameter name="dimEvolutionFile">dim_evolution.txt</Parameter>
    <Parameter name="dimRegressionFiles">dim_regression.txt</Parameter>
    <Parameter name="dimOutputNettingSet">CPTY_A</Parameter>      
    <Parameter name="dimOutputGridPoints">0</Parameter>
    <Parameter name="rawCubeOutputFile">rawcube.csv</Parameter>
    <Parameter name="netCubeOutputFile">netcube.csv</Parameter>
    <Parameter name="nettingSetCubeFile">nettingSetCube_A.csv.gz</Parameter>
    <Parameter name="cptyCubeFile">cptyCube_A.csv.gz</Parameter>
    <Parameter name="fullInitialCollateralisation">true</Parameter>
    <Parameter name="flipViewXVA">N</Parameter>
    <Parameter name="flipViewBorrowingCurvePostfix">_BORROW</Parameter>
    <Parameter name="flipViewLendingCurvePostfix">_LEND</Parameter>
    <Parameter name="mporCashFlowMode">NonePay</Parameter>
  </Analytic>
</Analytics>
\end{minted}
\caption{ORE analytic: xva}
\label{lst:ore_xva}
\end{listing}

Parameters:
\begin{itemize}
\item {\tt csaFile:} Netting set definitions file covering CSA details such as margining frequency, thresholds, minimum
transfer amounts, margin period of risk
\item {\tt cubeFile:} NPV cube file previously generated and to be post-processed here
\item {\tt scenarioFile:} Scenario data previously generated and used in the post-processor (simulated index fixings and
FX rates)
\item {\tt collateralBalancesFile:} References an xml file that contains current VM and IM balances by netting set
\item {\tt baseCurrency:} Expression currency for all NPVs, value adjustments, exposures
\item {\tt exposureProfiles:} Flag to enable/disable exposure output for each netting set
\item {\tt exposureProfilesByTrade:} Flag to enable/disable stand-alone exposure output for each trade
\item {\tt quantile:} Confidence level for Potential Future Exposure (PFE) reporting
\item {\tt calculationType:} Determines the settlement of margin calls. The admissible choices depend on having a close-out grid, see table \ref{tab:calcTypes}; \\
	\begin{itemize}
		\item if there isn't any ``close-out'' grid -see section \ref{sec:simulation}-, the choices are:
		\begin{itemize}
			\item {\em Symmetric} - margin for both counterparties settled after the margin period of risk;
			\item {\em AsymmetricCVA} - margin requested from the counterparty settles with delay,
			margin requested from us settles immediately;
			\item {\em AsymmetricDVA} - vice versa.
		\end{itemize}
		\item If there is a ``close-out'' grid -see section \ref{sec:simulation}-, only choice is:
		\begin{itemize}
			\item {\em NoLag} - used to disable any delayed settlement of the margin. 
		\end{itemize}
	\end{itemize}
	\todo[inline]{Move calculationType into the {\tt csaFile}?}
%
NoLag is the default configuration.
%
\begin{table}[!h]
\centering
\arrayrulecolor{black}
\begin{tabular}{!{\color{black}\vrule}c!{\color{black}\vrule}c!{\color{black}\vrule}l!{\color{black}\vrule}} 
\hline
\multicolumn{1}{!{\color{black}\vrule}l!{\color{black}\vrule}}{Grid Type} & \multicolumn{1}{l!{\color{black}\vrule}}{{\tt calculationType}} & Comment                                                                                                                                                                                           \\ 
\hline
\multirow{4}{*}{without close-out grid}                                    & {\em NoLag}                                                      & Not Supported                                                                                                                                                                                     \\ 
\cline{2-3}
                                                                          & {\em Symmetric}                                                  & Supported\tablefootnote{\label{note1} The calculations are correct only if the simulation grid (see section \ref{sec:simulation}) is equally-spaced with time steps that match the MPoR defined in netting-set definition (see section \ref{sec:CollNettingSet}). See section \ref{sec:mpor} for further explanation.}  \\ 
\cline{2-3}
                                                                          & {\em AsymmetricCVA}                                              & Supported \footref{note1} \\ 
\cline{2-3}
                                                                          & {\em AsymmetricDVA}                                              & Supported \footref{note1}\\ 
\hline
\multirow{4}{*}{with close-out grid}                                       & {\em NoLag}                                                      & Supported\tablefootnote{Close-out lag (see section \ref{sec:simulation}) must be equal to MPoR defined in netting-set definition (see section \ref{sec:CollNettingSet}). Otherwise, an error will be thrown.}                                           \\ 
\cline{2-3}
                                                                          & {\em Symmetric}                                                  & Not Supported                                                                                                                                                                                     \\ 
\cline{2-3}
                                                                          & {\em AsymmetricCVA}                                              & Not Supported                                                                                                                                                                                     \\ 
\cline{2-3}
                                                                          & {\em AsymmetricDVA}                                              & Not Supported                                                                                                                                                                                     \\
\hline
\end{tabular}
\arrayrulecolor{black}
\caption{Overview of admissible calculation types with combination of grid types.} \label{tab:calcTypes}
\end{table}
%
\item {\tt allocationMethod:} XVA allocation method, choices are {\em None, Marginal, RelativeXVA, RelativeFairValueGross, RelativeFairValueNet}
\item {\tt marginalAllocationLimit:} The marginal allocation method a la Pykhtin/Rosen breaks down when the netting set
value vanishes while the exposure does not. This parameter acts as a cutoff for the marginal allocation when the
absolute netting set value falls below this limit and switches to equal distribution of the exposure in this case.
\item {\tt exerciseNextBreak:} Flag to terminate all trades at their next break date before aggregation and the
subsequent analytics
\item {\tt cva, dva, fva, colva, collateralFloor, dim, mva:} Flags to enable/disable these analytics. \todo[inline]{Add
  collateral rates floor to the collateral model file (netting.xml)}
\item {\tt dimModel:} Type of dynamic initial margin model to be applied -- {\em Regression} or {\em Flat}. {\em Regression} is applied by default
  when the dimModel node is omitted (see appendix and further settings related to the regression DIM model below); {\em Flat} means a simple flat
  projection of todays's IM amount on each path (this requires providing today's IM using the {\tt collateralBalancesFile} parameter, see above)
\item {\tt dvaName:} Credit name to look up the own default probability curve and recovery rate for DVA calculation
\item {\tt fvaBorrowingCurve:} Identifier of the borrowing yield curve
\item {\tt fvaLendingCurve:} Identifier of the lending yield curve
%\item {\tt collateralSpread:} Deviation between collateral rate and overnight rate, expressed in absolute terms (one
%basis point is 0.0001) assuming the day count convention of the
%collateral rate. 
%basis point is 0.0001) assuming the day count convention of the collateral rate.
\item {\tt dynamicCredit:} Flag to enable using pathwise survival probabilities when calculating CVA, DVA, FVA and MVA increments from exposures. If set to N the survival probabilities are extracted from T0 curves.
\item {\tt kva:} Flag to enable setting the kva ccr parameters.
\item {\tt kvaCapitalDiscountRate, kvaAlpha, kvaRegAdjustment, kvaCapitalHurdle, kvaOurPdFloor, kvaTheirPdFloor kvaOurCvaRiskWeight, kvaTheirCvaRiskWeight:} the kva CCR parameters (see \ref{sec:app_kva} and \ref{sec:app_kva_cva}.
\item {\tt dimQuantile:} Quantile for Dynamic Initial Margin (DIM) calculation
\item {\tt dimHorizonCalendarDays:} Horizon for DIM calculation, 14 calendar days for 2 weeks, etc.
\item {\tt dimRegressionOrder:} Order of the regression polynomial (netting set clean NPV move over the simulation
period versus netting set NPV at period start)
\item {\tt dimRegressors:} Variables used as regressors in a single- or multi-dimensional regression; these variable
  names need to match entries in the {\tt simulation.xml}'s AggregationScenarioDataCurrencies and
  AggregationScenarioDataIndices sections (only these scenario data are passed on to the post processor); if the list is
  empty, the NPV will be used as a single regressor
\item {\tt dimLocalRegressionEvaluations:} Nadaraya-Watson local regression evaluated at the given number of points to
validate polynomial regression. Note that Nadaraya-Watson needs a large number of samples for meaningful
results. Evaluating the local regression at many points (samples) has a significant performance impact, hence the option
here to limit the number of evaluations.
\item {\tt dimLocalRegressionBandwidth:} Nadaraya-Watson local regression bandwidth in standard deviations of the
independent variable (NPV)
\item {\tt dimScaling:} Scaling factor applied to all DIM values used, e.g. to reconcile simulated DIM with actual IM at
$t_0$
\item {\tt dimEvolutionFile:} Output file name to store the evolution of zero order DIM and average of nth order DIM
through time
\item {\tt dimRegressionFiles:} Output file name(s) for a DIM regression snapshot, comma separated list
\item {\tt dimOutputNettingSet:} Netting set for the DIM regression snapshot
\item {\tt dimOutputGridPoints:} Grid point(s) (in time) for the DIM regression snapshot, comma separated list
\item {\tt rawCubeOutputFile:} File name for the trade NPV cube in human readable csv file format (per trade, date,
sample), leave empty to skip generation of this file.
\item {\tt netCubeOutputFile:} File name for the aggregated NPV cube in human readable csv file format (per netting set,
date, sample) {\em after} taking collateral into account. Leave empty to skip generation of this file.
\item {\tt fullInitialCollateralisation:} If set to {\tt true}, then for every netting set, the collateral balance at $t=0$ will be set to the NPV of the setting set. The resulting effect is that EPE, ENE and PFE are all zero at $t=0$. If set to {\tt false} (default value), then the collateral balance at $t=0$ will be set to zero.
\item {\tt flipViewXVA:} If set to {\tt Y}, the perspective in XVA calculations is switched to the cpty view, the npvs and the netting sets being reverted during calculation. In order to get the lending/borrowing curve, the calculation assumes these curves being set up with the cptyname + the postfix given in the next two settings.
\item {\tt flipViewBorrowingCurvePostfix:} postfix for the borrowing curve, the calculation assumes this is curves being set up with cptyname + postfix given.
\item {\tt flipViewLendingCurvePostfix:} postfix for the lending curve, the calculation assumes this is curve being set up with cptyname + postfix given.
\item {\tt mporCashFlowMode:} Assumption about payment of cashflows within mpor period. One of NonePay, BothPay, WePay,
  TheyPay, Unspecified. Defaults to Unspecified, in this case PP will assume NonePay if mpor sticky date is used,
  otherwise to BothPay.
\end{itemize}

The two cube file outputs {\tt rawCubeOutputFile} and {\tt netCubeOutputFile} are provided for interactive analysis and visualisation purposes, see section
\ref{sec:visualisation}.

\medskip The {\tt sensitivity} and {\tt stress} 'analytics' provide computation of bump and revalue (zero rate
resp. optionlet) sensitivities and NPV changes under user defined stress scenarios. Listing \ref{lst:ore_sensitivity}
shows a typical configuration for sensitivity calculation.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
 <Analytic type="sensitivity">
   <Parameter name="active">Y</Parameter>
   <Parameter name="marketConfigFile">simulation.xml</Parameter>
   <Parameter name="sensitivityConfigFile">sensitivity.xml</Parameter>
   <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
   <Parameter name="scenarioOutputFile">scenario.csv</Parameter>
   <Parameter name="sensitivityOutputFile">sensitivity.csv</Parameter>
   <Parameter name="crossGammaOutputFile">crossgamma.csv</Parameter>
   <Parameter name="outputSensitivityThreshold">0.000001</Parameter>
   <Parameter name="recalibrateModels">Y</Parameter>
   <!-- Additional parametrisation for par sensitivity analysis -->
   <Parameter name="parSensitivity">Y</Parameter>
   <Parameter name="parSensitivityOutputFile">parsensitivity.csv</Parameter>
   <Parameter name="outputJacobi">Y</Parameter>
   <Parameter name="jacobiOutputFile">jacobi.csv</Parameter>
   <Parameter name="jacobiInverseOutputFile">jacobi_inverse.csv</Parameter>
 </Analytic>
</Analytics>
\end{minted}
\caption{ORE analytic: sensitivity}
\label{lst:ore_sensitivity}
\end{listing}
%   <Parameter name="parRateSensitivityOutputFile">parsensi.csv</Parameter>

The parameters have the following interpretation:

\begin{itemize}
\item {\tt marketConfigFile:} Configuration file defining the simulation market under which sensitivities are computed,
  see \ref{sec:simulation}. Only a subset of the specification is needed (the one given under {\tt Market}, see
  \ref{sec:sim_market} for a detailed description).
\item {\tt sensitivityConfigFile:} Configuration file  for the sensitivity calculation, see section \ref{sec:sensitivity}.
\item {\tt pricingEnginesFile:} Configuration file for the pricing engines to be used for sensitivity calculation.
\item {\tt scenarioOutputFile:} File containing the results of the sensitivity calculation in terms of the base scenario
  NPV, the scenario NPV and their difference.
\item {\tt sensitivityOutputFile:} File containing the results of the sensitivity calculation in terms of the base scenario
  NPV, the shift size together with the risk-factor and the resulting first and (pure) second order finite differences.
  Also included is a second set of shift sizes together with the risk-factor with a (mixed) second order finite difference associated to a cross gamma calculation
%\item {\tt parRateSensitivityOutputFile:} File containing par sensitivities (only available in ORE+)
\item {\tt outputSensitivityThreshold:} Only finite differences with absolute value greater than this number are written
  to the output files.
\item {\tt recalibrateModels:} If set to Y, then recalibrate pricing models after each shift of relevant term structures; otherwise do not recalibrate
\item {\tt parSensitivity}: If set to Y, par sensitivity analysis is performed following the "raw" sensitivity analysis; note that in this case the 
{\tt sensitivityConfigFile} needs to contain {\tt ParConversion} sections, see {\tt Example\_40}   
\item {\tt parSensitivityOutputFile}: Output file name for the par sensitivity report
\item {\tt outputJacobi}: If set to Y, then the relevant Jacobi and inverse Jacobi matrix is written to a file, see below
\item {\tt jacobiOutputFile}: Output file name for the Jacobi matrx
\item {\tt jacobiInverseOutputFile}: Output file name for the inverse Jacobi matrix
\end{itemize}


The zero to par sensitivity conversion analytics configuration is similar to the one of the sensitivity calculation. Listing \ref{lst:ore_zerotoparconversion}
shows an example.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
 <Analytic type="zeroToParSensiConversion">
      <Parameter name="active">Y</Parameter>
      <Parameter name="marketConfigFile">simulation.xml</Parameter>
      <Parameter name="sensitivityConfigFile">sensitivity.xml</Parameter>
      <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
      <Parameter name="sensitivityInputFile">sensitivity.csv</Parameter>
      <Parameter name="outputThreshold">0.000001</Parameter>
      <Parameter name="outputFile">parconversion_sensitivity.csv</Parameter>
      <Parameter name="outputJacobi">Y</Parameter>
      <Parameter name="jacobiOutputFile">jacobi.csv</Parameter>
      <Parameter name="jacobiInverseOutputFile">jacobi_inverse.csv</Parameter>
    </Analytic>
</Analytics>
\end{minted}
\caption{ORE analytic: Zero to Par Sensitivity Conversion}
\label{lst:ore_zerotoparconversion}
\end{listing}

The parameters have the same interpretation as for the sensitivity analytic. There is one new parameter *sensitivityInputFile* which points to a csv file with the raw (zero)sensitivites. Those raw sensitivites will be converted into par sensitivities, using the same methodology described in \ref{app:par_sensi} and the configuration is described in \ref{sec:sensitivity}.

The raw sensitivites csv input file *sensitivityInputFile* needs to have at least six columns, the column names can be user configured in the master input file. Here is a description of each of the columns:

\begin{enumerate}
\item idColumn : Column with a unique identifier for the trade / nettingset / portfolio.
\item riskFactorColumn: Column with the identifier of the zero/raw sensitiviy. The risk factor name needs to follow the ORE naming convention, e.g. DiscountCurve/EUR/5/1Y (the 6th bucket in EUR discount curve as specified in the sensitivity.xml)\
\item deltaColumn: The raw sensitivity of the trade/nettingset / portfolio with respect to the risk factor
\item currencyColumn: The currency in which the raw sensitivity is expressed, need to be the same as the BaseCurrency in the simulation settings.
\item shiftSizeColumn: The shift size applied to compute the raw sensitivity, need to be consistent to the sensitivity configuration.
\item baseNpvColumn: The base npv of the trade / nettingset / portfolio in currency.
\end{enumerate}

Here is an example for an input file:

\begin{table}[hbt]
\scriptsize
\begin{center}
\begin{tabular}{lllrlrr}
\hline
{} & \#TradeId &                Factor\_1 &  ShiftSize\_1 & Currency &  Base NPV &  Delta \\
\hline
0 &     Swap &  DiscountCurve/EUR/3/6M &       0.0001 &      EUR &   1335.27 &   5.05 \\
1 &     Swap &  DiscountCurve/EUR/4/9M &       0.0001 &      EUR &   1335.27 &   0.35 \\
2 &     Swap &  DiscountCurve/EUR/5/1Y &       0.0001 &      EUR &   1335.27 &  -5.41 \\
3 &     Swap &  DiscountCurve/EUR/6/2Y &       0.0001 &      EUR &   1335.27 &  -0.22 \\
4 &     Swap &  DiscountCurve/EUR/7/3Y &       0.0001 &      EUR &   1335.27 &  -0.32 \\
\hline
\end{tabular}
\end{center}
\end{table}

The stress analytics configuration is similar to the one of the sensitivity calculation. Listing \ref{lst:ore_stress}
shows an example.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
 <Analytic type="stress">
   <Parameter name="active">Y</Parameter>
   <Parameter name="marketConfigFile">simulation.xml</Parameter>
   <Parameter name="stressConfigFile">stresstest.xml</Parameter>
   <Parameter name="pricingEnginesFile">../../Input/pricingengine.xml</Parameter>
   <Parameter name="scenarioOutputFile">stresstest.csv</Parameter>
   <Parameter name="outputThreshold">0.000001</Parameter>
 </Analytic>
</Analytics>
\end{minted}
\caption{ORE analytic: stress}
\label{lst:ore_stress}
\end{listing}

The parameters have the same interpretation as for the sensitivity analytic. The configuration file for the stress
scenarios is described in more detail in section \ref{sec:stress}.

\medskip The {\tt VaR} 'analytics' provide computation of Value-at-Risk measures based on the sensitivity (delta, gamma, cross gamma) data above. Listing \ref{lst:ore_var} shows a configuration example.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
    <Analytic type="parametricVar"> 
      <Parameter name="active">Y</Parameter> 
      <Parameter name="portfolioFilter">PF1|PF2</Parameter>
      <Parameter name="sensitivityInputFile">
         ../Output/sensitivity.csv,../Output/crossgamma.csv
      </Parameter> 
      <Parameter name="covarianceInputFile">covariance.csv</Parameter> 
      <Parameter name="salvageCovarianceMatrix">N</Parameter>
      <Parameter name="quantiles">0.01,0.05,0.95,0.99</Parameter> 
      <Parameter name="breakdown">Y</Parameter> 
      <!-- Delta, DeltaGammaNormal, Cornish-Fisher, Saddlepoint, MonteCarlo --> 
      <Parameter name="method">DeltaGammaNormal</Parameter> 
      <Parameter name="mcSamples">100000</Parameter> 
      <Parameter name="mcSeed">42</Parameter> 
      <Parameter name="outputFile">var.csv</Parameter> 
    </Analytic> 
</Analytics>
\end{minted}
\caption{ORE analytic: VaR}
\label{lst:ore_var}
\end{listing}

The parameters have the following interpretation:

\begin{itemize}
\item {\t portfolioFilter:} Regular expression used to filter the portfolio for which VaR is computed; if the filter is not provided, then the full portfolio is processed
\item {\tt sensitivityInputFile:} Reference to the sensitivity (deltas, vegas, gammas) and cross gamma input as generated by ORE in a comma separated list
\item {\tt covarianceFile:} Reference to the covariances input data; these are currently not calculated in ORE and need to be provided externally, in a blank/tab/comma separated file with three columns (factor1, factor2, covariance), where factor1 and factor2 follow the naming convention used in ORE's sensitivity and cross gamma output files. Covariances need to be consistent with the sensitivity data provided. For example, if sensitivity to factor1 is computed by absolute shifts and expressed in basis points, then the covariances with factor1 need to be based on absolute basis point shifts of factor1; if sensitivity is due to a relative factor1 shift of 1\%, then covariances with factor1 need to be based on relative shifts expressed in percentages to, etc. Also note that covariances are expected to include the desired holding period, i.e. no scaling with square root of time etc is performed in ORE; 
\item {\tt salvageCovarianceMatrix:} If set to Y, turn the input covariance matrix into a valid (positive definite) matrix applying a Salvaging algorithm; if set to N, throw an exception if the matrix is not positive definite
\item {\tt quantiles:} Several desired quantiles can be specified here in a comma separated list; these lead to several columns of results in the output file, see below. Note that e.g. the 1\% quantile corresponds to the lower tail of the P\&L distribution (VaR), 99\% to the upper tail.
\item {\tt breakdown:} If yes, VaR is computed by portfolio, risk class (All, Interest Rate, FX, Inflation, Equity, Credit) and risk type (All, Delta \& Gamma, Vega)
\item {\tt method:} Choices are {\em Delta, DeltaGammaNormal, Cornish-Fisher, Saddlepoint, MonteCarlo}, see appendix \ref{sec:app_var}
\item {\tt mcSamples:} Number of Monte Carlo samples used when the {\em MonteCarlo} method is chosen 
\item {\tt mcSeed:} Random number generator seed when the {\em MonteCarlo} method is chosen
\item {\tt outputFile:} Output file name
\end{itemize}

\medskip The {\tt simm} 'analytic' provides computation of initial margin using ISDA's Standard Initial Margin Model (SIMM) based on sensitivities in the Common Risk Interchange Format (CRIF) defined by ISDA. Listing \ref{lst:ore_simm} shows a configuration example.

\begin{listing}[H]
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
   <Analytic type="simm">
      <Parameter name="active">Y</Parameter>
      <Parameter name="version">2.1</Parameter>
      <Parameter name="crif">crif.csv</Parameter>
      <Parameter name="calculationCurrency">USD</Parameter>
      <Parameter name="resultCurrency">USD</Parameter>
      <Parameter name="enforceIMRegulations">true</Parameter>
      <Parameter name="mporDays">1</Parameter>
      <Parameter name="simmCalibration">simmcalibration.xml</Parameter>
    </Analytic>
<Analytics>
\end{minted}
\caption{ORE analytic: SIMM}
\label{lst:ore_simm}
\end{listing}

The parameters have the following interpretation:

\begin{itemize}
\item {\tt version:} SIMM model version string \\
Allowable values: 1.0, 1.1, 1.2, 1.3, 1.3.38, 2.0, 2.1, 2.2, 2.3, 2.4 (or 2.3.8), 2.5, 2.5A, 2.6 (or 2.5.6) \\
Note that any new SIMM model versions are integrated into ORE with each release, tested against the official ISDA SIMM unit tests.
\item {\tt crif:} Name of the CRIF file to be loaded
\item {\tt calculationCurrency:} Determines the {\tt Risk\_FX} CRIF entry that is ignored in ISDA SIMM calculation \\
Allowable values: See Table \ref{tab:currency} \lstinline!Currency!.
\item {\tt resultCurrency} (optional): Currency for expressing the amounts in the resulting SIMM report, by default set to the calculationCurrency. \\
Allowable values: See Table \ref{tab:currency} \lstinline!Currency!.
\item {\tt enforceIMRegulations} (optional): Whether to take collect/post regulations into account. \\
Allowable values: Allowable boolean values are given in Table \ref{tab:boolean_allowable}. Defaults to \emph{False} if omitted.
\item {\tt mporDays} (optional): Currency for expressing the amounts in the resulting SIMM report, by default set to the calculationCurrency. \\
Allowable values: See Table \ref{tab:currency} \lstinline!Currency!.
\item {\tt simmCalibration} (optional): Name of the SIMM calibration configuration file. See Section \ref{sec:simmcalibration} \lstinline!SIMM Calibration!.
\end{itemize}

The SIMM analytic requires minimal market data input and today's market configuration - FX rates for conversions calculation currency, USD and result currency.

\medskip The {\tt imschedule} 'analytic' provides initial margin calculation using the standard "IM Schedule" method that is based on 
trade NPV, notional, end date and product class. ORE expects the inputs in the form of a CRIF file.
Listing \ref{lst:ore_simm} shows a configuration example.

\begin{listing}[H]
\begin{minted}[fontsize=\footnotesize]{xml}
<Analytics>
   <Analytic type="imschedule">
      <Parameter name="active">Y</Parameter>
      <Parameter name="crif">schedule_crif.csv</Parameter>
      <Parameter name="calculationCurrency">USD</Parameter>
    </Analytic>
<Analytics>
\end{minted}
\caption{ORE analytic: IM Schedule}
\label{lst:ore_schedule}
\end{listing}

The input CRIF file here has two lines per trade, for RiskClass = PV respectively RiskClass Notional. 
Additonally required columns are product class and end date, see example 44. 
The idea is that the inputs can be externally provided. Therefore the IM Schedule analytic does not require a portfolio.
The alternative -- ORE uses a portfolio to compute NPV, imply product class and end date -- has not been implemented yet.

Note that the IM Schedule product class has to be in 
\begin{itemize}
  \item Rates
  \item FX
  \item Equity
  \item Credit
  \item Commodity
\end{itemize}
in contrast to SIMM where we use the combined RatesFX product class.

%--------------------------------------------------------
\subsection{Market: {\tt todaysmarket.xml}}\label{sec:market}
%--------------------------------------------------------

This configuration file determines the subset of the 'market' universe which is going to be built by ORE. It is the
user's responsibility to make sure that this subset is sufficient to cover the portfolio to be analysed. If it is not,
the application will complain at run time and exit.

\medskip We assume that the market configuration is provided in file {\tt todaysmarket.xml}, however, the file name can
be chosen by the user. The file name needs to be entered into the master configuration file {\tt ore.xml}, see section
\ref{sec:master_input}.

\medskip 
The file starts and ends with the opening and closing tags {\tt <TodaysMarket>} 
and {\tt </TodaysMarket>}. The file then contains configuration blocks for
\begin{itemize}
\item Discounting curves
\item Index curves (to project index fixings)
\item Yield curves (for other purposes, e.g. as benchmark curve for bond pricing)
\item Swap index curves (to project Swap rates)
\item FX spot rates
\item Inflation index curves (to project zero or yoy inflation fixings)
\item Equity curves (to project forward prices)
\item Default curves
\item Swaption volatility structures
\item Cap/Floor volatility structures
\item FX volatility structures
\item Inflation Cap/Floor volatility surfaces
\item Equity volatility structures
\item CDS volatility structures
\item Base correlation structures
\item Correlation structures
\item Securities
\end{itemize}

There can be alternative versions of each block each labeled with a unique identifier (e.g. Discount curve block with ID
'default', discount curve block with ID 'ois', another one with ID 'xois', etc). The purpose of these IDs will be
explained at the end of this section. We now discuss each block's layout.

\subsubsection{Discounting Curves} 

We pick one discounting curve block as an example here (see {\tt Examples/Input/todaysmarket.xml}), the one with ID 'ois' 

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <DiscountingCurves id="ois">
    <DiscountingCurve currency="EUR">Yield/EUR/EUR1D</DiscountingCurve>
    <DiscountingCurve currency="USD">Yield/USD/USD1D</DiscountingCurve>
    <DiscountingCurve currency="GBP">Yield/GBP/GBP1D</DiscountingCurve>
    <DiscountingCurve currency="CHF">Yield/CHF/CHF6M</DiscountingCurve>
    <DiscountingCurve currency="JPY">Yield/JPY/JPY6M</DiscountingCurve>
    <!-- ... -->
  </DiscountingCurves>
\end{minted}
\caption{Discount curve block with ID 'ois'}
\label{lst:discountcurve_spec}
\end{listing}

This block instructs ORE to build five discount curves for the indicated currencies. The string within the tags,
e.g. Yield/EUR/EUR1D, uniquely identifies the curve to be built.  Curve Yield/EUR/EUR1D is defined in the curve
configuration file explained in section \ref{sec:curveconfig} below. In this case ORE is instructed to build an Eonia
Swap curve made of Overnight Deposit and Eonia Swap quotes. The right most token of the string Yield/EUR/EUR1D (EUR1D)
is user defined, the first two tokens Yield/EUR have to be used to point to a yield curve in currency EUR.
 
\subsubsection{Index Curves} 

See an excerpt of the index curve block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<IndexForwardingCurves id="default">
  <Index name="EUR-EURIBOR-3M">Yield/EUR/EUR3M</Index>
  <Index name="EUR-EURIBOR-6M">Yield/EUR/EUR6M</Index>
  <Index name="EUR-EURIBOR-12M">Yield/EUR/EUR12M</Index>
  <Index name="EUR-EONIA">Yield/EUR/EUR1D</Index>
  <Index name="USD-LIBOR-3M">Yield/USD/USD3M</Index>
  <!-- ... -->
</IndexForwardingCurves>
\end{minted}
\caption{Index curve block with ID 'default'}
\label{lst:indexcurve_spec}
\end{listing}

This block of curve specifications instructs ORE to build another set of yield curves, unique strings
(e.g. Yield/EUR/EUR6M etc.) point to the {\tt curveconfig.xml} file where these curves are defined. Each curve is then
associated with an index name (of format Ccy-IndexName-Tenor, e.g. EUR-EURIBOR-6M) so that ORE will project the
respective index using the selected curve (e.g. Yield/EUR/EUR6M).

\subsubsection{Yield Curves}

See an excerpt of the yield curve block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<YieldCurves id="default">
  <YieldCurve name="BANK_EUR_LEND">Yield/EUR/BANK_EUR_LEND</YieldCurve>
  <YieldCurve name="BANK_EUR_BORROW">Yield/EUR/BANK_EUR_BORROW</YieldCurve>
  <!-- ... -->
</YieldCurves>
\end{minted}
\caption{Yield curve block with ID 'default'}
\label{lst:yieldcurve_spec}
\end{listing}

This block of curve specifications instructs ORE to build another set of yield curves, unique strings
(e.g. Yield/EUR/EUR6M etc.) point to the {\tt curveconfig.xml} file where these curves are defined. Other than
discounting and index curves the yield curves in this block are not tied to a particular purpose. The curves defined in
this block typically include

\begin{itemize}
\item additional curves needed in the XVA post processor, e.g. for the FVA calculation
\item benchmark curves used for bond pricing
\end{itemize}

\subsubsection{Swap Index Curves}

The following is an excerpt of the swap index curve block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<SwapIndexCurves id="default">
  <SwapIndex name="EUR-CMS-1Y">
    <Index>EUR-EURIBOR-6M</Index>
    <Discounting>EUR-EONIA</Discounting>
  </SwapIndex>
  <SwapIndex name="EUR-CMS-30Y">
    <Index>EUR-EURIBOR-6M</Index>
    <Discounting>EUR-EONIA</Discounting>
  </SwapIndex>
  <!-- ... -->
</SwapIndexCurves>
\end{minted}
\caption{Swap index curve block with ID 'default'}
\label{lst:swapindexcurve_spec}
\end{listing}

These instructions do not build any additional curves. They only build the respective swap index objects and associate
them with the required index forwarding and discounting curves already built above. This enables a swap index to project
the fair rate of forward starting Swaps. Swap indices are also containers for conventions. Swaption volatility surfaces
require two swap indices each available in the market object, a long term and a short term swap index. The curve
configuration file below will show that in particular the required short term index has term 1Y, and the required long
term index has 30Y term. This is why we build these two indices at this point.

\subsubsection{FX Spot}

The following is an excerpt of the FX spot block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<FxSpots id="default">
  <FxSpot pair="EURUSD">FX/EUR/USD</FxSpot>
  <FxSpot pair="EURGBP">FX/EUR/GBP</FxSpot>
  <FxSpot pair="EURCHF">FX/EUR/CHF</FxSpot>
  <FxSpot pair="EURJPY">FX/EUR/JPY</FxSpot>
  <!-- ... -->
</FxSpots>
\end{minted}
\caption{FX spot block with ID 'default'}
\label{lst:fxspot_spec}
\end{listing}

This block instructs ORE to provide four FX quotes, all quoted with target currency EUR so
that foreign currency amounts can be converted into EUR via multiplication with that rate.
 
\subsubsection{FX Volatilities}

The following is an excerpt of the FX Volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<FxVolatilities id="default">
  <FxVolatility pair="EURUSD">FXVolatility/EUR/USD/EURUSD</FxVolatility>
  <FxVolatility pair="EURGBP">FXVolatility/EUR/GBP/EURGBP</FxVolatility>
  <FxVolatility pair="EURCHF">FXVolatility/EUR/CHF/EURCHF</FxVolatility>
  <FxVolatility pair="EURJPY">FXVolatility/EUR/JPY/EURJPY</FxVolatility>
  <!-- ... -->
</FxVolatilities>
\end{minted}
\caption{FX volatility block with ID 'default'}
\label{lst:fxvol_spec}
\end{listing}

This instructs ORE to build four FX volatility structures for all FX pairs with target currency EUR, see curve
configuration file for the definition of the volatility structure.

\subsubsection{Swaption Volatilities}

The following is an excerpt of the Swaption Volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<SwaptionVolatilities id="default">
  <SwaptionVolatility currency="EUR">SwaptionVolatility/EUR/EUR_SW_N</SwaptionVolatility>
  <SwaptionVolatility currency="USD">SwaptionVolatility/USD/USD_SW_N</SwaptionVolatility>
  <SwaptionVolatility currency="GBP">SwaptionVolatility/GBP/GBP_SW_N</SwaptionVolatility>
  <SwaptionVolatility currency="CHF">SwaptionVolatility/CHF/CHF_SW_N</SwaptionVolatility>
  <SwaptionVolatility currency="JPY">SwaptionVolatility/CHF/JPY_SW_N</SwaptionVolatility>
</SwaptionVolatilities>
\end{minted}
\caption{Swaption volatility block with ID 'default'}
\label{lst:swaptionvol_spec}
\end{listing}

This instructs ORE to build five Swaption volatility structures, see the curve configuration file for the definition of
the volatility structure. The latter token (e.g. EUR\_SW\_N) is user defined and will be found in the curve
configuration's CurveId tag.

\subsubsection{Cap/Floor Volatilities}

The following is an excerpt of the Cap/Floor Volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CapFloorVolatilities id="default">
  <CapFloorVolatility currency="EUR">CapFloorVolatility/EUR/EUR_CF_N</CapFloorVolatility>
  <CapFloorVolatility currency="USD">CapFloorVolatility/USD/USD_CF_N</CapFloorVolatility>
  <CapFloorVolatility currency="GBP">CapFloorVolatility/GBP/GBP_CF_N</CapFloorVolatility>
</CapFloorVolatilities>
\end{minted}
\caption{Cap/Floor volatility block with ID 'default'}
\label{lst:capfloorvol_spec}
\end{listing}

This instructs ORE to build three Cap/Floor volatility structures, see the curve configuration file for the definition
of the volatility structure. The latter token (e.g. EUR\_CF\_N) is user defined and will be found in the curve
configuration's CurveId tag.

\subsubsection{Default Curves}

The following is an excerpt of the Default Curves block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<DefaultCurves id="default">
  <DefaultCurve name="BANK">Default/USD/BANK_SR_USD</DefaultCurve>
  <DefaultCurve name="CPTY_A">Default/USD/CUST_A_SR_USD</DefaultCurve>
  <DefaultCurve name="CPTY_B">Default/USD/CUST_A_SR_USD</DefaultCurve>
  <!-- ... -->
</DefaultCurves>
\end{minted}
\caption{Default curves block with ID 'default'}
\label{lst:defaultcurve_spec}
\end{listing}

This instructs ORE to build a set of default probability curves, again defined in the curve configuration file. Each
curve is then associated with a name (BANK, CUST\_A) for subsequent lookup.  As before, the last token
(e.g. BANK\_SR\_USD) is user defined and will be found in the curve configuration's CurveId tag.

\subsubsection{Securities}\label{sssec:securities}

The following is an excerpt of the Security block with ID 'default' from the same example file:

\begin{listing}[H]
	%\hrule\medskip
	\begin{minted}[fontsize=\footnotesize]{xml}
<Securities id="default">
  <Security name="SECURITY_1">Security/SECURITY_1</Security>
</Securities>
	\end{minted}
	\caption{Securities block with ID 'default'}
	\label{lst:secspread_spec}
\end{listing}

The pricing of bonds includes (among other components) a security specific spread and rate. 
This block links a security name to a spread and rate pair defined in the curve configuration file. This name may then be referenced 
as the security id in the bond trade definition.

\subsubsection{Equity Curves}
The following is an excerpt of the Equity curves block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<EquityCurves id="default">
  <EquityCurve name="SP5">Equity/USD/SP5</EquityCurve>
  <EquityCurve name="Lufthansa">Equity/EUR/Lufthansa</EquityCurve>
</EquityCurves>
\end{minted}
\caption{Equity curves block with ID 'default'}
\label{lst:eqcurve_spec}
\end{listing}

This instructs ORE to build a set of equity curves, again defined in the curve configuration file. Each equity curve 
after construction will consist of a spot equity price, as well as a term structure of dividend yields, which can be 
used to determine forward prices. This object is then associated with a name (e.g. SP5) for subsequent lookup. 

\subsubsection{Equity Volatilities}

The following is an excerpt of the equity volatilities block with ID 'default' from the same example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<EquityVolatilities id="default">
  <EquityVolatility name="SP5">EquityVolatility/USD/SP5</EquityVolatility>
  <EquityVolatility name="Lufthansa">EquityVolatility/EUR/Lufthansa</EquityVolatility>
</EquityVolatilities>
\end{minted}
\caption{EQ volatility block with ID 'default'}
\label{lst:eqvol_spec}
\end{listing}

This instructs ORE to build two equity volatility structures for SP5 and Lufthansa, respectively. See the curve
configuration file for the definition of the equity volatility structure.


\subsubsection{Inflation Index Curves}

The following is an excerpt of the Zero Inflation Index Curves block with ID 'default' from the sample example file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<ZeroInflationIndexCurves id="default">
    <ZeroInflationIndexCurve name="EUHICPXT">
        Inflation/EUHICPXT/EUHICPXT_ZC_Swaps
    </ZeroInflationIndexCurve>
    <ZeroInflationIndexCurve name="FRHICP">
        Inflation/FRHICP/FRHICP_ZC_Swaps
    </ZeroInflationIndexCurve>
    <ZeroInflationIndexCurve name="UKRPI">
        Inflation/UKRPI/UKRPI_ZC_Swaps
    </ZeroInflationIndexCurve>
    <ZeroInflationIndexCurve name="USCPI">
        Inflation/USCPI/USCPI_ZC_Swaps
    </ZeroInflationIndexCurve>
    ...
</ZeroInflationIndexCurves>
\end{minted}
\caption{Zero Inflation Index Curves block with ID 'default'}
\label{lst:zeroinflationindexcurve_spec}
\end{listing}

This instructs ORE to build a set of zero inflation index curves, which are defined in the curve configuration
file. Each curve is then associated with an index name (like e.g. EUHICPXT or UKRPI). The last token
(e.g. EUHICPXT\_ZC\_Swap) is user defined and will be found in the curve configuration's CurveId tag.

In a similar way, Year on Year index curves are specified:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <YYInflationIndexCurves id="default">
      <YYInflationIndexCurve name="EUHICPXT">
          Inflation/EUHICPXT/EUHICPXT_YY_Swaps
      </YYInflationIndexCurve>
      ...
  </YYInflationIndexCurves>
\end{minted}
\caption{YoY Inflation Index Curves block with ID 'default'}
\label{lst:yoyinflationindexcurve_spec}
\end{listing}

Note that the index name is the same as in the corresponding zero index curve definition, but the token corresponding to
the CurveId tag is different. This is because the actual underlying index (and in particular its fixings) are shared
between the two index types, while different projection curves are used to forecast future index realisations.

\subsubsection{Inflation Cap/Floor Volatility Surfaces}

The following is an excerpt of the Inflation Cap/Floor Volatility Surfaces blocks with ID 'default' from the sample example
file:

{
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <YYInflationCapFloorVolatilities id="default">
    <YYInflationCapFloorVolatility name="EUHICPXT">
        InflationCapFloorVolatility/EUHICPXT/EUHICPXT_YY_CF
    </InflationCapFloorVolatility>
  </YYInflationCapFloorVolatilities>

  <ZeroInflationCapFloorVolatilities id="default">
    <ZeroInflationCapFloorVolatility name="UKRPI">
        InflationCapFloorVolatility/UKRPI/UKRPI_ZC_CF
    </ZeroInflationCapFloorVolatility>
    <ZeroInflationCapFloorVolatility name="EUHICPXT">
        InflationCapFloorVolatility/EUHICPXT/EUHICPXT_ZC_CF
    </ZeroInflationCapFloorVolatility>
    <ZeroInflationCapFloorVolatility name="USCPI">
        InflationCapFloorVolatility/USCPI/USCPI_ZC_CF
    </ZeroInflationCapFloorVolatility>
  </ZeroInflationCapFloorVolatilities>
\end{minted}
\caption{Inflation Cap/Floor Volatility Surfaces block with ID 'default'}
\label{lst:inflation_cap_floor_surface_spec}
\end{listing}

This instructs ORE to build a set of year-on-year and zero inflation cap floor volatility surfaces, which are defined in the curve
configuration file. Each surface is associated with an index name. The last token (e.g. EUHICPXT\_ZC\_CF) is user defined
and will be found in the curve configuration's CurveId tag.

\subsubsection{CDS Volatility Structures}

CDS volatility structures are configured as follows
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <CDSVolatilities id="default">
   <CDSVolatility name="CDSVOL_A">CDSVolatility/CDXIG</CDSVolatility>
   <CDSVolatility name="CDSVOL_B">CDSVolatility/CDXHY</CDSVolatility>
  </CDSVolatilities>
\end{minted}
\caption{CDS volatility structure block with ID 'default'}
\label{lst:cdsvol_spec}
\end{listing}

The composition of the CDS volatility structures is defined in the curve configuration.

\subsubsection{Base Correlation Structures}

Base correlation structures are configured as follows
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
  <BaseCorrelations id="default">
   <BaseCorrelation name="CDXIG">BaseCorrelation/CDXIG</BaseCorrelation>
  </BaseCorrelations>
\end{minted}
\caption{Base Correlations block with ID 'default'}
\label{lst:basecorr_spec}
\end{listing}

The composition of the base correlation structure is defined in the curve configuration.

\subsubsection{Correlation Structures}

Correlation structures are configured as follows
\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
 <Correlations id="default">
      <Correlation name="EUR-CMS-10Y:EUR-CMS-1Y">Correlation/EUR-CORR</Correlation>  
      <Correlation name="USD-CMS-10Y:USD-CMS-1Y">Correlation/USD-CORR</Correlation>
 </Correlations>
\end{minted}
\caption{Correlations block with ID 'default'}
\label{lst:corr_spec}
\end{listing}

The composition of the correlation structure is defined in the curve configuration.
\subsubsection{Market Configurations}

Finally, representatives of each type of block (Discount Curves, Index Curves, Volatility structures, etc, up to
Inflation Cap/Floor Price Surfaces) can be bundled into a market configuration. This is done by adding the following to
the {\tt todaysmarket.xml} file:

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Configuration id="default">
  <DiscountingCurvesId>xois_eur</DiscountingCurvesId>
</Configuration>
<Configuration id="collateral_inccy">
  <DiscountingCurvesId>ois</DiscountingCurvesId>
</Configuration>
<Configuration id="collateral_eur">
  <DiscountingCurvesId>xois_eur</DiscountingCurvesId>
</Configuration>
<Configuration id="libor">
  <DiscountingCurvesId>inccy_swap</DiscountingCurvesId>
</Configuration>
\end{minted}
\caption{Market configurations}
\label{lst:config_spec}
\end{listing}

When ORE constructs the market object, all market configurations will be build and labelled using the 'Configuration
Id'.  This allows configuring a market setup for different alternative purposes side by side in the same {\tt
  todaysmarket.xml} file. Typical use cases are
\begin{itemize}
\item different discount curves needed for model calibration and risk factor evolution, respectively
\item different discount curves needed for collateralised and uncollateralised derivatives pricing.
\end{itemize}
The former is actually used throughout the {\tt Examples} section. Each master input file {\tt ore.xml} has a Markets
section (see \ref{sec:master_input}) where four market configuration IDs have to be provided - the ones used for
'lgmcalibration', 'fxcalibration', 'pricing' and 'simulation' (i.e. risk factor evolution).

\medskip The configuration ID concept extends across all curve and volatility objects though currently used only to
distinguish discounting.

\include{parameterisation/pricingengines}

%--------------------------------------------------------
\subsection{Simulation: {\tt simulation.xml}}\label{sec:simulation}
%--------------------------------------------------------

This file determines the behaviour of the risk factor simulation (scenario generation) module.
It is structured in three blocks of data.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Simulation>
  <Parameters> ... </Parameters>
  <CrossAssetModel> ... </CrossAssetModel>
  <Market> ... </Market>
</Simulation>
\end{minted}
\caption{Simulation configuration}
\label{lst:simulation_configuration}
\end{listing}

Each of the three blocks is sketched in the following.

\subsubsection{Parameters}\label{sec:sim_params}

Let us discuss this section using the following example

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Parameters>
  <Grid>80,3M</Grid>
  <Calendar>EUR,USD,GBP,CHF</Calendar>
  <DayCounter>ACT/ACT</DayCounter>
  <Sequence>SobolBrownianBridge</Sequence>
  <Seed>42</Seed>
  <Samples>1000</Samples>
  <Ordering>Steps</Ordering>
  <DirectionIntegers>JoeKuoD7</DirectionIntegers>
  <!-- The following two nodes are optional -->
  <CloseOutLag>2W</CloseOutLag>
  <MporMode>StickyDate</MporMode>
</Parameters>
\end{minted}
\caption{Simulation configuration}
\label{lst:simulation_params_configuration}
\end{listing}

\begin{itemize}
\item {\tt Grid:} Specifies the simulation time grid, here 80 quarterly steps.\footnote{For exposure calculation under DIM, the second parameter has to match the Margin Period of Risk, i.e. if {\tt MarginPeriodOfRisk} is set to for instance {\tt 2W} in a netting set definition in {\tt netting.xml}, then one has to set {\tt Grid} to for instance {\tt 80,2W}.}
\item {\tt Calendar:} Calendar or combination of calendars used to adjust the dates of the grid. Date adjustment is
required because the simulation must step over 'good' dates on which index fixings can be stored.
%\item {\tt Scenario: } Choose between {\em Simple } and {\em Complex } implementations, the latter optimized for
% more efficient memory usage. \todo[inline]{Remove Scenario choice}
\item {\tt DayCounter:} Day count convention used to translate dates to times. Optional, defaults to ActualActual ISDA.
\item {\tt Sequence:} Choose random sequence generator ({\em MersenneTwister, MersenneTwisterAntithetic, Sobol,
  Burley2020Sobol, SobolBrownianBridge, Burley2020SobolBrownianBridge}).
\item {\tt Seed:} Random number generator seed
\item {\tt Samples:} Number of Monte Carlo paths to be produced
%\item {\tt Fixings: } Choose whether fixings should be simulated or not, and if so which fixing simulation method to
use ({\em Backward, Forward, BestOfForwardBackward, InterpolatedForwardBackward}), which number of forward horizon days
to use if one of the {\em Forward } related methods is chosen.
\item {\tt Ordering:} If the sequence type {\em SobolBrownianBridge} or {\em Burley2020SobolBrownianBridge} is used,
  ordering of variates ({\em Factors, Steps, Diagonal})
\item {\tt DirectionIntegers:} If the sequence type {\em SobolBrownianBridge}, {\em Burley2020SobolBrownianBridge}, {\em
  Sobol} or {\em Burley2020Sobol} is used, type of direction integers in Sobol generator ({\em Unit, Jaeckel,
  SobolLevitan, SobolLevitanLemieux, JoeKuoD5, JoeKuoD6, JoeKuoD7, Kuo, Kuo2, Kuo3})
\item {\tt CloseOutLag}: If this tag is present, this specifies the close-out period length (e.g. 2W) used; otherwise no close-out grid is built. The close-out grid is an auxiliary time grid that is offset from the main default date grid by the close-out period, typically set to the applicable margin period of risk. If present, it is used to evolve the portfolio value and determine close-out values associated with the preceding default date valuation.
\item {\tt MporMode}: This tag is expected if the previous one is present, permissible values are then {\tt StickyDate} and {\tt ActualDate}. {\tt StickyDate} means that only market data is evolved from the default date to close-out date for close-out date valuation, the valuation as of date remains unchanged and trades do not ``age'' over the period. As a consequence, exposure evolutions will not show spikes caused by cash flows within the close-out period. {\tt ActualDate} means that trades will also age over the close-out period so that one can experience exposure evolution spikes due to cash flows. 
\end{itemize}

\subsubsection{Model}\label{sec:sim_model}

The {\tt CrossAssetModel} section determines the cross asset model's number of currencies covered, composition, and each
component's calibration. It is currently made of 
\begin{itemize}
\item a sequence of LGM models for each currency (say $n_c$ currencies), 
\item $n_c-1$ FX models for each exchange rate to the base currency, 
\item $n_e$ equity models,
\item $n_i$ inflation models, 
\item $n_{cr}$ credit models, 
\item $n_{com}$ commodity models, 
\item a specification of the correlation structure between all components.
\end{itemize}

\medskip The simulated currencies are specified as follows, with clearly identifying the domestic currency which is also
the target currency for all FX models listed subsequently. If the portfolio requires more currencies to be simulated,
this will lead to an exception at run time, so that it is the user's responsibility to make sure that the list of
currencies here is sufficient. The list can be larger than actually required by the portfolio. This will not lead to any
exceptions, but add to the run time of ORE.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>
  <DomesticCcy>EUR</DomesticCcy>
  <Currencies>
    <Currency>EUR</Currency>
    <Currency>USD</Currency>
    <Currency>GBP</Currency>
    <Currency>CHF</Currency>
    <Currency>JPY</Currency>
  </Currencies>
  <Equities>
	<!-- ... -->
  </Equities>
  <InflationIndices>
	<!-- ... -->
  </InflationIndices>
  <CreditNames>
	<!-- ... -->
  </CreditNames>
  <Commodities>
	<!-- ... -->
  </Commodities>
  <BootstrapTolerance>0.0001</BootstrapTolerance>
  <Measure>LGM</Measure><!-- Choices: LGM, BA -->
  <Discretization>Exact</Discretization>
  <!-- ... -->
</CrossAssetModel>
\end{minted}
\caption{Simulation model currencies configuration}
\label{lst:simulation_model_currencies_configuration}
\end{listing}
 
Bootstrap tolerance is a global parameter that applies to the calibration of all model components. If the calibration
error of any component exceeds this tolerance, this will trigger an exception at runtime, early in the ORE process.

The Measure tag allows switching between the LGM and the Bank Account (BA) measure for the risk-neutral market simulations using the Cross Asset Model. Note that within LGM one can shift the horizon (see ParameterTransformation below) to effectively switch to a T-Forward measure.

The Discretization tag chooses between time discretization schemes for the risk factor evolution. {\em Exact} means
exploiting the analytical tractability of the model to avoid any time discretization error. {\em Euler} uses a naive
time discretization scheme which has numerical error and requires small time steps for accurate results (useful for
testing purposes or if more sophisticated component models are used.)
 
\medskip

Each interest rate model is specified by a block as follows

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <InterestRateModels>
    <LGM ccy="default">
      <CalibrationType>Bootstrap</CalibrationType>
      <Volatility>
        <Calibrate>Y</Calibrate>
        <VolatilityType>Hagan</VolatilityType>
        <ParamType>Piecewise</ParamType>
        <TimeGrid>1.0,2.0,3.0,4.0,5.0,7.0,10.0</TimeGrid>
        <InitialValue>0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01<InitialValue>
      </Volatility>
      <Reversion>
        <Calibrate>N</Calibrate>
        <ReversionType>HullWhite</ReversionType>
        <ParamType>Constant</ParamType>
        <TimeGrid/>
        <InitialValue>0.03</InitialValue>
      </Reversion>
      <CalibrationSwaptions>
        <Expiries>1Y,2Y,4Y,6Y,8Y,10Y,12Y,14Y,16Y,18Y,19Y</Expiries>
        <Terms>19Y,18Y,16Y,14Y,12Y,10Y,8Y,6Y,4Y,2Y,1Y</Terms>
        <Strikes/>
      </CalibrationSwaptions>
      <ParameterTransformation>
        <ShiftHorizon>0.0</ShiftHorizon>
        <Scaling>1.0</Scaling>
      </ParameterTransformation>
      <FloatSpreadMapping>proRata</FloatSpreadMapping>
    </LGM>
    <LGM ccy="EUR">
      <!-- ... -->
    </LGM>
    <LGM ccy="USD">
      <!-- ... -->
    </LGM>
  </InterestRateModels>	
  <!-- ... -->		
</CrossAssetModel>
\end{minted}
\caption{Simulation model IR configuration}
\label{lst:simulation_model_ir_configuration}
\end{listing}

We have LGM sections by currency, but starting with a section for currency 'default'. As the name implies, this is used
as default configuration for any currency in the currency list for which we do not provide an explicit
parametrisation. Within each LGM section, the interpretation of elements is as follows:

\begin{itemize}
\item {\tt CalibrationType: } Choose between {\em Bootstrap} and {\em BestFit}, where Bootstrap is chosen when we expect
to be able to achieve a perfect fit (as with calibration of piecewise volatility to a series of co-terminal Swaptions)
\item {\tt Volatility/Calibrate: } Flag to enable/disable calibration of this particular parameter
\item {\tt Volatility/VolatilityType: } Choose volatility parametrisation a la {\em HullWhite} or {\em Hagan}
\item {\tt Volatility/ParamType: } Choose between {\em Constant} and {\em Piecewise}
\item {\tt Volatility/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
\item {\tt Volatility/InitialValue: } Vector of initial values, matching number of entries in time (for CalibrationType {\em BestFit} this should be one more entry than the {\tt Volatility/TimeGrid} entries, for {\em Bootstrap} this is ignored), or single value if the time grid is empty
\item {\tt Reversion/Calibrate: } Flag to enable/disable calibration of this particular parameter
\item {\tt Reversion/VolatilityType: } Choose reversion parametrisation a la {\em HullWhite} or {\em Hagan}
\item {\tt Reversion/ParamType: } Choose between {\em Constant} and {\em Piecewise}
\item {\tt Reversion/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
\item {\tt Reversion/InitialValue: } Vector of initial values, matching number of entries in time, or single value if
the time grid is empty
\item {\tt CalibrationSwaptions: } Choice of calibration instruments by expiry, underlying Swap term and strike. There have to be at least one more calibration options configured than {\tt Volatility/TimeGrid} entries were given.
\item {\tt ParameterTransformation: } LGM model prices are invariant under scaling and shift transformations
\cite{Lichters} with advantages for numerical convergence of results in long term simulations. These transformations can
be chosen here. Default settings are shiftHorizon 0 (time in years) and scaling factor 1.
\item {\tt FloatSpreadMapping: } mapping of float spreads in analytic swaption pricing for model calibration: proRata,
  nextCoupon, simple, optional, defaults to proRata.
\end{itemize}

The reason for having to specify one more {\tt Volatility/InitialValue} entries than {\tt Volatility/TimeGrid} entries (and at least one more calibration option than {\tt Volatility/TimeGrid} entries) is the fact that the intervals defined by the {\tt Volatility/TimeGrid} entries are spanning from $[0,t_1],[t_1,t_2]\ldots[t_n,\infty]$, which results in $n+1$ intervals.

\medskip

Each FX model is specified by a block as follows

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <ForeignExchangeModels>
    <CrossCcyLGM foreignCcy="default">
      <DomesticCcy>EUR</DomesticCcy>
      <CalibrationType>Bootstrap</CalibrationType>
      <Sigma>
        <Calibrate>Y</Calibrate>
        <ParamType>Piecewise</ParamType>
        <TimeGrid>1.0,2.0,3.0,4.0,5.0,7.0,10.0</TimeGrid>
        <InitialValue>0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1</InitialValue>
      </Sigma>
      <CalibrationOptions>
        <Expiries>1Y,2Y,3Y,4Y,5Y,10Y</Expiries>
        <Strikes/>
      </CalibrationOptions>
    </CrossCcyLGM>
    <CrossCcyLGM foreignCcy="USD">
      <!-- ... -->
    </CrossCcyLGM>
    <CrossCcyLGM foreignCcy="GBP">
      <!-- ... -->
    </CrossCcyLGM>
    <!-- ... -->
  </ForeignExchangeModels>
  <!-- ... -->
<CrossAssetModel>	
\end{minted}
\caption{Simulation model FX configuration}
\label{lst:simulation_model_fx_configuration}
\end{listing}

CrossCcyLGM sections are defined by foreign currency, but we also support a default configuration as above for the IR
model parametrisations.  Within each CrossCcyLGM section, the interpretation of elements is as follows:

\begin{itemize}
\item {\tt DomesticCcy: } Domestic currency completing the FX pair
\item {\tt CalibrationType: } Choose between {\em Bootstrap} and {\em BestFit} as in the IR section
\item {\tt Sigma/Calibrate: } Flag to enable/disable calibration of this particular parameter
\item {\tt Sigma/ParamType: } Choose between {\em Constant} and {\em Piecewise}
\item {\tt Sigma/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
\item {\tt Sigma/InitialValue: } Vector of initial values, matching number of entries in time (for CalibrationType {\em BestFit} this should be one more entry than the {\tt Sigma/TimeGrid} entries, for {\em Bootstrap} this is ignored), or single value if the time grid is empty
\item {\tt CalibrationOptions: } Choice of calibration instruments by expiry and strike, strikes can be empty (implying
the default, ATMF options), or explicitly specified (in terms of FX rates as absolute strike values, in delta notation
such as $\pm 25D$, $ATMF$ for at the money). There have to be at least one more calibration options configured than {\tt Sigma/TimeGrid} entries were given
\end{itemize}


\medskip

Each equity model is specified by a block as follows

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <EquityModels>
    <CrossAssetLGM name="default">
      <Currency>EUR</Currency>
      <CalibrationType>Bootstrap</CalibrationType>
      <Sigma>
        <Calibrate>Y</Calibrate>
        <ParamType>Piecewise</ParamType>
        <TimeGrid>1.0,2.0,3.0,4.0,5.0,7.0,10.0</TimeGrid>
        <InitialValue>0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1</InitialValue>
      </Sigma>
      <CalibrationOptions>
        <Expiries>1Y,2Y,3Y,4Y,5Y,10Y</Expiries>
        <Strikes/>
      </CalibrationOptions>
    </CrossAssetLGM>
    <CrossAssetLGM name="SP5">
      <!-- ... -->
    </CrossAssetLGM>
    <CrossAssetLGM name="Lufthansa">
      <!-- ... -->
    </CrossAssetLGM>
      <!-- ... -->
  </EquityModels>
  <!-- ... -->
<CrossAssetModel>	
\end{minted}
\caption{Simulation model equity configuration}
\label{lst:simulation_model_eq_configuration}
\end{listing}

CrossAssetLGM sections are defined by equity name, but we also support a default configuration as above for the IR and 
FX model parameterisations.  Within each CrossAssetLGM section, the interpretation of elements is as follows:

\begin{itemize}
	\item {\tt Currency: } Currency of denomination
	\item {\tt CalibrationType: } Choose between {\em Bootstrap} and {\em BestFit} as in the IR section
	\item {\tt Sigma/Calibrate: } Flag to enable/disable calibration of this particular parameter
	\item {\tt Sigma/ParamType: } Choose between {\em Constant} and {\em Piecewise}
	\item {\tt Sigma/TimeGrid: } Initial time grid for this parameter, can be left empty if ParamType is Constant
	\item {\tt Sigma/InitialValue: } Vector of initial values, matching number of entries in time (for CalibrationType {\em BestFit} this should be one more entry than the {\tt Sigma/TimeGrid} entries, for {\em Bootstrap} this is ignored), or single value if the time grid is empty
	\item {\tt CalibrationOptions: } Choice of calibration instruments by expiry and strike, strikes can be empty 
	(implying the default, ATMF options), or explicitly specified (in terms of equity prices as absolute strike values). There have to be at least one more calibration options configured than {\tt Sigma/TimeGrid} entries were given
\end{itemize}

\medskip

For the inflation model component, there is a choice between a Dodgson Kainth model and a Jarrow Yildrim model. The Dodgson Kainth 
model is specified in a \lstinline!LGM! or \lstinline!DodgsonKainth! node as outlined in Listing \ref{lst:simulation_model_dk_inflation_configuration}.
The inflation model parameterisation inherits from the LGM parameterisation for interest rate components, in particular the \lstinline!CalibrationType!, 
\lstinline!Volatility! and \lstinline!Reversion! elements. The \lstinline!CalibrationCapFloors! element specify the model's calibration to a selection of 
either CPI caps or CPI floors with specified strike.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  ...
  <InflationIndexModels>
    <LGM index="EUHICPXT">
      <Currency>EUR</Currency>
      <!-- As in the LGM parameterisation for any IR components -->
      <CalibrationType> ... </CalibrationType>
      <Volatility> ... </Volatility>
      <Reversion> ... </Reversion> 
      <ParameterTransformation> ... </ParameterTransformation>
      <!-- Inflation model specific -->
      <CalibrationCapFloors>
        <!-- not used yet, as there is only one strategy so far -->
        <CalibrationStrategy> ... </CalibrationStrategy> 
        <CapFloor> Floor </CapFloor> <!-- Cap, Floor -->
        <Expiries> 2Y, 4Y, 6Y, 8Y, 10Y </Expiries>
        <!-- can be empty, this will yield calibration to ATM -->
        <Strikes> 0.03, 0.03, 0.03, 0.03, 0.03 </Strikes> 
      </CalibrationCapFloors>
    </LGM>
    <LGM index="USCPI">
      ...
    </LGM>
    ...
  </InflationIndexModels>
  ...
<CrossAssetModel>	
\end{minted}
\caption{Simulation model DK inflation component configuration}
\label{lst:simulation_model_dk_inflation_configuration}
\end{listing}

The calibration instruments may be specified in an alternative way via a \lstinline!CalibrationBaskets! node. In general, a \lstinline!CalibrationBaskets! node 
can contain multiple \lstinline!CalibrationBasket! nodes each containing a list of calibration instruments of the same type. For Dodgson Kainth, only a single 
calibration basket is allowed and the instruments must be of type \lstinline!CpiCapFloor!. So, for example, the \lstinline!CalibrationCapFloors! node in 
Listing \ref{lst:simulation_model_dk_inflation_configuration} could be replaced with the \lstinline!CalibrationBaskets! node in \ref{lst:dk_inflation_calibration_basket}.

\begin{listing}[H]
\begin{minted}[fontsize=\footnotesize]{xml}
<CalibrationBaskets>
  <CalibrationBasket>
    <CpiCapFloor>
      <Type>Floor</Type>
      <Maturity>2Y</Maturity>
      <Strike>0.03</Strike>
    </CpiCapFloor>
    <CpiCapFloor>
      <Type>Floor</Type>
      <Maturity>4Y</Maturity>
      <Strike>0.03</Strike>
    </CpiCapFloor>
    <CpiCapFloor>
      <Type>Floor</Type>
      <Maturity>6Y</Maturity>
      <Strike>0.03</Strike>
    </CpiCapFloor>
    <CpiCapFloor>
      <Type>Floor</Type>
      <Maturity>8Y</Maturity>
      <Strike>0.03</Strike>
    </CpiCapFloor>
    <CpiCapFloor>
      <Type>Floor</Type>
      <Maturity>10Y</Maturity>
      <Strike>0.03</Strike>
    </CpiCapFloor>
  </CalibrationBasket>
</CalibrationBaskets>
\end{minted}
\caption{Calibration basket for DK inflation model component}
\label{lst:dk_inflation_calibration_basket}
\end{listing}

The Jarrow Yildrim model is specified in a \lstinline!JarrowYildirim! node as outlined in Listing \ref{lst:simulation_model_jy_inflation_configuration}. The \lstinline!RealRate! 
node describes the JY real rate process and has \lstinline!Volatility! and \lstinline!Reversion! nodes that follow those outlined in the interest rate LGM section above. The  
\lstinline!Index! node describes the JY index process and has a \lstinline!Volatility! component that follows the \lstinline!Sigma! component of the FX model above. The 
\lstinline!CalibrationBaskets! node is as outlined above for Dodgson Kainth but up to two baskets may be used and extra inflation instruments are supported in the calibration. More 
information is provided below.

The \lstinline!CalibrationType! determines the calibration approach, if any, that is used to calibrate the various parameters of the model i.e.\ the real rate reversion, the real 
rate volatility and the index volatility. If the \lstinline!CalibrationType! is \lstinline!None!, no calibration is attempted and all parameter values must be explicitly specified.
If the \lstinline!CalibrationType! is \lstinline!BestFit!, the parameters that have \lstinline!Calibrate! set to \lstinline!Y! will be calibrated to the instruments specified in 
the \lstinline!CalibrationBaskets! node. If the \lstinline!CalibrationType! is \lstinline!Bootstrap!, there are a number of options:

\begin{enumerate}
\item
The index volatility parameter may be calibrated, indicated by setting \lstinline!Calibrate! to \lstinline!Y! for that parameter, with both of the real rate parameters not calibrated 
and set explicitly in the \lstinline!RealRate! node. There should be exactly one \lstinline!CalibrationBasket! in the \lstinline!CalibrationBaskets! node and its \lstinline!parameter! 
attribute may be set to \lstinline!Index! or omitted.

\item
One of the real rate parameters may be calibrated, indicated by setting \lstinline!Calibrate! to \lstinline!Y! for that parameter, with the index volatility not calibrated and set 
explicitly in the \lstinline!Volatility! node. There should be exactly one \lstinline!CalibrationBasket! in the \lstinline!CalibrationBaskets! node and its \lstinline!parameter! 
attribute may be set to \lstinline!RealRate! or omitted.

\item
One of the real rate parameters and the index volatility parameter may be calibrated together. There should be exactly two \lstinline!CalibrationBasket! nodes in the \lstinline!CalibrationBaskets! 
node. The \lstinline!parameter! attribute should be set to \lstinline!RealRate! on the \lstinline!CalibrationBasket! node that should be used for the real rate parameter calibration. 
Similarly, the \lstinline!parameter! attribute should be set to \lstinline!Index! on the \lstinline!CalibrationBasket! node that should be used for the index volatility parameter calibration. 
The parameters are calibrated iteratively in turn until the root mean squared error over all calibration instruments in the two baskets is below the tolerance specified by the 
\lstinline!RmseTolerance! in the \lstinline!CalibrationConfiguration! node or until the maximum number of iterations as specified by the \lstinline!MaxIterations! in the 
\lstinline!CalibrationConfiguration! node has been reached. The \lstinline!CalibrationConfiguration! node is optional. If it is omitted, the \lstinline!RmseTolerance! defaults to 0.0001 and the 
\lstinline!MaxIterations! defaults to 50.

\end{enumerate}

Note that it is an error to attempt to calibrate both of the real rate parameters together when \lstinline!CalibrationType! is \lstinline!Bootstrap!. If a parameter is being calibrated 
with \lstinline!CalibrationType! set to \lstinline!Bootstrap!, the \lstinline!ParamType! should be \lstinline!Piecewise!. The \lstinline!TimeGrid! will be overridden for that parameter by 
the relevant calibration instrument times and the parameter's initial values are set to the first element of the \lstinline!InitialValue! list. So, leaving the \lstinline!TimeGrid! node 
empty and giving a single value in the \lstinline!InitialValue! node is the clearest XML setup in this case.

\begin{listing}[H]
\begin{minted}[fontsize=\footnotesize]{xml}
<JarrowYildirim index="EUHICPXT">
  <Currency>EUR</Currency>
  <CalibrationType>Bootstrap</CalibrationType>
  <RealRate>
    <Volatility>
      <Calibrate>Y</Calibrate>
      <VolatilityType>Hagan</VolatilityType>
      <ParamType>Piecewise</ParamType>
      <TimeGrid/>
      <InitialValue>0.0001</InitialValue>
    </Volatility>
    <Reversion>
      <Calibrate>N</Calibrate>
      <ReversionType>HullWhite</ReversionType>
      <ParamType>Constant</ParamType>
      <TimeGrid/>
      <InitialValue>0.5</InitialValue>
    </Reversion>
    <ParameterTransformation>
      <ShiftHorizon>0.0</ShiftHorizon>
      <Scaling>1.0</Scaling>
    </ParameterTransformation>
  </RealRate>
  <Index>
    <Volatility>
      <Calibrate>Y</Calibrate>
      <ParamType>Piecewise</ParamType>
      <TimeGrid/>
      <InitialValue>0.0001</InitialValue>
    </Volatility>
  </Index>
  <CalibrationBaskets>
    <CalibrationBasket parameter="Index">
      <CpiCapFloor>
        <Type>Floor</Type>
        <Maturity>2Y</Maturity>
        <Strike>0.0</Strike>
      </CpiCapFloor>
      ...
    </CalibrationBasket>
    <CalibrationBasket parameter="RealRate">
      <YoYSwap>
        <Tenor>2Y</Tenor>
      </YoYSwap>
      ...
    </CalibrationBasket>
  </CalibrationBaskets>
  <CalibrationConfiguration>
    <RmseTolerance>0.00000001</RmseTolerance>
    <MaxIterations>40</MaxIterations>
  </CalibrationConfiguration>
</JarrowYildirim>
\end{minted}
\caption{Simulation model JY inflation component configuration}
\label{lst:simulation_model_jy_inflation_configuration}
\end{listing}

The \lstinline!CpiCapFloor! and \lstinline!YoYSwap! calibration instruments can be seen in Listing \ref{lst:simulation_model_jy_inflation_configuration}. A \lstinline!YoYCapFloor! is 
also allowed and it has the structure shown in Listing \ref{lst:yoy_cf_calibration_inst}. The \lstinline!Type! may be \lstinline!Cap! or \lstinline!Floor!. The \lstinline!Tenor! should 
be a maturity period e.g.\ \lstinline!5Y!. The \lstinline!Strike! should be an absolute strike level for the year on year cap or floor e.g.\ \lstinline!0.01! for 1\%.

\begin{listing}[H]
\begin{minted}[fontsize=\footnotesize]{xml}
<YoYCapFloor>
  <Type>...</Type>
  <Tenor>...</Tenor>
  <Strike>...</Strike>
</YoYCapFloor>
\end{minted}
\caption{Layout for \lstinline!YoYCapFloor! calibration instrument.}
\label{lst:yoy_cf_calibration_inst}
\end{listing}

% credit models: todo

% commodity models

For commodity simulation we currently provide one model, as described in the methodology appendix. 
Commodity model components are specified by commodity name, by a block as follows

\begin{listing}[H]
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>	
  <!-- ... -->
  <CommodityModels>
    <CommoditySchwartz name="default">
      <Currency>EUR</Currency>
      <CalibrationType>None</CalibrationType>
      <Sigma>
        <Calibrate>Y</Calibrate>
        <InitialValue>0.1</InitialValue>
      </Sigma>
      <Kappa>
        <Calibrate>Y</Calibrate>
        <InitialValue>0.1</InitialValue>
      </Kappa>
      <CalibrationOptions>
           ...
      </CalibrationOptions>
      <DriftFreeState>false</DriftFreeState>
    </CommoditySchwartz>
    <CommoditySchwartz name="WTI">
      <!-- ... -->
    </CommoditySchwartz>
    <CommoditySchwartz name="NG">
      <!-- ... -->
    </CommoditySchwartz>
      <!-- ... -->
  </CommodityModels>
  <!-- ... -->
<CrossAssetModel>	
\end{minted}
\caption{Simulation model commodity configuration}
\label{lst:simulation_model_com_configuration}
\end{listing}

CommoditySchwartz sections are defined by commodity name, but we also support a default configuration as above for the IR and 
FX model parameterisations.  Each component is parameterised in terms of two constant, non time-dependent parameters $\sigma$ and $\kappa$ so far (see appendix).
Within each CommoditySchwartz section, the interpretation of elements is as follows:

\begin{itemize}
\item {\tt Currency: } Currency of denomination
\item {\tt CalibrationType:} Choose between {\em BestFit} and {\em None}.  The choice {\em None} will deactivate calibration as usual. {\em BestFit} will attempt to set the model parameter(s) such that the error in matching calibration instrument prices is minimised.  The option  {\em Bootstrap} is not available here because the model parameters are not time-dependent and the model's degrees of freedom in general do not suffice to perfectly match the calibration instrument prices.
\item {\tt Sigma/Calibrate:} Flag to enable/disable calibration of this particular parameter
\item {\tt Sigma/InitialValue:} Initial value of the constant parameter
\item {\tt Kappa/Calibrate:} Flag to enable/disable calibration of this particular parameter
\item {\tt Kappa/InitialValue:} Initial value of the constant parameter
\item {\tt CalibrationOptions:} Choice of calibration instruments by expiry and strike, strikes can be empty 	(implying the default, ATMF options), or explicitly specified (in terms of commodity prices as absolute strike values). 
\item {\tt DriftFreeState[Optional]:} Boolean to switch between the two implementations of the state variable, see appendix. By default this is set to {\tt false}.        
\end{itemize}

\medskip
Finally, the instantaneous correlation structure is specified as follows.

\begin{listing}[H]
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<CrossAssetModel>
  <!-- ... -->
  <InstantaneousCorrelations>
    <Correlation factor1="IR:EUR" factor2="IR:USD">0.3</Correlation>
    <Correlation factor1="IR:EUR" factor2="IR:GBP">0.3</Correlation>
    <Correlation factor1="IR:USD" factor2="IR:GBP">0.3</Correlation>
    <Correlation factor1="IR:EUR" factor2="FX:USDEUR">0</Correlation>
    <Correlation factor1="IR:EUR" factor2="FX:GBPEUR">0</Correlation>
    <Correlation factor1="IR:GBP" factor2="FX:USDEUR">0</Correlation>
    <Correlation factor1="IR:GBP" factor2="FX:GBPEUR">0</Correlation>
    <Correlation factor1="IR:USD" factor2="FX:USDEUR">0</Correlation>
    <Correlation factor1="IR:USD" factor2="FX:GBPEUR">0</Correlation>
    <Correlation factor1="FX:USDEUR" factor2="FX:GBPEUR">0</Correlation>
    <!-- ... --> 
  </InstantaneousCorrelations>
</CrossAssetModel>
\end{minted}
\caption{Simulation model correlation configuration}
\label{lst:simulation_model_correlation_configuration}
\end{listing}

Any risk factor pair not specified explicitly here will be assumed to have zero correlation. Note that the commodity components can have non-zero correlations among each other, but correlations to all other CAM components must remain set to zero for the time being.

\subsubsection{Market}\label{sec:sim_market}

The last part of the simulation configuration file covers the specification of the simulated market.  Note that the
simulation model will yield the evolution of risk factors such as short rates which need to be translated into entire
yield curves that can be 'understood' by the instruments which we want to price under scenarios.  

Moreover we need to specify how volatility structures evolve even if we do not explicitly simulate volatility. This 
translation happens based on the information in the {\em simulation market} object, which is configured in the section 
within the enclosing tags {\tt <Market>} and {\tt </Market>}, as shown in the following small example.

It should be noted that equity volatilities are taken to be a curve by default. To simulate an equity volatility surface with smile the xml node {\tt <Surface> } must be supplied.
There are two methods in ORE for equity volatility simulation: 
\begin{itemize}
\item Simulating ATM volatilities only (and shifting other strikes relative to this using the $T_{0}$ smile). In this
  case set {\tt <SimulateATMOnly>} to true and no surface node is given.
\item Simulating the full volatility surface. The node {\tt <SimulateATMOnly>} should be omitted or set to false, and
  explicit moneyness levels for simulation should be provided.
\end{itemize}

Swaption volatilities are taken to be a surface by default. To simulate a swaption volatility cube with smile the xml node {\tt <Cube> } must be supplied.
There are two methods in ORE for swaption volatility cube simulation: 
\begin{itemize}
\item Simulating ATM volatilities only (and shifting other strikes relative to this using the $T_{0}$ smile). In this case set {\tt <SimulateATMOnly>} to true.
\item Simulating the full volatility cube. The node {\tt <SimulateATMOnly>} should be omitted or set to false, and
  explicit strike spreads for simulation should be provided.
\end{itemize}

FX volatilities are taken to be a curve by default. To simulate an FX volatility cube with smile the xml node {\tt <Surface> } must be supplied. The surface node contains the moneyness levels to be simulated.

For Yield Curves, Swaption Volatilities, CapFloor Volatilities, Default Curves, Base Correlations and Inflation Curves, a DayCounter may be specified for each risk factor using the node {\tt <DayCounter name="EXAMPLE\_CURVE">}.  
If no day counter is specified for a given risk factor then the default Actual365 is used. To specify a new default for a risk factor type then use the daycounter node without any attribute,  {\tt <DayCounter>}.

For Yield Curves, there are several choices for the interpolation and extrapolation:
\begin{itemize}
\item Interpolation: This can be LogLinear or LinearZero. If not given, the value defaults to LogLinear.
\item Extrapolation: This can be FlatFwd or FlatZero. If not given, the value defaults to FlatFwd.
\end{itemize}

For Default Curve, there is a similar choice for the extrapolation:
\begin{itemize}
\item Extrapolation: This can be FlatFwd or FlatZero. If not given, the value defaults to FlatFwd.
\end{itemize}

For swap, yield, interest cap-floor, yoy inflation cap-floor, zc inflation cap-floor, cds, fx, equity, commodity
volatilities the smile dynamics can be specified as shown in listing \ref{lst:smile_dynamics_configuration} for swap
vols. The empty key serves as a default configuration for all keys for which no own smile dynamics node is present. The
allowed smile dynamics values are StickyStrike and StickyMoneyness. If not given, the smile dynamics defaults to
StickyStrike.

\begin{listing}
  \begin{minted}[fontsize=\footnotesize]{xml}
    <SmileDynamics key="">StickyStrike</SmileDynamics>
    <SmileDynamics key="EUR-ESTER">StickyMoneyness</SmileDynamics>
  \end{minted}
\caption{Smile Configuration Node}
\label{lst:smile_dynamics_configuration}
\end{listing}

\begin{longlisting}
%\hrule\medskip
\begin{minted}[fontsize=\footnotesize]{xml}
<Market>
  <BaseCurrency>EUR</BaseCurrency>
  <Currencies>
    <Currency>EUR</Currency>
    <Currency>USD</Currency>
  </Currencies>
  <YieldCurves>
    <Configuration>
      <Tenors>3M,6M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,12Y,15Y,20Y</Tenors>
      <Interpolation>LogLinear</Interpolation>
      <Extrapolation>FlatFwd</Extrapolation>
      <DayCounter>ACT/ACT</DayCounter> <!-- Sets a new default for all yieldCurves -->
    </Configuration>
  </YieldCurves>
  <Indices>
    <Index>EUR-EURIBOR-6M</Index>
    <Index>EUR-EURIBOR-3M</Index>
    <Index>EUR-EONIA</Index>
    <Index>USD-LIBOR-3M</Index>
  </Indices>
  <SwapIndices>
    <SwapIndex>
      <Name>EUR-CMS-1Y</Name>
      <ForwardingIndex>EUR-EURIBOR-6M</ForwardingIndex>
      <DiscountingIndex>EUR-EONIA</DiscountingIndex>
    </SwapIndex>
  </SwapIndices>
  <DefaultCurves> 
      <Names> 
        <Name>CPTY1</Name> 
        <Name>CPTY2</Name> 
      </Names> 
      <Tenors>6M,1Y,2Y</Tenors> 
      <SimulateSurvivalProbabilities>true</SimulateSurvivalProbabilities> 
      <DayCounter name="CPTY1">ACT/ACT</DayCounter>
      <Extrapolation>FlatFwd</Extrapolation>
  </DefaultCurves> 
  <SwaptionVolatilities>
    <ReactionToTimeDecay>ForwardVariance</ReactionToTimeDecay>
    <Currencies>
      <Currency>EUR</Currency>
      <Currency>USD</Currency>
    </Currencies>
    <Expiries>6M,1Y,2Y,3Y,5Y,10Y,12Y,15Y,20Y</Expiries>
    <Terms>1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,30Y</Terms>
    <SimulateATMOnly>false</SimulateATMOnly>
    <Cube>
     <StrikeSpreads>-0.02,-0.01,0.0,0.01,0.02</StrikeSpreads>
    </Cube>
    <!-- Sets a new daycounter for just the EUR swaptionVolatility surface -->
    <DayCounter ccy="EUR">ACT/ACT</DayCounter> 
  </SwaptionVolatilities> 
  <CapFloorVolatilities>
    <ReactionToTimeDecay>ConstantVariance</ReactionToTimeDecay>
    <Currencies>
      <Currency>EUR</Currency>
      <Currency>USD</Currency>
    </Currencies>
    <DayCounter ccy="EUR">ACT/ACT</DayCounter>
  </CapFloorVolatilities>
  <FxVolatilities>
    <ReactionToTimeDecay>ForwardVariance</ReactionToTimeDecay>
    <CurrencyPairs>
      <CurrencyPair>EURUSD</CurrencyPair>
    </CurrencyPairs>
    <Expiries>6M,1Y,2Y,3Y,4Y,5Y,7Y,10Y</Expiries>
    <Surface>
     <Moneyness>0.5,0.6,0.7,0.8,0.9</Moneyness>
    </Surface>
  </FxVolatilities>
  <EquityVolatilities>
      <Simulate>true</Simulate>
      <ReactionToTimeDecay>ForwardVariance</ReactionToTimeDecay>
      <!-- Alternative: ConstantVariance -->
      <Names>
        <Name>SP5</Name>
        <Name>Lufthansa</Name>
      </Names>
      <Expiries>6M,1Y,2Y,3Y,4Y,5Y,7Y,10Y</Expiries>
      <SimulateATMOnly>false</SimulateATMOnly>
      <Surface>
        <Moneyness>0.1,0.5,1.0,1.5,2.0,3.0</Moneyness>
      </Surface>
      <TimeExtrapolation>Flat</TimeExtrapolation>
      <StrikeExtrapolation>Flat</StrikeExtrapolation>

  </EquityVolatilities>
  ...
  <BenchmarkCurves>
    <BenchmarkCurve>
      <Currency>EUR</Currency>
      <Name>BENCHMARK_EUR</Name>
  </BenchmarkCurve>
  ...
  </BenchmarkCurves>
  <Securities>
    <Simulate>true</Simulate>
    <Names>
      <Name>SECURITY_1</Name>
      ...
    </Names>
  </Securities>
  <ZeroInflationIndexCurves>
    <Names>
      <Name>EUHICP</Name>
      <Name>UKRPI</Name>
      <Name>USCPI</Name>
      ...
    </Names>
    <Tenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</Tenors>
  </ZeroInflationIndexCurves>
  <YYInflationIndexCurves>
    <Names>
      <Name>EUHICPXT</Name>
      ...
    </Names>
    <Tenors>1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</Tenors>
  </YYInflationIndexCurves>
  <DefaultCurves>
    <Names>
      <Name>ItraxxEuropeCrossoverS26V1</Name>
      ...
    </Names>
    <Tenors>1Y,2Y,3Y,5Y,10Y</Tenors>
    <SimulateSurvivalProbabilities>true</SimulateSurvivalProbabilities>
  </DefaultCurves>
  <BaseCorrelations/>
  <CDSVolatilities/>
  <Correlations>
    <Simulate>true</Simulate>
    <Pairs>
      <Pair>EUR-CMS-10Y,EUR-CMS-2Y</Pair>
    </Pairs>
    <Expiries>1Y,2Y</Expiries>
  </Correlations>
  <AggregationScenarioDataCurrencies>
    <Currency>EUR</Currency>
    <Currency>USD</Currency>
  </AggregationScenarioDataCurrencies>
  <AggregationScenarioDataIndices>
    <Index>EUR-EURIBOR-3M</Index>
    <Index>EUR-EONIA</Index>
    <Index>USD-LIBOR-3M</Index>
  </AggregationScenarioDataIndices>
</Market>
\end{minted}
\caption{Simulation market configuration}
\label{lst:simulation_market_configuration}
\end{longlisting}

\todo[inline]{Comment on cap/floor surface structure and reaction to time decay}

%--------------------------------------------------------
\subsection{Sensitivity Analysis: {\tt sensitivity.xml}}\label{sec:sensitivity}
%--------------------------------------------------------

ORE currently supports sensitivity analysis with respect to
\begin{itemize}
\item Discount curves  (in the zero rate domain)
\item Index curves (in the zero rate domain)
\item Yield curves including e.g. equity forecast yield curves (in the zero rate domain)
\item FX Spots
\item FX volatilities
\item Swaption volatilities, ATM matrix or cube 
\item Cap/Floor volatility matrices (in the caplet/floorlet domain)
\item Default probability curves (in the ``zero rate'' domain, expressing survival probabilities $S(t)$ in term of zero rates $z(t)$ via $S(t)=\exp(-z(t)\times t)$ with Actual/365 day counter)
\item Equity spot prices
\item Equity volatilities, ATM or including strike dimension 
\item Zero inflation curves
\item Year-on-Year inflation curves
\item CDS volatilities
\item Bond credit spreads
\item Base correlation curves
\item Correlation termstructures
\end{itemize}

The {\tt sensitivity.xml} file specifies how sensitivities are computed for each market component. 
The general structure is shown in listing \ref{lst:sensitivity_config}, for a more comprehensive case see {\tt Examples/Example\_15}. A subset of the following parameters is used in each market component to specify the sensitivity analysis:

\begin{itemize}
\item {\tt ShiftType:} Both absolute or relative shifts can be used to compute a sensitivity, specified by the key words
  {\tt Absolute} resp. {\tt Relative}.
\item {\tt ShiftSize:} The size of the shift to apply.
\item {\tt ShiftScheme:} The finite difference scheme to use ({\tt Forward}, {\tt Backward}, {\tt Central}), if not given, this parameter defaults to {\tt Forward}
\item {\tt ShiftTenors:} For curves, the tenor buckets to apply shifts to, given as a comma separated list of periods.
\item {\tt ShiftExpiries:} For volatility surfaces, the option expiry buckets to apply shifts to, given as a comma
  separated list of periods.
\item {\tt ShiftStrikes:} For cap/floor, FX option and equity option volatility surfaces, the strikes to apply shifts to, given as a comma separated
  list of absolute strikes
\item {\tt ShiftTerms:} For swaption volatility surfaces, the underlying terms to apply shifts to, given as a comma
  separated list of periods.
\item {\tt Index:} For cap / floor volatility surfaces, the index which together with the currency defines the surface.
  list of absolute strikes
\item {\tt CurveType:} In the context of Yield Curves used to identify an equity ``risk free'' rate forecasting curve; set to {\tt EquityForecast} in this case
\end{itemize}

The ShiftType, ShiftSize, ShiftScheme nodes take an optional attribute key that allows to configure different values for
different sensitivity templates. The sensitivity templates are defined in the pricing engine configuration. This is best
explained by an example: In Example 15 the product type BermudanSwaption has a sensitivity template \verb+IR_FD+
attached, see \ref{lst:sensi_template}. This can be used to specify different shifts for trades that were built against
this engine configuration, see \ref{lst:sensi_config_template}: For Bermudan swaptions a larger shift size of 10bp and a
central difference scheme is used to compute discount curve sensitivities in EUR. Since no separate shift type is
specified, the default shift type {\tt Absolute} is used. Note regarding the reports:

\begin{itemize}
\item the sensi scenario report contains scenario NPVs related to the possibly product specific configured shift sizes
\item the sensi report contains renormalized sensitivities, i.e. sensitivities are always expressed w.r.t. the default shift sizes
\item the sensi config report only contains the default configuration
\end{itemize}

\begin{longlisting}
\begin{minted}[fontsize=\scriptsize]{xml}
  <Product type="BermudanSwaption">
    <Model>LGM</Model>
    <ModelParameters>
      ...
    </ModelParameters>
    <Engine>Grid</Engine>
    <EngineParameters>
      ...
      <Parameter name="SensitivityTemplate">IR_FD</Parameter>
    </EngineParameters>
  </Product>
\end{minted}
\caption{Sensitivity template definition}
\label{lst:sensi_template}
\end{longlisting}

\begin{longlisting}
\begin{minted}[fontsize=\scriptsize]{xml}
    <DiscountCurve ccy="EUR">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftScheme>Forward</ShiftScheme>
      <ShiftSize key="IR_FD">0.001</ShiftSize>
      <ShiftScheme key="IR_FD">Central</ShiftScheme>
      <ShiftTenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</ShiftTenors>
    </DiscountCurve>
\end{minted}
\caption{Sensitivity template definition}
\label{lst:sensi_config_template}
\end{longlisting}

The cross gamma filter section contains a list of pairs of sensitivity keys. For each possible pair of sensitivity keys
matching the given strings, a cross gamma sensitivity is computed. The given pair of keys can be (and usually are)
shorter than the actual sensitivity keys. In this case only the prefix of the actual key is matched. For example, the
pair {\tt DiscountCurve/EUR,DiscountCurve/EUR} matches all actual sensitivity pairs belonging to a cross sensitivity by
one pillar of the EUR discount curve and another (different) pillar of the same curve. We list the possible keys by
giving an example in each category:

\begin{itemize}
\item {\tt DiscountCurve/EUR/5/7Y}: 7y pillar of discounting curve in EUR, the pillar is at position 5 in the list of
  all pillars (counting starts with zero)
\item {\tt YieldCurve/BENCHMARK\_EUR/0/6M}: 6M pillar of yield curve ``BENCHMARK\_EUR'', the index of the 6M pillar is
  zero (i.e. it is the first pillar)
\item {\tt IndexCurve/EUR-EURIBOR-6M/2/2Y}: 2Y pillar of index forwarding curve for the Ibor index ``EUR-EURIBOR-6M'',
  the pillar index is 2 in this case
\item {\tt OptionletVolatility/EUR/18/5Y/0.04}: EUR caplet volatility surface, at 5Y option expiry and $4\%$ strike, the
  running index for this expiry - strike pair is 18; the index counts the points in the surface in lexical order
  w.r.t. the dimensions option expiry, strike
\item {\tt FXSpot/USDEUR/0/spot}: FX spot USD vs EUR (with EUR as base ccy), the index is always zero for FX spots, the
  pillar is labelled as ``spot'' always
\item {\tt SwaptionVolatility/EUR/11/10Y/10Y/ATM}: EUR Swaption volatility surface at 10Y option expiry and 10Y
  underlying term, ATM level, the running index for this expiry, term, strike triple has running index 11; the index
  counts the points in the surface in lexical order w.r.t. the dimensions option expiry, underlying term and strike
\end{itemize}

Additional flags:

\begin{itemize}
\item ComputeGamma: If set to false, second order sensitivity computation is suppressed
\item UseSpreadedTermStructures: If set to true, spreaded termstructures over t0 will be used for sensitivity
  calculation (where supported), to improve the alignment of the scenario sim market and t0 curves
\end{itemize}

\begin{longlisting}
%\hrule\medskip
  \begin{minted}[fontsize=\scriptsize]{xml}
<SensitivityAnalysis>
  <DiscountCurves>
    <DiscountCurve ccy="EUR">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</ShiftTenors>
    </DiscountCurve>
    ...
  </DiscountCurves>
  ...
  <IndexCurves>
    <IndexCurve index="EUR-EURIBOR-6M">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</ShiftTenors>
    </IndexCurve>
  </IndexCurves>
  ...
  <YieldCurves>
    <YieldCurve name="BENCHMARK_EUR">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</ShiftTenors>
    </YieldCurve>
  </YieldCurves>
  ...
  <FxSpots>
    <FxSpot ccypair="USDEUR">
      <ShiftType>Relative</ShiftType>
      <ShiftSize>0.01</ShiftSize>
    </FxSpot>
  </FxSpots>
  ...
  <FxVolatilities>
    <FxVolatility ccypair="USDEUR">
      <ShiftType>Relative</ShiftType>
      <ShiftSize>0.01</ShiftSize>
      <ShiftExpiries>1Y,2Y,3Y,5Y</ShiftExpiries>
      <ShiftStrikes/>
    </FxVolatility>
  </FxVolatilities>
  ...
  <SwaptionVolatilities>
    <SwaptionVolatility ccy="EUR">
      <ShiftType>Relative</ShiftType>
      <ShiftSize>0.01</ShiftSize>
      <ShiftExpiries>1Y,5Y,7Y,10Y</ShiftExpiries>
      <ShiftStrikes/>
      <ShiftTerms>1Y,5Y,10Y</ShiftTerms>
    </SwaptionVolatility>
  </SwaptionVolatilities>
  ...
  <CapFloorVolatilities>
    <CapFloorVolatility ccy="EUR">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftExpiries>1Y,2Y,3Y,5Y,7Y,10Y</ShiftExpiries>
      <ShiftStrikes>0.01,0.02,0.03,0.04,0.05</ShiftStrikes>
      <Index>EUR-EURIBOR-6M</Index>
    </CapFloorVolatility>
  </CapFloorVolatilities>
  ...
  <SecuritySpreads>
    <SecuritySpread security="SECURITY_1">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
    </SecuritySpread>
  </SecuritySpreads>
  ...
  <Correlations>
    <Correlation index1="EUR-CMS-10Y" index2="EUR-CMS-2Y">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.01</ShiftSize>
      <ShiftExpiries>1Y,2Y</ShiftExpiries>
      <ShiftStrikes>0</ShiftStrikes>
    </Correlation>
  </Correlations>
  ...
  <CrossGammaFilter>
    <Pair>DiscountCurve/EUR,DiscountCurve/EUR</Pair>
    <Pair>IndexCurve/EUR,IndexCurve/EUR</Pair>
    <Pair>DiscountCurve/EUR,IndexCurve/EUR</Pair>
  </CrossGammaFilter>
  ...
  <ComputeGamma>true</ComputeGamma>
  <UseSpreadedTermStructures>false</UseSpreadedTermStructures>
</SensitivityAnalysis>
\end{minted}
\caption{Sensitivity configuration}
\label{lst:sensitivity_config}
\end{longlisting}

\subsubsection*{Par Sensitivity Analysis}

To perform a par sensitivity analysis, additional sensitivity configuration is required that describes the assumed par instruments and related conventions.
This additional data is required for:
\begin{itemize}
\item DiscountCurves
\item IndexCurves
\item CapFloorVolatilities
\item CreditCurves
\item ZeroInflationIndexCurves
\item YYInflationIndexCurves
\item YYCapFloorVolatilities
\end{itemize}

Using DiscountCurves as an example, the full sensitivity specification including par conversion data is as follows:

\begin{longlisting}
  \begin{minted}[fontsize=\footnotesize]{xml}
    <DiscountCurve ccy="EUR">
      <ShiftType>Absolute</ShiftType>
      <ShiftSize>0.0001</ShiftSize>
      <ShiftTenors>2W,1M,3M,6M,9M,1Y,2Y,3Y,4Y,5Y,7Y,10Y,15Y,20Y,25Y,30Y</ShiftTenors>
      <ParConversion>
        <!--DEP, FRA, IRS, OIS, FXF, XBS -->
	<Instruments>OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS,OIS</Instruments>
	<SingleCurve>true</SingleCurve>
	<Conventions>
	  <Convention id="DEP">EUR-EURIBOR-CONVENTIONS</Convention>
	  <Convention id="IRS">EUR-6M-SWAP-CONVENTIONS</Convention>
	  <Convention id="OIS">EUR-OIS-CONVENTIONS</Convention>
	</Conventions>
      </ParConversion>
    </DiscountCurve>   
\end{minted}
\caption{Par sensitivity configuration}
\label{lst:par_sensitivity_config}
\end{longlisting}

Note
\begin{itemize}
\item The list of shift tenors needs to match the list of tenors matches the corresponding grid in the simulation (market) configuration
\item The length of list of (par) instruments needs to match the length of the list of shift tenors
\item Permissible codes for the assumed par instruments:
	\begin{itemize}
	\item DEP, FRA, IRS, OIS, TBS, FXF, XBS in the case of DiscountCurves 
	\item DEP, FRA, IRS, OIS, TBS in the case of IndexCurves
	\item DEP, FRA, IRS, OIS, TBS, XBS in the case of YieldCurves 
	\item ZIS, YYS for YYInflationIndexCurves, interpreted as Year-on-Year Inflation Swaps linked to Zero Inflation resp. YoY Inflation curves
	\item ZIS, YYS for YYCapFloorVolatilities, interpreted as Year-on-Year Inflation Cap Floor linked to Zero Inflation resp. YoY Inflation curves
	\item Any code for CreditCurves, interpreted as CDS
	\item Any code for ZeroInflationIndexCurves, interpreted as CPI Swaps linked to Zero Inflation curves
	\item Any code for CapFloorVolatilities, interpreted as flat Cap/Floor
	\end{itemize}
\item One convention needs to be referenced for each of the instrument codes	
\end{itemize}

%--------------------------------------------------------
\subsection{Stress Scenario Analysis: {\tt stressconfig.xml}}\label{sec:stress}
%--------------------------------------------------------

Stress tests can be applied in ORE to the same market segments and with same granularity as described in the sensitivity section \ref{sec:sensitivity}.

\medskip
This file {\tt stressconfig.xml} specifies how stress tests can be configured. The general structure is shown in listing
\ref{lst:stress_config}.

In this example, two zero stress scenarios ``parallel\_rates'' and ``twist'' and one par rate ``par\_parallel'' are defined. 
Each scenario definition contains
the market components to be shifted in this scenario in a similar syntax that is also used for the sensitivity
configuration, see \ref{sec:sensitivity}. Components that should not be shifted, can just be omitted in the definition
of the scenario. Shifts for rate curves, credit curves and interest rate cap/floors can be given as par or zero rate shifts.
By default shifts are zero rate shifts. If shifts are marked as par rate shifts all components (rate/credit/caps) shifts are 
par shifts in that category, for example it is not possible to have par rate first for one yield curve and zero rate for 
another curve in the same scenario. In case of par stress scenario, the shifted par instruments and related conventions are defined 
in a sensitivity configuration.  The number number stress shifts (tenors/expiries and strikes) need to be allign with
 the tenors/expiries and strikes of par instruments \ref{sec:sensitivity}.

However, instead of specifying one shift size per market component, here a whole vector of shifts can be given, with
different shift sizes applied to each point of the curve (or surface / cube).

In case of the swaption volatility shifts, the single value given as {\tt Shift} (without the attributes {\tt expiry}
and {\tt term}) represents a default value that is used whenever no explicit value is given for a expiry / term pair.

UseSpreadedTermStructures: If set to true, spreaded termstructures over t0 will be used for the scenario calculation, to
improve the alignment of the scenario sim market and t0 curves.

\begin{longlisting}
%\hrule\medskip
  \begin{minted}[fontsize=\scriptsize]{xml}
<StressTesting>
  <UseSpreadedTermStructures>false</UseSpreadedTermStructures>
  <StressTest id="parallel_rates">
    <DiscountCurves>
      <DiscountCurve ccy="EUR">
        <ShiftType>Absolute</ShiftType>
        <ShiftTenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</ShiftTenors>
        <Shifts>0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01</Shifts>
      </DiscountCurve>
      ...
    </DiscountCurves>
    <IndexCurves>
      ...
    </IndexCurves>
    <YieldCurves>
      ...
    </YieldCurves>
    <FxSpots>
      <FxSpot ccypair="USDEUR">
        <ShiftType>Relative</ShiftType>
        <ShiftSize>0.01</ShiftSize>
      </FxSpot>
    </FxSpots>
    <FxVolatilities>
      ...
    </FxVolatilities>
    <SwaptionVolatilities>
      <SwaptionVolatility ccy="EUR">
        <ShiftType>Absolute</ShiftType>
        <ShiftExpiries>1Y,10Y</ShiftExpiries>
        <ShiftTerms>5Y</ShiftTerms>
        <Shifts>
          <Shift>0.0010</Shift>
          <Shift expiry="1Y" term="5Y">0.0010</Shift>
          <Shift expiry="1Y" term="5Y">0.0010</Shift>
          <Shift expiry="1Y" term="5Y">0.0010</Shift>
          <Shift expiry="10Y" term="5Y">0.0010</Shift>
          <Shift expiry="10Y" term="5Y">0.0010</Shift>
          <Shift expiry="10Y" term="5Y">0.0010</Shift>
        </Shifts>
      </SwaptionVolatility>
    </SwaptionVolatilities>
    <CapFloorVolatilities>
      <CapFloorVolatility ccy="EUR">
        <ShiftType>Absolute</ShiftType>
        <ShiftExpiries>6M,1Y,2Y,3Y,5Y,10Y</ShiftExpiries>
        <Shifts>0.001,0.001,0.001,0.001,0.001,0.001</Shifts>
      </CapFloorVolatility>
    </CapFloorVolatilities>
  </StressTest>
  <StressTest id="twist">
    ...
  </StressTest>
  <StressTest id="par_parallel">
    <ParShifts>
      <IRCurves>true</IRCurves>
      <SurvivalProbability>true</SurvivalProbability>
      <CapFloorVolatilities>true</CapFloorVolatilities>
    </ParShifts>
    <DiscountCurves>
      <DiscountCurve ccy="EUR">
        <ShiftType>Absolute</ShiftType>
        <ShiftTenors>6M,1Y,2Y,3Y,5Y,7Y,10Y,15Y,20Y</ShiftTenors>
        <Shifts>0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01</Shifts>
      </DiscountCurve>
      ...
    </DiscountCurves>
    <IndexCurves>
      ...
    </IndexCurves>
    <YieldCurves />
    <FxSpots />
    <FxVolatilities />
    <SwaptionVolatilities />
    <CapFloorVolatilities>
      <CapFloorVolatility key="EUR-EURIBOR-6M">
        <ShiftType>Absolute</ShiftType>
        <ShiftExpiries>1Y, 2Y, 3Y, 4Y, 5Y, 6Y, 7Y, 8Y, 9Y</ShiftExpiries>
        <Shifts>
          <Shift tenor="1Y">0.01</Shift>
          <Shift tenor="2Y">0.01</Shift>
          <Shift tenor="3Y">0.01</Shift>
          <Shift tenor="4Y">0.01</Shift>
          <Shift tenor="5Y">0.01</Shift>
          <Shift tenor="6Y">0.01</Shift>
          <Shift tenor="7Y">0.01</Shift>
          <Shift tenor="8Y">0.01</Shift>
          <Shift tenor="9Y">0.01</Shift>
        </Shifts>
      </CapFloorVolatility>
    </CapFloorVolatilities>
    <EquitySpots />
    <EquityVolatilities />
    <SecuritySpreads />
    <RecoveryRates />
    <SurvivalProbabilities>
      <SurvivalProbability name="Underlying1">
        <ShiftType>Absolute</ShiftType>
        <Shifts>0.01, 0.01, 0.01, 0.01, 0.01</Shifts>
        <ShiftTenors>1Y, 2Y, 3Y, 5Y, 10Y</ShiftTenors>
      </SurvivalProbability>
    </SurvivalProbabilities>
  </StressTest>
</StressTesting>
  \end{minted}
\caption{Stress configuration}
\label{lst:stress_config}
\end{longlisting}

%--------------------------------------------------------
\subsection{Calendar Adjustment: {\tt calendaradjustment.xml}}\label{sec:calendaradjustment}
%--------------------------------------------------------

\medskip
 This file {\tt calendaradjustment.xml} list out all additional holidays and business days that are added to a specified calendar in ORE.
 These dates would originally be missing from the calendar and has to be added.The general structure is shown in listing \ref{lst:calendar_adjustment}.
In this example, two additional dates had been added to the calendar "Japan", one additional holiday and one additional business day. If the user is not certain
wether the date is already included or not, adding it to the {\tt calendaradjustment.xml} to be safe won't raise any errors.
A sample {\tt calendaradjustment.xml} file can be found in the global example input directory. However, it is only used in Example\_1.

\begin{longlisting}
\begin{minted}[fontsize=\scriptsize]{xml}
<CalendarAdjustments>
  <Calendar name="Japan">
    <AdditionalHolidays>
      <Date>2020-01-01</Date>
    </AdditionalHolidays>
    <AdditionalBusinessDays>
      <Date>2020-01-02</Date>
    </AdditionalBusinessDays>
</CalendarAdjustments>
\end{minted}
\caption{Calendar Adjustment}\label{lst:calendar_adjustment}
\end{longlisting}

If the parameter \lstinline!BaseCalendar! is provided then a new calendar will be created using the specified calendar as a base, and adding any \lstinline!AdditionalHolidays! or \lstinline!AdditionalBusinessDays!. In the example below a new calendar \lstinline!CUSTOM_Japan! is being created, it will include any additional holidays or business days specified in the original \lstinline!Japan! calendar plus one additional date.

If a new calendar is added in this way and the schema is being used to validate XML input, the corresponding calendar name must be prefixed with `CUSTOM\_'.

\begin{longlisting}
\hrule\medskip
\begin{minted}[fontsize=\scriptsize]{xml}
<CalendarAdjustments>
  <Calendar name="CUSTOM_Japan">
    <BaseCalendar>Japan</BaseCalendar>
    <AdditionalHolidays>
      <Date>2020-04-06</Date>
    </AdditionalHolidays>
</CalendarAdjustments>
\end{minted}
\caption{Calendar Adjustment creating a new calendar}
\label{lst:calendar_adjustment_2}
\end{longlisting}

\include{parameterisation/curveconfig}

\include{referencedata}

\include{iborfallbackconfig}

\include{simmcalibration}

%--------------------------------------------------------
%\subsection{Conventions: {\tt conventions.xml}}
%\label{sec:conventions}
%--------------------------------------------------------
\include{conventions}


%========================================================
%\section{Trade Data}
%========================================================
\input{tradedata/intro}
\input{tradedata/envelope}
\input{tradedata/nettingsetdetails}
\input{tradedata/tradespecifics}

\input{tradedata/swap}
\input{tradedata/zerocouponswap}
\input{tradedata/capfloor}
\input{tradedata/forwardrateagreement}
\input{tradedata/swaption}
\input{tradedata/callableswap}
\input{tradedata/fxforward}
\input{tradedata/fxswap}
\input{tradedata/fxoption}
\input{tradedata/fx_asianoption}
\input{tradedata/fx_barrieroption}
\input{tradedata/fx_digitalbarrieroption}
\input{tradedata/fx_digitaloption}
\input{tradedata/fx_doublebarrieroption}
\input{tradedata/fx_doubletouchoption}
\input{tradedata/fx_europeanbarrieroption}
\input{tradedata/fx_kikobarrieroption}
\input{tradedata/fx_touchoption}
\input{tradedata/fxvarianceswap}
\input{tradedata/equityoption}
\input{tradedata/equityfuturesoption}
\input{tradedata/equityforward}
\input{tradedata/equityswap}
\input{tradedata/eq_asianoption}
\input{tradedata/eq_barrieroption}
\input{tradedata/eq_digitaloption}
\input{tradedata/eq_doublebarrieroption}
\input{tradedata/eq_doubletouchoption}
\input{tradedata/eq_europeanbarrieroption}
\input{tradedata/eq_touchoption}
\input{tradedata/equityvarianceswap}
\input{tradedata/equitycliquetoption}
\input{tradedata/equityposition}
\input{tradedata/equityoptionposition}

\input{tradedata/cpiswap}
\input{tradedata/yyswap}

\input{tradedata/bond}
\input{tradedata/bondposition}
\input{tradedata/forwardbond}
\input{tradedata/bondForward_refdata}
\input{tradedata/bondrepo}
\input{tradedata/bondoption}
\input{tradedata/bondoption_refdata}
\input{tradedata/bondTotalReturnSwap}
\input{tradedata/convertiblebond}
\input{tradedata/ascot}
\input{tradedata/cbodata.tex}

\input{tradedata/compositetrade}

\input{tradedata/creditdefaultswap}
\input{tradedata/indexcds}
\input{tradedata/indexcdsoption}
\input{tradedata/syntheticcdo}
\input{tradedata/creditlinkedswap}

\input{tradedata/commodityforward}
\input{tradedata/commodityswap}
\input{tradedata/commodityswaption}
\input{tradedata/commodityoption}
\input{tradedata/commodityapo}
\input{tradedata/commodityoptionstrip}
\input{tradedata/commodityvarianceswap}
\input{tradedata/commodityposition}

\input{tradedata/totalReturnSwap}

\input{tradedata/eq_outperformance.tex}
\input{tradedata/doubledigitaloption}
\input{tradedata/europeanoptioncontingentonabarrier}
\input{tradedata/autocallable_01}
\input{tradedata/performanceoption_01}
\input{tradedata/window_barrieroption}
\input{tradedata/generic_barrieroption}
\input{tradedata/bestentryoption}
\input{tradedata/basketoption}
\input{tradedata/worstofbasketswap}
\input{tradedata/rainbow_option}
\input{tradedata/var_and_vol_derivatives}
\input{tradedata/accumulator}
\input{tradedata/tarf}
\input{tradedata/knockoutswap}

\input{tradedata/flexiswap}
\input{tradedata/balanceguaranteedswap}

%\include{tradecomponents}
\input{tradecomponents/tradecomponentsintro}
\input{tradecomponents/optiondata}
\input{tradecomponents/premiums}
\input{tradecomponents/legdatanotionals}
\input{tradecomponents/scheduledata}
\input{tradecomponents/fixedlegdatarates}
\input{tradecomponents/floatinglegdata}
\input{tradecomponents/legdataamortisation}
\input{tradecomponents/indexings}
\input{tradecomponents/cashflowleg}
\input{tradecomponents/cmsleg}
\input{tradecomponents/cmbleg}
\input{tradecomponents/digitalcmsleg}
\input{tradecomponents/durationadjustedcmsleg}
\input{tradecomponents/cmsspreadleg}
\input{tradecomponents/digitalcmsspreadleg}
\input{tradecomponents/equityleg}
\input{tradecomponents/cpileg}
\input{tradecomponents/yyleg}
\input{tradecomponents/zerocouponleg}
\input{tradecomponents/commodityfixedleg}
\input{tradecomponents/commodityfloatingleg}
\input{tradecomponents/equitymarginleg}
\input{tradecomponents/cdsreferenceinformation}
\input{tradecomponents/basketdata}
\input{tradecomponents/underlying}
\input{tradecomponents/strikedata}
\input{tradecomponents/barrierdata}
\input{tradecomponents/rangebound}
\input{tradecomponents/bondbasketdata.tex}
\input{tradecomponents/cbotranches.tex}
\input{tradecomponents/formulabasedlegdata}

\include{allowablevalues}

%========================================================
%\section{Netting Set Definitions}\label{sec:nettingsetinput}
%========================================================
\include{nettingdata}

%========================================================
%\section{Market Data}\label{sec:market_data}
%========================================================
\include{marketdata}

%========================================================
%\section{Fixing History}
%========================================================
\include{fixingdata}

%========================================================
%\section{Dividend History}
%========================================================
\include{dividenddata}

\newpage
\begin{appendix}

%========================================================
\section{Methodology Summary}
%========================================================

\subsection{Risk Factor Evolution Model}\label{sec:app_rfe}

ORE applies the cross asset model described in detail in \cite{Lichters} to evolve  the market through time. So far the
evolution model in ORE supports IR and FX risk factors for any number of currencies, Equity and Inflation as well as Credit. Extensions to full simulation of Commodity is planned. \\

The Cross Asset Model is based on the Linear Gauss Markov model (LGM) for interest rates, lognormal FX and equity 
processes, Dodgson-Kainth model for inflation, LGM or Extended Cox-Ingersoll-Ross model (CIR++) for credit, and a single-factor log-normal model for commodity curves.
We identify a single {\em domestic} currency; its LGM process,
which is labelled $z_0$; and a set of $n$ foreign currencies with associated LGM processes that are labelled $z_i$, 
$i=1,\dots,n$. 

We denote the equity spot price processes with state variables $s_j$ and the index of the denominating 
currency for the equity process as $\phi(j)$. The dividend yield corresponding to each equity process $s_j$ is denoted 
by $q_j$.

Following \cite{Lichters}, 13.27 - 13.29 we write the inflation processes 
in the domestic LGM measure with state variables $z_{I,k}$ and $y_{I,k}$ for $k=1,\ldots,K$
and the credit processes in the domestic LGM measure with state variables ${C,k}$ and $y_{C,k}$ for $k=1,\ldots,K$.
If we consider $n$ 
foreign exchange rates for converting foreign currency amounts into the single domestic currency by multiplication, 
$x_i$, $i=1,\dots,n$, then the cross asset model is given by the system of SDEs
\begin{eqnarray*}
dz_0 &=& \alpha_0\,dW_0^z \\
dz_i &=& \gamma_i\,dt + \alpha_i\,dW_i^z,  \qquad i>0 \\
\frac{d x_i}{x_i} &=& \mu_i\, dt + \sigma_i\,dW_i^x, \qquad i > 0 \\
\frac{d s_j}{s_j} &=& \mu_j^S\, dt + \sigma_j^S\,dW_j^S \\
dz_{I,k} &=& \alpha_{I,k}(t)dW_k^I \\
dy_{I,k} &=& \alpha_{I,k}(t)H_{I,k}(t)dW_k^I \\
dz_{C,k} &=& \alpha_{C,k}(t)dW_k^C \\
dy_{C,k} &=& H_{C,k}(t)\alpha_{C,k}(t)dW_k^C \\ \\
\gamma_i &=&
-\alpha_i^2\,H_i -\rho_{ii}^{zx}\,\sigma_i\,\alpha_i + \rho_{i0}^{zz}\,\alpha_i\,\alpha_0\,H_0\\
\mu_i &=& r_0 - r_i + \rho_{0i}^{zx}\,\alpha_0\,H_0\,\sigma_i\\
\mu_j^S &=& (r_{\phi(j)}(t) - q_j(t) + \rho_{0j}^{zs} \alpha_0 H_0 \sigma_j^S - \epsilon_{\phi(j)}
\rho_{j \phi(j)}^{sx}\sigma_j^S \sigma_{\phi(j)}) \\
r_i &=& f_i(0,t) + z_i(t)\,H'_i(t) + \zeta_i(t)\,H_i(t)\,H'_i(t),
\quad \zeta_i(t) = \int_0^t \alpha_i^2(s)\,ds  \\ \\
dW^\alpha_a\,dW^\beta_b &=& \rho^{\alpha\beta}_{ij}\,dt, \qquad \alpha, \beta \in \{z, x, I, C\}, \qquad a, b \text{
                              suitable indices }
%\zeta_i(t) &=& \int_0^t \alpha_i^2(s)\,ds,
%\qquad H_i(t) = \int_0^t e^{-\beta_i(s)} \,ds \\
%\beta_i(t) &=& \int_0^t \lambda_i(s)\,ds,
%\qquad \alpha_i(t) = \sigma_i^{HW}(t)\,e^{\beta(t)} \\
\end{eqnarray*}
where we have dropped time dependencies for readability, $f_i(0,t)$ is the instantaneous forward curve in currency $i$, 
and $\epsilon_i$ is an indicator such that $\epsilon_i = 1 - \delta_{0i}$, where $\delta$ is the Kronecker delta.

\medskip Parameters $H_i(t)$ and $\alpha_i(t)$ (or alternatively $\zeta_i(t)$) are LGM model parameters which determine,
together with the stochastic factor $z_i(t)$, the evolution of numeraire and zero bond prices in the LGM model:
\begin{align}
N(t) &= \frac{1}{P(0,t)}\exp\left\{H_t\, z_t + \frac{1}{2}H^2_t\,\zeta_t \right\}
\label{lgm1f_numeraire} \\
P(t,T,z_t)
&= \frac{P(0,T)}{P(0,t)}\:\exp\left\{ -(H_T-H_t)\,z_t - \frac{1}{2} \left(H^2_T-H^2_t\right)\,\zeta_t\right\}.
\label{lgm1f_zerobond}
\end{align}

Note that the LGM model is closely related to the Hull-White model in T-forward measure \cite{Lichters}.

\medskip The parameters $H_{I,k}(t)$ and $\alpha_{I,k}(t)$ determine together with the factors $z_{I,k}(t), y_{I,k}(t)$
the evolution of the spot Index $I(t)$ and the forward index $\hat{I}(t,T) = P_I(t,T) / P_n(t,T)$ defined as the ratio
of the inflation linked zero bond and the nominal zero bond,

\begin{eqnarray*}
  \hat{I}(t,T) &=& \frac{\hat{I}(0,T)}{\hat{I}(0,t)} e^{(H_{I,k}(T)-H_{I,k}(t))z_{I,k}(t)+\tilde{V}(t,T)} \\
  I(t) &=& I(0) \hat{I}(0,t)e^{H_{I,k}(t)z_{I,k}(t)-y_{I,k}(t)-V(0,t)}
\end{eqnarray*}

with, in case of domestic currency inflation,

\begin{eqnarray*}
  V(t,T) &=& \frac{1}{2} \int_t^T (H_{I,k}(T)-H_{I,k}(s))^2 \alpha_{I,k}^2(s) ds \\
         & & - \rho^{zI}_{0,k} H_0(T) \int_t^T (H_{I,k}(t)-H_{I,k}(s))\alpha_0(s)\alpha_{I,k}(s)ds \\
  \tilde{V}(t,T) &=& V(t,T) - V(0,T) -V(0,t) \\
         &=& -\frac{1}{2}(H_{I,k}^2(T)-H_{I,k}^2(t))\zeta_{I,k}(t,0) \\
         & & +(H_{I,k}(T)-H_{I,k}(t)) \zeta_{I,k}(t,1) \\
         & & +(H_0(T)H_{I,k}(T) - H_0(t)H_{I,k}(t))\zeta_{0I}(t,0) \\
         & & -(H_0(T)-H_0(t))\zeta_{0I}(t,1) \\
  V(0,t) &=& \frac{1}{2}H_{I,k}^2(t)\zeta_{I,k}(t,0)-H_{I,k}(t)\zeta_{I,k}(t,1)+\frac{1}{2}\zeta_{I,k}(t,2) \\
         & & -H_0(t)H_{I,k}(t)\zeta_{0I}(t,0)+H_0(t)\zeta_{0I}(t,1) \\
  \zeta_{I,k}(t,k) &=& \int_0^t H_{I,k}^k(s)\alpha_{I,k}^2(s) ds \\
  \zeta_{0I}(t,k) &=& \rho^{zI}_{0,k}\int_0^t H_{I,k}^k(t) \alpha_0(s) \alpha_{I,k}(s) ds
\end{eqnarray*}

and for foreign currency inflation in currency $i>0$, with

\begin{eqnarray*}
  \tilde{V}(t,T) &=& V(t,T) -V(0,T) + V(0,T)
\end{eqnarray*}

and

\begin{eqnarray*}
  V(t,T) &=& \frac{1}{2}\int_t^T (H_{I,k}(T)-H_{I,k}(s))^2 \alpha_{I,k}(s) ds \\
  & & -\rho^{zI}_{0,k} \int_t^T H_0(s)\alpha_0(s)(H_{I,k}(T)-H_{I,k}(s)\alpha_{I,k}(s)) ds \\
  & & -\rho^{zI}_{i,k} \int_t^T (H_i(T)-H_i(s))\alpha_i(s)(H_{I,k}(T)-H_{I,k}(s))\alpha_{I,k}(s) ds \\
  & & +\rho^{xI}_{i,k} \int_t^T \sigma_i(s)(H_{I,k}(T)-H_{I,k}(s))\alpha_{I,k}(s) ds
\end{eqnarray*}

\subsubsection*{Commodity}

Each commodity component models the commodity price curve as 
\begin{eqnarray}
\frac{dF(t,T)}{F(t,T)} &=& \sigma\,e^{-\kappa\,(T-t)}\, dW(t)  \label{gabillon1f}
\end{eqnarray}
which is a single-factor version of the Gabillon (1991) model that is e.g. described in \cite{Lichters}. It can also be seen as the Schwartz (1997) model formulated in terms of forward curve dynamics. The extension to the full Gabillon model with two factors and time-dependent multiplier
\begin{eqnarray}
\frac{dF(t,T)}{F(t,T)} &=& \alpha(t)\,
\left( \sigma_S \,e^{-\kappa\,(T-t)}\, dW_S(t) + \sigma_L\,\left(1-e^{-\kappa\,(T-t)}\right)\,dW_L(t)\right) \label{gabillon2f}
\end{eqnarray}
for richer dynamics of the curve and accurate calibration to options will follow. 

The commodity components' Wiener processes can be correlated. However, the integration of commodity components into the overall CAM assumes zero correlations between commodities and non-commodity drivers for the time being.

To propagate the one-factor model, we can use an artificial (Ornstein-Uhlenbeck) spot price process
\begin{align*}
dX(t) &= -\kappa\,X(t)\,dt + \sigma(t)\,dW(t), \qquad X(0)=0\\
X(t) &= X(s)\,e^{-\kappa(t-s)}+ \int_s^t \sigma\,e^{-\kappa(t-u)}\, dW(u)
\end{align*}
with 
\begin{align*}
F(t,T) &= F(0,T) \:\exp\left( X(t)\,e^{-\kappa\,(T-t)} - \frac{1}{2}\,(V(0,T)-V(t,T))  \right) \\
V(t,T) &= e^{-2\kappa T}\int_t^T\sigma^2\:e^{2\kappa u}\,du.
\end{align*}
Note that 
$$
\V[\ln F(T,T)] = \V[X(T)] 
$$
is the variance that is used in the pricing of a Futures Option which in turn is used in the calibration of the Schwartz model.

Alternatively, one can use the drift-free state variable $Y(t)=e^{\kappa t} X(t)$ with
\begin{align*}
dY(t) &= \sigma \: e^{\kappa \, t} \, dW(t).
\end{align*}
Both choices of state dynamics are possible in ORE. 

\subsection{Analytical Moments of the Risk Factor Evolution Model}\label{sec:app_analytical_moments}

We follow \cite{Lichters}, chapter 16. The expectation of the interest rate process $z_i$ conditional on $\mathcal{F}_{t_0}$ at $t_0+\Delta t$ is

\begin{eqnarray*}
  \mathbb{E}_{t_0}[z_i(t_0+\Delta t)] &=& z_i(t_0) + \mathbb{E}_{t_0}[\Delta z_i],
  \qquad\mbox{with}\quad \Delta z_i = z_i(t_0+\Delta t) - z_i(t_0) \\
  &=& z_i(t_0) -\int_{t_0}^{t_0+\Delta t} H^z_i\,(\alpha^z_i)^2\,du + \rho^{zz}_{0i} \int_{t_0}^{t_0+\Delta t}
  H^z_0\,\alpha^z_0\,\alpha^z_i\,du \\
  & & - \epsilon_i  \rho^{zx}_{ii}\int_{t_0}^{t_0+\Delta t} \sigma_i^x\,\alpha^z_i\,du
\end{eqnarray*}

where $\epsilon_i$ is zero for $i=0$ (domestic currency) and one otherwise.

\bigskip

The expectation of the FX process $x_i$ conditional on $\mathcal{F}_{t_0}$ at $t_0+\Delta t$ is

\begin{eqnarray*}
  \mathbb{E}_{t_0}[\ln x_i(t_0+\Delta t)] &=& \ln x_i(t_0) +  \mathbb{E}_{t_0}[\Delta \ln x_i],
  \qquad\mbox{with}\quad \Delta \ln x_i = \ln x_i(t_0+\Delta t) - \ln x_i(t_0) \\
  &=& \ln x_i(t_0) + \left(H^z_0(t)-H^z_0(s)\right) z_0(s) -\left(H^z_i(t)-H^z_i(s)\right)z_i(s)\\
  &&+ \ln \left( \frac{P^n_0(0,s)}{P^n_0(0,t)} \frac{P^n_i(0,t)}{P^n_i(0,s)}\right) \\
  && - \frac12 \int_s^t (\sigma^x_i)^2\,du \\
  &&+\frac12 \left((H^z_0(t))^2 \zeta^z_0(t) -  (H^z_0(s))^2 \zeta^z_0(s)- \int_s^t (H^z_0)^2
  (\alpha^z_0)^2\,du\right)\\
  &&-\frac12 \left((H^z_i(t))^2 \zeta^z_i(t) -  (H^z_i(s))^2 \zeta^z_i(s)-\int_s^t (H^z_i)^2 (\alpha^z_i)^2\,du
  \right)\\
  && + \rho^{zx}_{0i} \int_s^t H^z_0\, \alpha^z_0\, \sigma^x_i\,du \\
  &&  - \int_s^t \left(H^z_i(t)-H^z_i\right)\gamma_i \,du, \qquad\mbox{with}\quad s = t_0, \quad t = t_0+\Delta t
\end{eqnarray*}

with

\begin{eqnarray*}
  \gamma_i = -H^z_i\,(\alpha^z_i)^2  + H^z_0\,\alpha^z_0\,\alpha^z_i\,\rho^{zz}_{0i} - \sigma_i^x\,\alpha^z_i\,
  \rho^{zx}_{ii}
\end{eqnarray*}

The expectation of the Inflation processes $z_{I,k}, y_{I,k}$ conditional on $\mathcal{F}_{t_0}$ at any time $t>t_0$ is
equal to $z_{I,k}(t_0)$ resp. $y_{I,k}(t_0)$ since both processes are drift free.

\bigskip

The expectation of the equity processes $s_j$ conditional on $\mathcal{F}_{t_0}$ at $t_0+\Delta t$ is
\begin{eqnarray*}
\mathbb{E}_{t_0}[\ln s_j(t_0+\Delta t)] &=& \ln s_j(t_0) +  \mathbb{E}_{t_0}[\Delta \ln s_j],
\qquad\mbox{with}\quad \Delta \ln s_j = \ln s_j(t_0+\Delta t) - \ln s_j(t_0) \\
&=& \ln s_j(t_0) +  \ln \left[\frac{P_{\phi(j)}(0,s)}{P_{\phi(j)}(0,t)} \right] - \int_s^t 
q_j(u) 
du - \frac{1}{2} \int_s^t \sigma_{j}^{S}(u) \sigma_{j}^{S}(u) du\\
&&
+\rho_{0j}^{zs} \int_s^t \alpha_0(u) H_0(u) \sigma_j^S(u) du
- \epsilon_{\phi(j)} \rho_{j \phi(j)}^{sx} \int_s^t \sigma_j^S (u)\sigma_{\phi(j)}(u) du\\
&&+\frac{1}{2} \left( H_{\phi(j)}^2(t) \zeta_{\phi(j)}(t) - H_{\phi(j)}^2(s) \zeta_{\phi(j)}(s)
- \int_s^t H_{\phi(j)}^2(u) \alpha_{\phi(j)}^2(u) du \right)\\
&&  + (H_{\phi(j)}(t) - H_{\phi(j)}(s)) z_{\phi(j)}(s) 
+\epsilon_{\phi(j)} \int_s^t \gamma_{\phi(j)} (u) (H_{\phi(j)}(t) - H_{\phi(j)}(u)) du\\
\end{eqnarray*}

The IR-IR covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
      \mathrm{Cov} [\Delta z_a, \Delta \ln x_b] &=& \rho^{zz}_{0a}\int_s^t \left(H^z_0(t)-H^z_0\right)
  \alpha^z_0\,\alpha^z_a\,du \nonumber\\
      &&- \rho^{zz}_{ab}\int_s^t \alpha^z_a \left(H^z_b(t)-H^z_b\right) \alpha^z_b \,du \nonumber\\
      &&+\rho^{zx}_{ab}\int_s^t \alpha^z_a \, \sigma^x_b \,du.
\end{eqnarray*}

The IR-FX covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
      \mathrm{Cov} [\Delta z_a, \Delta \ln x_b] &=& \rho^{zz}_{0a}\int_s^t \left(H^z_0(t)-H^z_0\right)
  \alpha^z_0\,\alpha^z_a\,du \nonumber\\
      &&- \rho^{zz}_{ab}\int_s^t \alpha^z_a \left(H^z_b(t)-H^z_b\right) \alpha^z_b \,du \nonumber\\
      &&+\rho^{zx}_{ab}\int_s^t \alpha^z_a \, \sigma^x_b \,du.
\end{eqnarray*}

The FX-FX covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
      \mathrm{Cov}[\Delta \ln x_a, \Delta \ln x_b] &=&
      \int_s^t \left(H^z_0(t)-H^z_0\right)^2 (\alpha_0^z)^2\,du \nonumber\\
      && -\rho^{zz}_{0a} \int_s^t \left(H^z_a(t)-H^z_a\right) \alpha_a^z\left(H^z_0(t)-H^z_0\right) \alpha_0^z\,du
  \nonumber\\
      &&- \rho^{zz}_{0b}\int_s^t \left(H^z_0(t)-H^z_0\right)\alpha_0^z \left(H^z_b(t)-H^z_b\right)\alpha_b^z\,du
  \nonumber\\
      &&+ \rho^{zx}_{0b}\int_s^t \left(H^z_0(t)-H^z_0\right)\alpha_0^z \sigma^x_b\,du \nonumber\\
      &&+ \rho^{zx}_{0a}\int_s^t \left(H^z_0(t)-H^z_0\right)\alpha_0^z\,\sigma^x_a\,du \nonumber\\
      &&- \rho^{zx}_{ab}\int_s^t \left(H^z_a(t)-H^z_a\right)\alpha_a^z \sigma^x_b,du\nonumber\\
      &&- \rho^{zx}_{ba}\int_s^t \left(H^z_b(t)-H^z_b\right)\alpha_b^z\,\sigma^x_a\, du \nonumber\\
      &&+ \rho^{zz}_{ab}\int_s^t \left(H^z_a(t)-H^z_a\right)\alpha_a^z \left(H^z_b(t)-H^z_b\right)\alpha_b^z\,du
  \nonumber\\
      &&+ \rho^{xx}_{ab}\int_s^t\sigma^x_a\,\sigma^x_b \,du
\end{eqnarray*}

The IR-INF covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
  \mathrm{Cov}[ \Delta z_a, \Delta z_{I,b} ] & = & \rho_{ab}^{zI} \int_s^t \alpha_a(s) \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta z_a, \Delta y_{I,b} ] & = & \rho_{ab}^{zI} \int_s^t \alpha_a(s) H_{I,b}(s) \alpha_{I,b}(s) ds
\end{eqnarray*}

The FX-INF covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
  \mathrm{Cov}[ \Delta x_a, \Delta z_{I,b} ] & = & \rho_{0b}^{zI} \int_s^t \alpha_0(s) (H_0(t)-H_0(s)) \alpha_{I,b}(s) ds \\
                                             & & -\rho_{ab}^{zI} \int_s^t \alpha_a(s)(H_a(t)-H_a(s))\alpha_{I,b}(s) ds \\
                                             & & +\rho_{ab}^{xI}\int_s^t \sigma_a(s) \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta x_a, \Delta y_{I,b} ] & = & \rho_{0b}^{zI} \int_s^t \alpha_0(s) (H_0(t)-H_0(s)) H_{I,b}(s)\alpha_{I,b}(s) ds \\
                                             & & -\rho_{ab}^{zI} \int_s^t \alpha_a(s)(H_a(t)-H_a(s))H_{I,b}(s)\alpha_{I,b}(s) ds \\
                                             & & +\rho_{ab}^{xI}\int_s^t \sigma_a(s) H_{I,b}(s)\alpha_{I,b}(s) ds
\end{eqnarray*}

The INF-INF covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is

\begin{eqnarray*}
  \mathrm{Cov}[ \Delta z_{I,a}, \Delta z_{I,b} ] & = & \rho_{ab}^{II} \int_s^t \alpha_{I,a}(s) \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta z_{I,a}, \Delta y_{I,b} ] & = & \rho_{ab}^{II} \int_s^t \alpha_{I,a}(s) H_{I,b}(s)
                                                       \alpha_{I,b}(s) ds \\
  \mathrm{Cov}[ \Delta y_{I,a}, \Delta y_{I,b} ] & = & \rho_{ab}^{II} \int_s^t H_{I,a}(s) \alpha_{I,a}(s) H_{I,b}(s) \alpha_{I,b}(s) ds
\end{eqnarray*}

The equity/equity covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is
\begin{eqnarray*}
	Cov \left[\Delta ln[s_i], \Delta ln[s_j] \right] &=&
	\rho_{\phi(i) \phi(j)}^{zz}\int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) (H_{\phi(j)} (t)\\
	&& - H_{\phi(j)} (u)) \alpha_{\phi(i)}(u) \alpha_{\phi(j)}(u) du\\
	&&+ \rho_{\phi(i) j}^{zs} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)}(u) \sigma_j^S(u) du\\
	&&+ \rho_{\phi(j) i}^{zs} \int_s^t (H_{\phi(j)} (t) - H_{\phi(j)} (u)) \alpha_{\phi(j)}(u) \sigma_i^S(u) du\\
	&&+ \rho_{ij}^{ss} \int_s^t \sigma_i^S(u) \sigma_j^S(u) du\\
\end{eqnarray*}

The equity/FX covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is
\begin{eqnarray*}
	Cov \left[\Delta ln[s_i], \Delta ln[x_j] \right] &=&
	\rho_{\phi(i)0}^{zz} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) (H_0 (t) - H_0 (u)) \alpha_{\phi(i)}(u) 
	\alpha_0(u) 
	du\\
	&& - \rho_{\phi(i)j}^{zz} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) (H_j (t) - H_j (u)) \alpha_{\phi(i)} 
	(u)\alpha_j(u) du\\
	&& + \rho_{\phi(i)j}^{zx} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)} (u) \sigma_j(u) du\\
	&&+ \rho_{i0}^{sz} \int_s^t (H_0 (t) - H_0 (u)) \alpha_0 (u) \sigma_i^S(u) du\\
	&&- \rho_{ij}^{sz} \int_s^t (H_j (t) - H_j (u)) \alpha_j (u) \sigma_i^S(u) du\\
	&&+ \rho_{ij}^{sx} \int_s^t \sigma_i^S(u) \sigma_j(u) du\\
\end{eqnarray*}

The equity/IR covariance over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) is
\begin{eqnarray*}
	Cov \left[\Delta ln[s_i], \Delta z_j \right] &=&
	\rho_{\phi(i)j}^{zz} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)} (u) \alpha_j (u) du\\
	&&+ \rho_{ij}^{sz} \int_s^t \sigma_i^S (u) \alpha_j (u) du\\
\end{eqnarray*}

The equity/inflation covariances over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) are as follows:
\begin{eqnarray*}
	Cov \left[\Delta ln[s_i], \Delta z_{I,j} \right] &=&
	\rho_{\phi(i)j}^{zI} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)} (u) \alpha_{I,j} (u) du\\
	&&+ \rho_{ij}^{sI} \int_s^t \sigma_i^S (u) \alpha_{I,j} (u) du\\	
	Cov \left[\Delta ln[s_i], \Delta y_{I,j} \right] &=&
	\rho_{\phi(i)j}^{zI} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)} (u) H_{I,j} (u) \alpha_{I,j} (u) du\\
	&&+ \rho_{ij}^{sI} \int_s^t \sigma_i^S (u) H_{I,j} (u) \alpha_{I,j} (u) du\\
\end{eqnarray*}

The expectation of the Credit processes $z_{C,k}, y_{C,k}$ conditional on $\mathcal{F}_{t_0}$ at any time $t>t_0$ is
equal to $z_{C,k}(t_0)$ resp. $y_{C,k}(t_0)$ since both processes are drift free.

The credit/credit covariances over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) are as follows:
\begin{eqnarray*}
	Cov \left[\Delta z_{C,a}, \Delta z_{C,b} \right] &=&
	\rho_{ab}^{CC}\int_s^t \alpha_{C, a}(u) \alpha_{C, b}(u) du\\
  Cov \left[\Delta z_{C,a}, \Delta y_{C,b} \right] &=&
	\rho_{ab}^{CC}\int_s^t \alpha_{C, a}(u) H_{C,b}(u) \alpha_{C, b}(u) du\\
  Cov \left[\Delta y_{C,a}, \Delta y_{C,b} \right] &=&
	\rho_{ab}^{CC}\int_s^t \alpha_{C, a}(u) H_{C,a}(u) \alpha_{C, b}(u) H_{C,b}(u) du\\
\end{eqnarray*}

The IR/credit covariances over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) are as follows:
\begin{eqnarray*}
	Cov \left[\Delta z_a, \Delta z_{C,b} \right] &=&
	\rho_{ab}^{zC}\int_s^t \alpha_a(u) \alpha_{C, b}(u) du\\
  Cov \left[\Delta z_a, \Delta y_{C,b} \right] &=&
	\rho_{ab}^{zC}\int_s^t \alpha_a(u) H_{C,b}(u) \alpha_{C, b}(u) du\\
\end{eqnarray*}

The FX/credit covariances over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) are as follows:
\begin{eqnarray*}
  \mathrm{Cov}[ \Delta x_a, \Delta z_{C,b} ] & = & \rho_{0b}^{zC} \int_s^t \alpha_0(s) (H_0(t)-H_0(s)) \alpha_{C,b}(s) ds \\
                                             & & -\rho_{ab}^{zC} \int_s^t \alpha_a(s)(H_a(t)-H_a(s))\alpha_{C,b}(s) ds \\
                                             & & +\rho_{ab}^{xC}\int_s^t \sigma_a(s) \alpha_{C,b}(s) ds \\
  \mathrm{Cov}[ \Delta x_a, \Delta y_{C,b} ] & = & \rho_{0b}^{zC} \int_s^t \alpha_0(s) (H_0(t)-H_0(s)) H_{C,b}(s)\alpha_{C,b}(s) ds \\
                                             & & -\rho_{ab}^{zC} \int_s^t \alpha_a(s)(H_a(t)-H_a(s))H_{C,b}(s)\alpha_{C,b}(s) ds \\
                                             & & +\rho_{ab}^{xC}\int_s^t \sigma_a(s) H_{C,b}(s)\alpha_{C,b}(s) ds
\end{eqnarray*}

The inflation/credit covariances over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) are as follows:
\begin{eqnarray*}
  \mathrm{Cov}[ \Delta z_{I,a}, \Delta z_{C,b} ] &=&
  \rho_{ab}^{IC}\int_s^t \alpha_{I,a} \alpha_{C,b}(u) du\\
  \mathrm{Cov}[ \Delta z_{I,a}, \Delta y_{C,b} ] &=&
  \rho_{ab}^{IC}\int_s^t \alpha_{I,a} H_{C,b}(u) \alpha_{C,b}(u) du\\
  \mathrm{Cov}[ \Delta y_{I,a}, \Delta z_{C,b} ] &=&
  \rho_{ab}^{IC}\int_s^t \alpha_{I,a} H_{I,a}(u) \alpha_{C,b}(u) du\\
  \mathrm{Cov}[ \Delta y_{I,a}, \Delta y_{C,b} ] &=&
  \rho_{ab}^{IC}\int_s^t \alpha_{I,a} H_{I,a}(u) \alpha_{C,b}(u) H_{C,b}(u) du\\
\end{eqnarray*}

The equity/credit covariances over the interval $[s,t] := [t_0, t_0+\Delta t]$ (conditional on $\mathcal{F}_{t_0}$) are as follows:
\begin{eqnarray*}
	Cov \left[\Delta ln[s_i], \Delta z_{C,j} \right] &=&
	\rho_{\phi(i)j}^{zC} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)} (u) \alpha_{C,j} (u) du\\
	&&+ \rho_{ij}^{sC} \int_s^t \sigma_i^S (u) \alpha_{C,j} (u) du\\	
	Cov \left[\Delta ln[s_i], \Delta y_{C,j} \right] &=&
	\rho_{\phi(i)j}^{zC} \int_s^t (H_{\phi(i)} (t) - H_{\phi(i)} (u)) \alpha_{\phi(i)} (u) H_{C,j} (u) \alpha_{C,j} (u) du\\
	&&+ \rho_{ij}^{sC} \int_s^t \sigma_i^S (u) H_{C,j} (u) \alpha_{C,j} (u) du\\
\end{eqnarray*}

\subsection{Change of Measure}

We can change measure from LGM to the T-Forward measure by applying a shift transformation to the $H$ parameter of the domestic LGM process, as explained in \cite{Lichters} and shown in Example 12, section \ref{sec:longterm}. This does not involve amending the system of SDEs above.

\medskip
\noindent
In the following we show how to move from the LGM to the Bank Account measure when we start with the Cross Asset Model in the LGM measure. This description and the implementation in ORE is limited so far to the cross currency case.

First note that the stochastic Bank Account (BA) can be written
\begin{align*}
B(t) &= \frac{1}{P(0,t)}\exp\left(\int_0^t (H_t-H_s)\,\alpha_s\,dW_s^B + \frac{1}{2}\int_0^t (H_t-H_s)^2\,\alpha^2_s\,ds \right)
\end{align*} 
with Wiener processes in the BA measure. We can express this in terms of the domestic LGM's state variable $z(t)$ and an auxiliary random variable $y(t)$
\begin{align*}
B(t) &= \frac{1}{P(0,t)}\exp\left(H(t)\,z(t) - y(t) + \frac{1}{2} \left(H^2(t)\,\zeta_0(t) + \zeta_2(t)\right)\right)
\intertext{with}
dz(t) &= \alpha(t)\,dW^B(t) - H(t)\,\alpha^2(t)\,dt \\
dy(t) &= H(t)\,\alpha(t)\,dW^B(t) \\
\zeta_n(t) &= \int_0^t \alpha^2(s)\,H^n(s) \,ds
\end{align*}
Note the drift of LGM state variable $z(t)$ in the BA measure and the auxiliary state variable $y(t)$ which is driven by the same Wiener process as $z(t)$. The instantaneous correlation of $dz$ and $dy$ is one, but the terminal correlation of $z(t)$ and $y(t)$ is less than one because of their different volatility functions. This is all we need to switch measure to BA in a pure domestic currency case.

To change measure in the cross currency case we need to make changes to the SDE beyond adding an auxiliary state variable $y$ and adding a drift to the domestic LGM state. Let us write down the SDEs in the LGM and BA measure with respective drift terms that ensure martingale properties.

SDE in the LGM measure
\begin{align*}
dz_0 &= \alpha_0\,dW_0^z \\
dz_i &= \left(-\alpha_i^2\,H_i -\rho_{ii}^{zx}\,\sigma_i\,\alpha_i + {\color{red} \rho_{i0}^{zz}\,\alpha_i\,\alpha_0\,H_0}\right)\,dt + \alpha_i\,dW_i^z \\
d\ln x_i &= \left(r_0 - r_i - \frac{1}{2}\sigma^2_i + {\color{red} \rho_{0i}^{zx}\,\alpha_0\,H_0\,\sigma_i} \right)\, dt + \sigma_i\,dW_i^x \\
\intertext{SDE in the BA measure}
{\color{blue}dy_0}  & = {\color{blue}\alpha_0\,H_0\,d\widetilde W_0^z} \\
dz_0 &= {\color{blue}-\alpha_0^2\,H_0\,dt} + \alpha_0\,d\widetilde W_0^z \\
dz_i &= \left(-\alpha_i^2\,H_i-\rho_{ii}^{zx}\,\sigma_i\,\alpha_i\right)\,dt + \alpha_i\,d\widetilde W_i^z \\
d\ln x_i &= \left(r_0 - r_i - \frac{1}{2}\sigma^2_i\right)\, dt + \sigma_i\,d\widetilde W_i^x,\qquad 
r_i = f_i(0,t) + z_i(t)\,H'_i(t) + \zeta_i(t)\,H_i(t)\,H'_i(t)
\end{align*}

Blue terms are {\color{blue}added}, red terms are {\color{red}removed} when moving from LGM to BA.

\medskip\noindent

These drift term changes lead to the following changes in conditional expectations 
\begin{align*}
\E[\Delta y_0] =& 0 \\
\E[\Delta z_0] =& - {\color{blue}\int_s^t H_0\,\alpha_0^2\,du}  \\
\E[\Delta z_i] =& - \int_s^t H_i\,\alpha_i^2\,du 
  - \rho^{zx}_{ii}\int_s^t \sigma_i^x\,\alpha_i\,du
  + {\color{red}\rho^{zz}_{0i} \int_s^t H_0\,\alpha_0\,\alpha_i\,du } \\
\E[\Delta \ln x] 
  =& \left(H_0(t)-H_0(s)\right) z_0(s) -\left(H_i(t)-H_i(s)\right)\,z_i(s)\\
  &+ \ln \left( \frac{P^n_0(0,s)}{P^n_0(0,t)} \frac{P^n_i(0,t)}{P^n_i(0,s)}\right) \\
  & - \frac12 \int_s^t (\sigma^x_i)^2\,du \\
  &+\frac12 \left(H^2_0(t)\, \zeta_0(t) -  H^2_0(s) \,\zeta_0(s) - \int_s^t H_0^2 \alpha_0^2\,du\right)\\
  &-\frac12 \left(H^2_i(t) \,\zeta_i(t) -  H^2_i(s) \,\zeta_i(s) - \int_s^t H_i^2 \alpha_i^2\,du\right)\\
  & + {\color{red} \rho^{zx}_{0i} \int_s^t H_0\, \alpha_0\, \sigma^x_i\,du} \\
  &  - \int_s^t \left(H_i(t)-H_i\right)\gamma_i \,du \qquad\mbox{with}\qquad
  \gamma_i = -\alpha_i^2\,H_i -\rho_{ii}^{zx}\,\sigma_i\,\alpha_i + {\color{red}\rho_{i0}^{zz}\,\alpha_i\,\alpha_0\,H_0}   \\
  & + {\color{blue}\int_s^t \left(H_0(t)-H_0\right)\,\gamma_0 \,du \qquad \mbox{with}\qquad \gamma_0 = - H_0\,\alpha_0^2}
\end{align*}
and the following additional variances and covariances
\begin{align*}
\mathrm{Var}[\Delta y_0] =& \int_s^t \alpha_0^2\,H_0^2\,du \\
\mathrm{Cov}[\Delta y_0, \Delta z_i] =& \rho^{zz}_{0i} \int_s^t \alpha_0\,H_0\,\alpha_i\,du \\
\mathrm{Cov}[\Delta y_0, \Delta \ln x_i] =& \int_s^t \left(H_0(t)-H_0\right) \alpha_0^2\,H_0\,du \\
&  - \rho^{zz}_{0i}\int_s^t \alpha_0\,H_0\left(H_i(t)-H_i\right)\, \alpha_i \,du \\
&  +\rho^{zx}_{0i}\int_s^t \alpha_0 \, H_0\,\sigma^x_i \,du 
%\mathrm{Var}[\Delta z_i] =& \int_s^t \alpha_i^2\,du \\
%\mathrm{Var}[\Delta \ln x_i] =&
%      \int_s^t \left(H_0(t)-H_0\right)^2 \alpha_0^2\,du \nonumber\\
%      & -2\rho^{zz}_{0i} \int_s^t \left(H_i(t)-H_i\right) \alpha_i\left(H_0(t)-H_0\right) \alpha_0\,du
%  \nonumber\\
%      &+ 2\rho^{zx}_{0i}\int_s^t \left(H_0(t)-H_0\right)\alpha_0 \,\sigma^x_i\,du \nonumber\\
%      &- 2\rho^{zx}_{ii}\int_s^t \left(H_i(t)-H_i\right)\alpha_i \,\sigma^x_i\,du\nonumber\\
%      &+ \int_s^t \left(H_i(t)-H_i\right)^2\alpha_i^2 \,du
%  \nonumber\\
%      &+ \int_s^t(\sigma^x_i)^2\,du \\
%\mathrm{Cov} [\Delta z_i, \Delta z_j] =& \rho^{zz}_{ij}\int_s^t \alpha_i\,\alpha_j\,du \\
%\mathrm{Cov} [\Delta z_i, \Delta \ln x_j] =& \rho^{zz}_{0i}\int_s^t \left(H_0(t)-H_0\right)
%  \alpha_0\,\alpha_i\,du \nonumber\\
%      &- \rho^{zz}_{ij}\int_s^t \alpha_i \,\alpha_j \,\left(H_j(t)-H_j\right) \,du \nonumber\\
%      &+\rho^{zx}_{ij}\int_s^t \alpha_i \, \sigma^x_j \,du.
\end{align*}

Example 36 in section \ref{example:36} illustrates the effect of the choice of measure on exposure simulations.

\subsection{Exposures}\label{sec:app_exposure}

In ORE we use the following exposure definitions
\begin{align}
\EE(t) = \EPE(t) &= \E^N\left[ \frac{(NPV(t)-C(t))^+}{N(t)} \right] \label{EE}\\
\ENE(t) &= \E^N\left[ \frac{(-NPV(t)+C(t))^+}{N(t)} \right] \label{ENE}
\end{align}
where $\NPV(t)$ stands for the netting set NPV and $C(t)$ is the collateral balance\footnote{$C(t)>0$ means that we have
  {\em received} collateral from the counterparty} at time $t$. Note that these exposures are expectations of values
discounted with numeraire $N$ (in ORE the Linear Gauss Markov model's numeraire) to today, and expectations are taken in
the measure associated with numeraire $N$. These are the exposures which enter into unilateral CVA and DVA calculation,
respectively, see next section. Note that we sometimes label the expected exposure (\ref{EE}) EPE, not to be confused
with the Basel III Expected Positive Exposure below.

\medskip
Basel III defines a number of exposures each of which is a 'derivative' of Basel's Expected Exposure:
\begin{align}
\intertext{Expected Exposure}
EE_B(t) &= \E[\max(NPV(t) - C(t), 0)] \label{basel_ee}\\
\intertext{Expected Positive Exposure}
EPE_B(T) &= \frac{1}{T} \sum_{t<T} EE_B(t)\cdot \Delta t  \label{basel_epe} \\
\intertext{Effective Expected Exposure, recursively defined as running maximum}
EEE_B(t) &= \max(EEE_B(t-\Delta t), EE_B(t)) \label{basel_eee}\\
\intertext{Effective Expected Positive Exposure}
EEPE_B(T) &= \frac{1}{T} \sum_{t<T} EEE_B(t)\cdot \Delta t \label{basel_eepe}
\end{align}
The last definition, Effective EPE, is used in Basel documents since Basel II for Exposure At Default and capital
calculation. Following \cite{bcbs128,bcbs189} the time averages in the EPE and EEPE calculations are taken over {\em the
  first year} of the exposure evolution (or until maturity if all positions of the netting set mature before one year).

\medskip
To compute $EE_B(t)$ consistently in a risk-neutral setting, we compound (\ref{EE}) with the deterministic discount factor $P(t)$ up to horizon $t$:
$$
EE_B(t) = \frac{1}{P(t)} \:\EE(t)
$$

Finally, we define another common exposure measure, the {\em Potential Future Exposure} (PFE), as a (typically high)
quantile $\alpha$ of the NPV distribution through time, similar to Value at Risk but at the upper end of the NPV
distribution:

\begin{align}
  \PFE_\alpha(t) = \left(\inf\left\{ x | F_t(x) \geq \alpha\right\}\right)^+ \label{PFE}
\end{align}

where $F_t$ is the cumulative NPV distribution function at time $t$. Note that we also take the positive part to ensure
that PFE is a positive measure even if the quantile yields a negative value which is possible in extreme cases.
 
\subsection{Exposures using American Monte Carlo}
\label{sec:app_amc}

The exposure analysis implemented in ORE that is used in the bulk of the examples in this user guide, mostly vanilla portfolios, 
is divided into two independent steps:

\begin{enumerate}
\item in a first step a list of NPVs (or a ``NPV cube'') is computed. The list is indexed by the trade ID, the
  simulation time step and the scenario sample number. Each entry of the cube is computed using the same pricers as for
  the T0 NPV calculation by shifting the evaluation date to the relevant time step of the simulation and updating the
  market term structures to the relevant scenario market data. The market data scenarios are generated using a {\em risk
    factor evolution model} which can be a cross asset model, but also be based on e.g. historical simulation.
\item in a second step the generated NPV cube is passed to a post processor that aggregates the results to XVA figures
  of different kinds.
\end{enumerate}

We label this approach in the following as the {\em classic} exposure analysis.

The AMC module in ORE allows to replace the first step by a different approach which works faster in particular for exotic
deals. The second step remains the same. The risk factor evolution model coincides with the pricing models for the
single trades in this approach and is always a cross asset model operated in a pricing measure.

For AMC the entries of the NPV cube are now viewed as conditional NPVs at the simulation time given the information that
is generated by the cross asset model's driving stochastic process up to the simulation time. The conditional
expectations are then computed using a regression analysis of some type. In our current implementation this is chosen to
be a parametric regression analysis.

The regression models are calibrated per trade during a training phase and later on evaluated in the simulation
phase. The set of paths in the two phases is in general different w.r.t. their number, time step structure, and
generation method (Sobol, Mersenne Twister) and seed. Typically the regressand is the (deflated) dirty {\em path} NPV of
the trade in question, or also its underlying NPV or an option continuation value (to take exercise decisions or
represent the physical underlying for physical exercise rights). The regressor is typically the model state. Certain
exotic features that introduce path-dependency (e.g. a TaRN structure) may require an augmentation of the regressor
though (e.g. by the already accumulated amount in case of the TaRN).

The path NPVs are generated at their {\em natural event dates}, like the fixing date for floating rate coupons or the
payment date for fixed cashflows. This reduces the requirements for the cross asset model to provide closed form
expressions for the numeraire and conditional zero bonds only.

Since the evaluation of the regression functions is computationally cheap the overall timings of the NPV cube generation
are generally smaller compared to the classic approach, in particular for exotic deals like Bermudan Swaptions.

From a methodology point of view an important difference between the classic and the AMC exposure analysis lies in the
model consistency: While the conditional NPVs computed with AMC are by construction consistent with the risk factor
evolution model driving the XVA simulation, the scenario NPVs in the classic approach are in general not consistent in
this sense unless the market scenarios are fully implied by the cross asset model. Here ``fully implied'' means that not
only rate curves, but also market volatility and correlation term structures like FX volatility surfaces, Swaption
volatilities or CMS correlation term structures as well as other parameters used by the single trade pricers have to be
deduced from the cross asset model, e.g. the mean reversion of the Hull White 1F model and a suitable model volatility
feeding into a Bermudan Swaption pricer.

We note that the generation of such implied term structures can be computationally expensive even for simple versions of
a cross asset model like one composed from LGM IR and Black-Scholes FX components etc., and even more so for more exotic
component flavours like Cheyette IR components, Heston FX components etc.

In the current implementation only a subset of all ORE trade types can be simulated using AMC while all other trade types
are still simulated using the classic engine. The separation of the trades and the joining of the resulting classic and AMC
cubes is automatic. The post processing step is run on the joint cube from the classic and AMC simulations as before.

Trade types supported by AMC so far:
\begin{enumerate}
\item Swap
\item CrossCurrencySwap
\item FxOption
\item BermudanSwaption
\item MultiLegOption
\end{enumerate}

\subsubsection{*Implementation Details}\label{sec:implementation_details}

\subsubsection*{AMC valuation engine and AMC pricing engines}

The \verb+AMCValuationEngine+ is responsible for generating a NPV cube for a portfolio of AMC enabled trades and
(optionally) to populate a \verb+AggregationScenarioData+ instance with simulation data for post processing, very
similar to the classic \verb+ValuationEngine+ in ORE.

The AMC valuation engine takes a cross asset model defining the risk factor evolution. This is set up identically to the
cross asset model used in the \\ \verb+CrossAssetModelScenarioGenerator+. Similarly the same parameters for the path
generation (given as a \verb+ScenarioGeneratorData+ instance) are used, so that it is guaranteed that both the AMC
engine and the classic engine produce the same paths, hence can be combined to a single cube for post processing. It is
checked, that a non-zero seed for the random number generation is used.

The portfolio that the AMC engine consumes is build against an engine factory set up by a pricing engine configuration
given in the amc analytics type (see \ref{sec:amc_applicationconfig}). This configuration should select special AMC engine
builders which (by a pure naming convention) have the engine type ``AMC''. These engine builders are retrieved from
\verb+getAmcEngineBuilders()+ in \verb+oreappplus.cpp+ and are special in that unlike usual engine builders they take
two parameters

\begin{enumerate}
\item the cross asset model which serves as a risk factor evolution model in the AMC valuation engine
\item the date grid used within the AMC valuation engine
\end{enumerate}

For technical reasons, the configuration also contains configurations for \\ \verb+CapFlooredIborLeg+ and \verb+CMS+
because those are used within the trade builders (more precisely the leg builders called from these) to build the
trade. The configuration can be the same as for T0 pricing for them, it is actually not used by the AMC pricing engines.

The AMC engine builders build a smaller version of the global cross asset model only containing the model components
required to price the specific trade. Note that no deal specific calibration of the model is performed.

The AMC pricing engines perform a T0 pricing and - as a side product - can be used as usual T0 pricing engines if a
corresponding engine builder is supplied, see \ref{sec:amc_sideproducts}.

In addition the AMC pricing engines perform the necessary calculations to yield conditional NPVs on the given global
simulation grid. How these calcuations are performed is completely the responsibility of the pricing engines, altough
some common framework for many trade types is given by a base engine, see \ref{sec:amc_base_engine}. This way the
approximation of conditional NPVs on the simulation grid can be taylored to each product and also each single trade,
with regards to

\begin{enumerate}
\item the number of traning paths and the required date grid for the training (e.g. containing all relevant coupon and
  exercise event dates of a trade)
\item the order and type of regressoin basis functions to be used
\item the choice of the regressor (e.g. a TaRN might require a regressor augmented by the accumulated coupon amount)
\end{enumerate}

The AMC pricing engines then provide an additional result labelled \verb+amcCalculator+ which is a class implementing
the \verb+AmcCalculator+ interface which consists of two methods: The method \verb+simulatePath()+ takes a
\verb+MultiPath+ instance representing one simulated path from the global risk factor evolution model and returns an
array of conditional, deflated NPVs for this path. The method \verb+npvCurrency()+ returns the currency $c$ of the
calculated conditional NPVs. This currency can be different from the base currency $b$ of the global risk factor
evolution model. In this case the conditional NPVs are converted to the global base currency within the AMC valuation
engine by multiplying them with the conversion factor

\begin{equation}\label{currency_conversion_factor}
\frac{N_c(t) X_{c,b}(t)}{N_b(t)}
\end{equation}

where $t$ is the simulation time, $N_c(t)$ is the numeraire in currency $c$, $N_b(t)$ is the numeraire in currency
$b$ and $X_{c,b}(t)$ is the FX rate at time $t$ converting from $c$ to $b$.

The technical criterion for a trade to be processed within the AMC valuation is engine is that a) it can be built
against the AMC engine factory described above and b) it provides an additional result \verb+amcCalculator+. If a trade
does not meet these criteria it is simulated using the classic valuation engine. The logic that does this is located in
the overide of the method \verb+OREAppPlus::generateNPVCube()+.

The AMC valuation engine can also populate an aggregation scenario data instance. This is done only if necessary,
i.e. only if no classic simulation is performed anyway. The numeraire and fx spot values produced by the AMC valuation
engine are identical to the classic engine. Index fixings are close, but not identical, because the AMC engine used the
T0 curves for projection while the classic engine uses scenario simulation market curves, which are not exactly matching
those of the T0 market. In this sense the AMC valuation engine produces more precise values compared to the classic
engine.

\subsubsection*{The multileg option AMC base engine and derived engines}\label{sec:amc_base_engine}

Table \ref{tbl:amcconfig} provides an overview of the implemented AMC engine builders. These builders use the following
QuantExt pricing engines

\begin{enumerate}
\item \verb+McLgmSwapEngine+ for single currency swaps
\item \verb+McCamCurrencySwapEngine+ for cross currency swaps
\item \verb+McCamFxOptionEngine+ for fx options
\item \verb+McLgmSwaptionEngine+ for Bermudan swaptions
\item \verb+McMultiLegOptionEngine+ for Multileg option
\end{enumerate}

All these engine are based on a common \verb+McMultiLegBaseEngine+ which does all the computations. For this each of the
engines sets up the following protected member variables (serving as parameters for the base engine) in their
\verb+calculate()+ method:

\begin{enumerate}
\item \verb+leg_+: a vector of \verb+QuantLib::Leg+
\item \verb+currency_+: a vector of \verb+QuantLib::Currency+ corresponding to the leg vector
\item \verb+payer_+: a vector of $+1.0$ or $-1.0$ double values indicating receiver or payer legs
\item \verb+exercise_+: a \verb+QuantLib::Exercise+ instance describing the exercise dates (may be \verb+nullptr+, if
  the underlying represents the deal already)
\item \verb+optionSettlement_+: a \verb+Settlement::Type+ value indicating whether the option is settled physically or
  in cash
\end{enumerate}

A call to \verb+McMultiLegBaseEngine::calculate()+ will set the result member variables

\begin{enumerate}
\item \verb+resultValue_+: T0 NPV in the base currency of the cross asset model passed to the pricing engine
\item \verb+underlyingValue_+: T0 NPV of the underlying (again in base ccy)
\item *\verb+amcCalculator_+: the AMC calculator engine to be used in the AMC valuation engine
\end{enumerate}

The specific engine implementations should convert the \verb+resultValue_+ to the npv currency of the trade (as defined
by the (ORE) trade builder) so that they can be used as regular pricing engine consistently within ORE. Note that only
the additional \verb+amcCalculator+ result is used by the AMC valuation engine, not any of the T0 NPVs directly.

\subsection*{Limitations and Open Points}
\label{sec:amc_limitations}

This sections lists known limitations of the AMC simulation engine.

\subsubsection*{Trade Features}

Some trade features are not yet supported by the multileg option engine:

\begin{enumerate}
\item exercise flows (like a notional exchange common to cross currency swaptions) are not supported
\end{enumerate}

\subsubsection*{Flows Generation (for DIM Analysis)}

At the current stage the AMC engine does not generate flows which are required for the DIM analysis in the post
processor.

\subsubsection*{State interpolation for exercise decisions}

During the simulation phase exercise times of a specific trade are not necessarily part of the simulated time
grid. Therefore the model state required to take the exercise decision has in to be interpolated in general on the
simulated path. Currently this is done using a simple linear interpolation while from a pure methodology point of view a
Brownian Bridge would be preferable. In our tests we do not see a big impact of this approximation though.

\subsubsection*{Basis Function Selection}

Currently the basis function system is generated by specifying the type of the functions and the order, see
\ref{sec:amc_pricingengineconfig}. The number of independent variables varies by product type and details. Depending on
the number of independent variables and the order the number of generated basis functions can get quite big which slows
down the computation of regression coefficients. It would be desirable to have the option to filter the full set of
basis functions, e.g. by explicitly enumerating them in the configuration, so that a high order can be chosen even for
products with a relatively large number of independent variables (like e.g. FX Options or Cross Currency Swaps).

\subsection*{Outlook: Trade Compression}

For vanilla trades where the regression is only required to produce the NPV cube entries (and not to take exercise
decisions etc.) it is not strictly necessary to do the regression analysis on a single trade level\footnote{except
  single trade exposures are explicitly required of course}. Although in the current implementation there is no direct
way to do the regression analysis on whole (sub-)portfolios instead of single trades, one can represent such a
subportfolio as a single technical trade (e.g. as a single swap or multileg option trade) to achieve a similar
result. This might lead to better performance than the usual single trade calculation. However one should also try to
keep the regressions as low-dimensional as possible (for performance and accuracy reasons) and therefore define the
sub-portfolios by e.g. currency, i.e. as big as possible while at the same time keeping the associated model dimension as
small as possible.

\subsection{CVA and DVA}\label{sec:app_cvadva}

Using the expected exposures in \ref{sec:app_exposure} unilateral discretised CVA and DVA are given by \cite{Lichters}
\begin{align}
\CVA &= \sum_{i} \PD(t_{i-1},t_i)\times\LGD\times \EPE(t_i) \label{CVA}\\
\DVA &= \sum_{i} \PD_{Bank}(t_{i-1},t_i)\times\LGD_{Bank}\times \ENE(t_i) \label{DVA}
\end{align}
where
\begin{align*}
\EPE(t) & \mbox{ expected exposure (\ref{EE})}\\
\ENE(t) & \mbox{ expected negative exposure (\ref{ENE})}\\
PD(t_i,t_j) & \mbox{ counterparty probability of default in } [t_i;t_j]\\
PD_{Bank}(t_i,t_j) & \mbox{ our probability of default in } [t_i;t_j]\\
LGD & \mbox{ counterparty loss given default}\\
LGD_{Bank} & \mbox{ our loss given default}\\
\end{align*}

Note that the choice $t_i$ in the arguments of $\EPE(t_i)$ and $\ENE(t_i)$ means we are choosing the {\em advanced}
rather than the {\em postponed} discretization of the CVA/DVA integral \cite{BrigoMercurio}. This choice can be easily
changed in the ORE source code or made configurable. \\

Moreover, formulas (\ref{CVA}, \ref{DVA}) assume independence of credit and other market risk factors, so that $\PD$ and
$\LGD$ factors are outside the expectations. With the extension of ORE to credit asset classes and in particular for
wrong-way-risk analysis, CVA/DVA formulas is generaised and is applicable to calculations with dynamic credit

\begin{align}
\CVA^{dyn} &= \sum_{i} \E^N\left[\frac{\PD^{dyn}(t_{i-1},t_i)\times \PE(t_i)}{N(t)} \right]\times\LGD \label{CVA_dynamic} \\
\DVA^{dyn} &= \sum_{i} \E^N\left[\frac{\PD^{dyn}_{Bank}(t_{i-1},t_i)\times \NE(t_i)}{N(t)} \right]\times\LGD_{Bank} \label{DVA_dynamic}
\end{align}
where
\begin{align*}
\PE(t) & \mbox{ random variables representing positive exposure at } t: (NPV(t)-C(t))^+\\
\NE(t) & \mbox{ random variables representing negative exposure at } t: (-NPV(t)+C(t))^+\\
PD^{dyn}(t_i,t_j) & \mbox{ random variables representing counterparty probability of default in } [t_i;t_j]\\
PD^{dyn}_{Bank}(t_i,t_j) & \mbox{ random variables representing our probability of default in } [t_i;t_j]\\
LGD & \mbox{ counterparty loss given default}\\
LGD_{Bank} & \mbox{ our loss given default}\\
\end{align*}

\subsection{FVA}\label{sec:fva}

%Any exposure (uncollateralised or residual after taking collateral into account) gives rise to funding cost or benefits
%depending on the sign of the residual position. This can be expressed as a Funding Value Adjustment (FVA). A simple
%definition of FVA can be given in a very similar fashion as the sum of unilateral CVA and DVA which we defined by
%(\ref{CVA},\ref{DVA}), namely as an expectation of exposures times funding spreads:
%\begin{align}
%  \FVA &= \underbrace{\sum_{i=1}^n f_b(t_{i-1},t_i)\,\delta_i \, \E^N\left[S_C(t_{i-1})\, S_B(t_{i-1})\, (\NPV(t_i))^+\,
%         D(t_i)\right]}_{\mbox{Funding Benefit Adjustment (FBA)}}\nonumber\\
%       & {} - \underbrace{\sum_{i=1}^n f_l(t_{i-1},t_i)\,\delta_i \, \E^N\left[S_C(t_{i-1})\, S_B(t_{i-1})\, (-\NPV(t_i))^+\, D(t_i)\right]}_{\mbox{Funding Cost Adjustment (FCA)}}\label{eq_simple_fva}
%\end{align}
%where
%\begin{align*}
%D(t_i) & \mbox{ stochastic discount factor, $1/N(t_i)$ in LGM}\\
%\NPV(t_i) & \mbox{ portfolio value after potential collateralization}\\
%S_C(t_j) & \mbox{ survival probability of the counterparty}\\
%S_B(t_j) & \mbox{ survival probability of the bank}\\
%f_b(t_j) & \mbox{ borrowing spread for the bank relative to the collateral compounding rate}\\
%f_l(t_j) & \mbox{ lending spread for the bank relative to the collateral compounding rate}
%\end{align*}
%For details see e.g. Chapter 14 in Gregory \cite{Gregory12} and the discussion in \cite{Lichters}.

Any exposure (uncollateralised or residual after taking collateral into account) gives rise to funding cost or benefits
depending on the sign of the residual position. This can be expressed as a Funding Value Adjustment (FVA). A simple
definition of FVA can be given in a very similar fashion as the sum of unilateral CVA and DVA which we defined by
(\ref{CVA},\ref{DVA}), namely as an expectation of exposures times funding spreads:
\begin{align}
  \FVA &= \underbrace{\sum_{i=1}^n f_l(t_{i-1},t_i)\,\delta_i \, \E^N\left\{S_C(t_{i-1})\, S_B(t_{i-1})\, [-\NPV(t_i)+C(t_i)]^+\,
         D(t_i)\right\}}_{\mbox{Funding Benefit Adjustment (FBA)}}\nonumber\\
       & {} - \underbrace{\sum_{i=1}^n f_b(t_{i-1},t_i)\,\delta_i \, \E^N\left\{S_C(t_{i-1})\, S_B(t_{i-1})\, [\NPV(t_i)-C(t_i)]^+\, D(t_i)\right\}}_{\mbox{Funding Cost Adjustment (FCA)}}\label{eq_simple_fva}
%  \FVA &= - \underbrace{\sum_{i=1}^n f_b(t_{i-1},t_i)\,\delta_i \, \E^N\left[S_C(t_{i-1})\, S_B(t_{i-1})\, (\NPV(t_i))^+\,
 %        D(t_i)\right]}_{\mbox{Funding Cost Adjustment (FCA)}}\nonumber\\
 %      & {} \underbrace{\sum_{i=1}^n f_l(t_{i-1},t_i)\,\delta_i \, \E^N\left[S_C(t_{i-1})\, S_B(t_{i-1})\, (-\NPV(t_i))^+\, D(t_i)\right]}_{\mbox{Funding Benefit Adjustment (FBA)}}\label{eq_simple_fva}
\end{align}
where
\begin{align*}
D(t_i) & \mbox{ stochastic discount factor, $1/N(t_i)$ in LGM}\\
\NPV(t_i) & \mbox{ portfolio value at time } t_i\\
C(t_i) & \mbox{Collateral account balance at time } t_i \\ 
S_C(t_j) & \mbox{ survival probability of the counterparty}\\
S_B(t_j) & \mbox{ survival probability of the bank}\\
f_b(t_j) & \mbox{ borrowing spread for the bank relative to OIS flat}\\
f_l(t_j) & \mbox{ lending spread for the bank relative to OIS flat}
\end{align*}
For details see e.g. Chapter 14 in Gregory \cite{Gregory12} and the discussion in \cite{Lichters}.

\medskip
The reasoning leading to the expression above is as follows. Consider, for example, a single partially collateralised derivative (no collateral at all or CSA with a significant threshold) between us (the Bank) and counterparty 1 (trade 1). 

We assume that we enter into an offsetting trade with (hypothetical) counterparty 2 which is perfectly collateralised (trade 2). We label the NPV of trade 1 and 2 $\NPV_{1,2}$ respectively (from our perspective, excluding CVA). Then $\NPV_2=-\NPV_1$. The respective collateral amounts due to trade 1 and 2 are $C_1$ and $C_2$ from our perspective. Because of the perfect collateralisation of trade 2 we assume $C_2=\NPV_2$. The imperfect collateralisation of trade 1 means $C_1 \ne \NPV_1$. The net collateral balance from our perspective is then $C=C_1+C_2$ which can be written $C=C_1+C_2 = C_1 + \NPV_2 = -\NPV_1 + C_1$.

\begin{itemize}
\item If $C>0$ we receive net collateral and pay the overnight rate on this notional amount. On the other hand we can invest the received collateral and earn our lending rate, so that we have a benefit proportional to the lending spread $f_l$ (lending rate minus overnight rate). It is a benefit assuming $f_l >0$. $C>0$ means $-\NPV_1 + C_1 > 0$ so that we can cover this case with ``lending notional'' $[-\NPV_1 + C_1]^+$.
\item If $C<0$ we post collateral amount $-C$ and receive the overnight rate on this amount. Amount $-C$ needs to be funded in the market, and we pay our borrowing rate on it. This leads to a funding cost proportional to the borrowing spread $f_b$ (borrowing rate minus overnight). $C<0$ means $\NPV_1 - C_1 > 0$, so that we can cover this case with ``borrowing notional'' $[\NPV_1 - C_1]^+$. If the borrowing spread is positive, this term proportional to $f_b \times [\NPV_1 - C_1]^+$ is indeed a cost and therefore needs to be subtracted from the benefit above.
\end{itemize}
   
Formula \eqref{eq_simple_fva} evaluates these funding cost components on the basis of the original trade's or portfolio's $\NPV$. Perfectly collateralised portfolios hence do not contribute to FVA because under the hedging fiction, they are hedged with a perfectly collateralised opposite portfolio, so any collateral payments on portfolio 1 are canceled out by those of the opposite sign on portfolio 2.

\subsection{COLVA}

When the CSA defines a collateral compounding rate that deviates from the overnight rate, this gives rise to another
value adjustment labeled COLVA \cite{Lichters}. In the simplest case the deviation is just given by a constant spread
$\Delta$:
\begin{align}
\COLVA &= \E^N\left[ \sum_i -C(t_i)\cdot \Delta \cdot \delta_i \cdot D(t_{i+1}) \right]
\label{COLVA}
\end{align}
where $C(t)$ is the collateral balance\footnote{see \ref{sec:app_exposure}, $C(t)>0$ means that we have {\em received}
  collateral from the counterparty} at time $t$ and $D(t)$ is the stochastic discount factor $1/N(t)$ in LGM. Both
$C(t)$ and
$N(t)$ are computed in ORE's Monte Carlo framework, and the expectation yields the desired adjustment. \\
 
Replacing the constant spread by a time-dependent deterministic function in ORE is straight forward. 
  
\subsection{Collateral Floor Value}

A less trivial extension of the simple COLVA calculation above, also covered in ORE, is the case where the deviation
between overnight rate and collateral rate is stochastic itself. A popular example is a CSA under which the collateral
rate is the overnight rate {\em floored at zero}. To work out the value of this CSA feature one can take the difference
of discounted margin cash flows with and without the floor feature. It is shown in \cite{Lichters} that the following
formula is a good approximation to the collateral floor value
\begin{align}
\Pi_{Floor} &= \E^N\left[ \sum_i -C(t_i)\cdot (-r(t_i))^+\cdot\delta_i \cdot D(t_{i+1}) \right]
\label{CSA_floor_value_approx}
\end{align}
where $r$ is the stochastic overnight rate and $(-r)^+ = r^+ - r$ is the difference between floored and 'un-floored' compounding rate. \\

Taking both collateral spread and floor into account, the value adjustment is 
\begin{align}
\Pi_{Floor,\Delta} &= \E^N\left[ \sum_i -C(t_i)\cdot ((r(t_i)-\Delta)^+-r(t_i))\cdot\delta_i \cdot D(t_{i+1}) \right] 
\label{CSA_floor_value_approx_2}
\end{align}

\subsection{Dynamic Initial Margin and MVA}\label{sec:app_dim}

The introduction of Initial Margin posting in non-cleared OTC derivatives business reduces residual credit exposures and
the associated value adjustments, {\bf CVA} and {\bf DVA}.

On the other hand, it gives rise to additional funding cost. The value of the latter is referred to as Margin Value Adjustment ({\bf MVA}).\\

To quantify these two effects one needs to model Initial Margin under future market scenarios, i.e. Dynamic Initial Margin ({\bf DIM}). Potential approaches comprise 
\begin{itemize}
\item Monte Carlo VaR embedded into the Monte Carlo simulation
\item Regression-based methods
\item Delta VaR under scenarios
\item ISDA's Standard Initial Margin (SIMM) under scenarios
\end{itemize} 

We skip the first option as too computationally expensive for ORE. In the current ORE release we focus on a relatively
simple regression approach as in \cite{Anfuso2016,LichtersEtAl}. Consider the netting set values $\NPV(t)$ and $\NPV(t+\Delta)$ that
are spaced one margin period of risk $\Delta$ apart. Moreover, let $F(t,t+\Delta)$ denote cumulative netting set cash
flows between time $t$ and $t+\Delta$, converted into the NPV currency. Let $X(t)$ then denote the netting set value
change during the margin period of risk excluding cash flows in that period:
$$
X(t) = \NPV(t+\Delta) + F(t, t+\Delta) - \NPV(t) 
$$  
ignoring discounting/compounding over the margin period of risk. We actually want to determine the distribution of
$X(t)$ conditional on the `state of the world' at time $t$, and pick a high (99\%) quantile to determine the Initial
Margin amount for each time $t$. Instead of working out the distribution, we content ourselves with estimating the
conditional variance $\V(t)$ or standard deviation $S(t)$ of $X(t)$, assuming a normal distribution and scaling $S(t)$
to the desired 99\% quantile by multiplying with the usual factor $\alpha=2.33$ to get an estimate of the Dynamic
Initial Margin $\DIM$:
$$
\V(t) = \E_t[X^2] - \E_t^2[X], \qquad S(t)=\sqrt{\V(t)}, \qquad \DIM(t) = \alpha \,S(t)
$$ 
We further assume that $\E_t[X]$ is small enough to set it to the expected value of $X(t)$ across all Monte Carlo
samples $X$ at time $t$ (rather than estimating a scenario dependent mean). The remaining task is then to estimate the
conditional expectation $\E_t[X^2]$. We do this in the spirit of the Longstaff Schwartz method using regression of
$X^2(t)$ across all Monte Carlo samples at a given time. As a regressor (in the one-dimensional case) we could use
$\NPV(t)$ itself. However, we rather choose to use an adequate market point (interest rate, FX spot rate) as regression
variable $x$, because this is generalised more easily to the multi-dimensional case. As regression basis functions we
use polynomials, i.e. regression functions of the form $c_0 + c_1\,x + c_2\,x^2 + ...+ c_n\,x^n$ where the order $n$ of
the polynomial can be selected by the user. Choosing the lowest order $n=0$, we obtain the simplest possible estimate,
the variance of $X$ across all samples at time $t$, so that we apply a single $\DIM(t)$ irrespective of the 'state of
the world' at time $t$ in that case.  The extension to multi-dimensional regression is also implemented in ORE. The user
can choose several regressors simultaneously (e.g. a EUR rate, a USD rate, USD/EUR spot FX rate, etc.) in order order to
cover complex multi-currency portfolios.

\medskip
Given the DIM estimate along all paths, we can next work out the Margin Value Adjustment \cite{Lichters} in discrete form
%{\color{red}
\begin{align}
\MVA &= \sum_{i=1}^n (f_b - s_I)\, \delta_i\: S_C(t_i)\: S_B(t_i) \times \E^N\left[
\DIM(t_i)\,D(t_i)\right]. \label{MVA} 
\end{align}
%}
with borrowing spread $f_b$ as in the FVA section \ref{sec:fva} and spread $s_I$ received on initial margin, both
spreads relative to the cash collateral rate.

\subsection{KVA (CCR)}\label{sec:app_kva}

The KVA is calculated for the Counterparty Credit Risk Capital charge
(CCR) following the IRB method concisely  described in
\cite{Gregory15}, Appendix 8A.
It is following the Basel rules by computing risk capital as the
product of alpha weighted  exposure at default, worst case probability
of default at 99.9  and a maturity adjustment factor also described in
the Basel annex 4.
The risk capital charges are discounted with a capital discount factor
and summed up to  give the total CCR KVA after being multiplied with
the risk  weight and a capital charge (following the RWA method).

\medskip Basel II internal rating based (IRB) estimate of worst case
probability of  default: large homogeneous pool (LHP) approximation of
Vasicek (1997), KVA regulatory probability of default is the worst
case probability of default floored at 0.03 (the latter is valid for 
corporates and banks, no such floor applies to sovereign counterparties):
$$
\PD_{99.9\%} = \max\left(floor, N \left(\frac{N^{-1}(\PD) + \sqrt{\rho}
  N^{-1}(0.999)}{\sqrt{1 - \rho}}\right) - \PD\right)
$$
$N$ is the cumulative standard normal distribution,

$$
\rho = 0.12 \frac{1 - e^{-50 \PD}}{1 - e^{-50}} + 0.24 \left(1 - \frac{1 -
  e^{-50 \PD}}{1 - e^{-50}}\right)
$$

\medskip Maturity adjustment factor for RWA method capped at 5, floored at 1:
$$
\MA(\PD, M) = \min\left(5, \max\left(1, \frac{1 + (M - 2.5) B(\PD)}{1 - 1.5 B(\PD)}\right)\right)
$$
\medskip where $B(\PD) = (0.11852 - 0.05478 \ln(\PD))^2$ and M is the
effective  maturity of the portfolio (capped at 5):

$$M = \min\left(5, 1 + \frac{\sum\limits_{t_k > 1yr} \EE_B(t_k)\Delta t_k
  B(0,t_k)}{\sum\limits_{t_k \leq 1yr} \EEE_B(t_k)\Delta t_k B(0,t_k)}\right)
$$

\medskip where $B(0,t_k)$ is the risk-free discount factor from the
simulation  date $t_k$ to today, $\Delta t_k$ is the difference
between time points, 
$\EE_B(t_k)$ is the expected (Basel) exposure at time $t_k$ and $\EEE_B(t_k)$ is the
associated effective expected exposure.

\medskip 
Expected risk capital at $t_i$:
$$
\RC(t_i) = EAD(t_i) \times LGD \times \PD_{99.9\%} \times \MA(\PD, M)
$$
where
\begin{itemize}
\item $\EAD(t_i) = \alpha \times \EEPE(t_i)$
\item $\EEPE(t_i)$ is estimated as the time average of the running maximum of $\EPE(t)$ over the time interval $t_i\leq t\leq t_i+1$
\item $\alpha$ is the multiplier resulting from the IRB calculations (Basel II defines a supervisory alpha of 1.4, but gives banks the option to estimate their own $\alpha$,subject to a floor of 1.2).
\item the maturity adjustment MA is derived from the EPE profile for times $t\geq t_i$
\end{itemize}

\medskip 
$\KVA_{CCR}$ is the sum of the expected risk capital amount discounted at {\em capital discount rate} $r_{cd}$ and compounded at rate given by the product of {\em capital hurdle} $h$ and {\em regulatory adjustment} $a$:
$$
\KVA_{CCR} = \sum_i \RC(t_i) \times \frac{1}{ (1 + r_{cd})^{\delta(t_{i-1}, t_i)}} \times \delta(t_{i-1}, t_i) \times h \times a
$$
assuming Actual/Actual day count to compute the year factions $delta$.

In ORE we compute KVA CCR from both perspectives - ``our'' KVA driven by EPE and the counterparty default risk, and similarly ``their'' KVA driven by ENE and our default risk.

\subsection{KVA (BA-CVA)}\label{sec:app_kva_cva}

This section briefly summarizes the calculation of a capital value adjustment associated with the CVA capital charge (in the basic approach, BA-CVA) as introduced in Basel III \cite{bcbs189, d325, d424}. ORE implements the {\em stand-alone} capital charge $\SCVA$ for a netting set and computes a KVA for it\footnote{In the reduced version of BA-CVA, where hedges are not recognized, the total BA-CVA capital charge across all counterparties $c$ is given by
$$
K = \sqrt{\left(\rho \sum_c \SCVA_c\right)^2 +(1-\rho^2)\sum_c \SCVA_c^2}
$$  
with supervisory correlation $\rho=0.5$ to reflect that the credit spread risk factors across counterparties are not perfectly correlated. Each counterparty $\SCVA_c$ is given by a sum over all netting sets with this counterparty.}. In the basic approach, the stand-alone capital charge for a netting set is given by
$$
\SCVA = \RW_c\cdot M\cdot \EEPE \cdot\DF
$$
with 
\begin{itemize}
\item supervisory risk weight $\RW_c$ for the counterparty;
\item effective netting set maturity $M$ as in section \ref{sec:app_kva} (for a bank using IMM to calculate EAD), but without applying a cap of 5;
\item supervisory discount $\DF$ for the netting set which is equal to one for banks using IMM to calculate $\EEPE$ and $\DF=\left(1-\exp\left(-0.05\,M\right)\right)/(0.05\,M)$ for banks not using IMM to calculate $\EEPE$. 
\end{itemize}

The associated capital value adjustment is then computed for each netting set's stand-alone CVA charge as above
$$
\KVA_{BA-\CVA} = \sum_i \SCVA(t_i) \times \frac{1}{ (1 + r_{cd})^{\delta(t_{i-1}, t_i)}} \times \delta(t_{i-1}, t_i) \times h \times a
$$
with 
$$
\SCVA(t_i) = \RW_c \cdot M(t_i)\cdot \EEPE(t_i)\cdot\DF
$$
where we derive both $M$ and EEPE from the EPE profile for times $t\geq t_i$.

In ORE we compute KVA BA-CVA from both perspectives - ``our'' KVA driven by EPE and the counterparty risk weight, and similarly ``their'' KVA driven by ENE and our risk weight. \\

Note: Banks that use the BA-CVA for calculating CVA capital requirements are allowed to cap the maturity adjustment factor $\MA(\PD,M)$ in section \ref{sec:app_kva} at 1 for netting sets that contribute to CVA capital, if using the IRB approach for CCR capital.

\subsection{Collateral Model}\label{sec:app_collateral}

The collateral model implemented in ORE is based on the evolution of collateral account balances along each Monte Carlo
path taking into account thresholds, minimum transfer amounts and independent amounts defined in the CSA, as well as
margin periods of risk.

ORE computes the collateral requirement (aka \emph{Credit Support Amount}) through time along each Monte Carlo path
\begin{align}\label{eq:CSA}
CSA(t_m) &= 
\begin{cases}
\max(0, \NPV(t_m) + \IA - \Th_{rec}),& \NPV(t_m) + \IA \ge 0 \\
\min(0, \NPV(t_m) + \IA + \Th_{pay}),& \NPV(t_m) + \IA < 0
\end{cases}
\end{align}
where
\begin{itemize}
\item $\NPV(t_m)$ is the value of the netting set as of
  time $t_m$ from our persepctive,
  \item $\Th_{rec}$ is the threshold exposure below which we do not 
  require collateral, likewise $\TH_{pay}$ is the threshold that applies to collateral posted to the counterparty,
%\item $MTA$ is the minimum transfer amount for collateral margin
%  flow requests (possibly asymmetric)
\item $\IA$ is the sum of all collateral independent amounts attached to
  the underlying portfolio of trades (positive amounts imply that we
  have received a net inflow of independent amounts from the
  counterparty), assumed here to be cash.
\end{itemize}

As the collateral account already has a value of $C(t_m)$ at time $t_m$, the collateral shortfall is simply the
difference between $C(t_m)$ and $\CSA(t_m)$. However, we also need to account for the possibility that margin calls
issued in the past have not yet been settled (for instance, because of disputes). If $M(t_m)$ denotes the net value of
all outstanding margin calls at $t_m$, and $\Delta(t)$ is the difference 
$$
\Delta(t) = \CSA(t_m) - C(t_m) - M(t_m)
$$
between the {\em Credit Support Amount} and the current and outstanding collateral, then the actual margin
\emph{Delivery Amount} $D(t_m)$ is calculated as follows:
\begin{align}\label{eq:DA}
D(t_m) &= 
\begin{cases}
\Delta(t),& \left| \Delta(t) \right| \ge MTA \\
0,& \left| \Delta(t) \right| < MTA
\end{cases}
\end{align}
where $MTA$ is the minimum transfer amount. 

Consider the upper case of \eqref{eq:CSA}: If the initial value of the netting set is zero ($\NPV(t_0)=0$) and 
if $\Th_{rec}=0$, but the combined $\IA>0$, then the Credit Support Amount equals the Independent Amount, $\CSA(t_0)=\IA$.
If moreover the initial collateral balance is zero (because the Independent Amount has not been received yet),
then $\Delta(t_0)=\CSA(t_0)=\IA$, and the delivery amount $D(t_0)$ also matches the $\IA$ (assuming this exceeds the MTA),
so that the next call leads to the transfer of the Independent Amount to us. For a positive $\Th_{rec}>0$, the transfer to us is reduced accordingly.
In that case we can view the Independent Amount as an offset to the threshold.

Consider the lower case of \eqref{eq:CSA}: If the netting set value is negative from our perspective and in absolute terms larger than the $\IA$, 
then the Credit Support Amount is just the negative difference $\CSA=-|\NPV| + \IA + \Th_{pay}$ so that we need to post collateral, but only the amount 
beyond the combined threshold $\IA + \Th_{pay}$.

\subsubsection*{Margin Period of Risk} \label{sec:mpor}
After a counterparty defaults, it takes time to close out the portfolio. During this time period the portfolio value will change upon market conditions, therefore the portfolio's close-out value is subject to market risk, which is referred also as the close-out risk and the corresponding close-out period is called as the {\em Margin Period of Risk} (MPoR).  

Therefore, when a loss on the defaulted counterparty is realised at time $t_d$, the last time the collateral could be received is $t_d-\tau$, where $\tau$ denotes the MPoR. That is, the collateral at time $t_d$ is determined by the collateral value at $t_d-\tau$, namely $CSA(t_d-\tau)$, see equation \ref{eq:CSA}.

In ORE, we have two approaches to incorporate MPoR in the exposure simulations:
\begin{itemize}
 \item {\em Close-out Approach}: Simulating on an auxiliary close-out grid additional to the default time grid.
 \item {\em Lagged Approach}: Simulating only on a default time grid and delaying the margin calls on the grid.
\end{itemize}

\medskip In the {\em Close-out Approach}, we use an auxiliary ``close-out'' grid in addition to the main simulation grid (see section \ref{sec:simulation}). The main simulation grid is used to compute default values which feed into the collateral balance $C(t$) filtered by MTA and Threshold etc. The auxiliary close-out grid, offset from the main grid by the MPoR, is used to compute the delayed close-out values $V(t)$ associated with default time $t$\footnote{We note that in ORE when the exposure of an uncollateralised netting-set or a single trade without considering the netting-set is calculated, then the default value is calculated at the main simulation grid, not on the close-out grid.}. The difference between $V(t)$ and $C(t)$ causes a residual exposure $[V (t)-C(t)]^+$ even if minimum transfer amounts and thresholds are zero, see for example \cite{Pykhtin2010}. This approach allows a detailed modelling of what happens in the close-out period by calculating the close-out values in different ways. ORE currently supports two options: 
%
\begin{itemize}
\item the close-out value can be computed as of default date, by just evolving the market from default date to close-out date (sticky date), or
\item the close-out value can be computed as of close-out date, by evolving both valuation date and market over the close-out period (actual date), i.e., the portfolio ages and cash flows might occur in the close-out period causing spikes in the evolution of exposures.
\end{itemize}

The option ``sticky date'' is more aggressive in that it avoids any exposure evolution spikes due to contractual cashflows that occur in the close-out period after default, the only exposure effect is due to market evolution over the period. The ``actual date'' option is more conservative in that it includes the effect of all contractual cash flows in the close-out period, in particular outgoing cashflows at any time in the period which cause an exposure jump upwards. A more detailed framework for collateralised exposure modelling is introduced in the article \cite{Andersen2016}, indicating a potential route for extending ORE.

\medskip On the other hand, in the {\em Lagged Approach} the simulation is conducted only on a default time grid. The collateral values are calculated, by delaying the delivery amounts between default times, specified by the {\em Margin Period of Risk} (MPoR) which leads to residual exposure. 

In table \ref{table:lagged}, we present a toy example to illustrate how the delayed margin calls lead to residual exposures. In this example, we assume that the default time grid is equally-spaced with time steps that match the MPoR (which is 1M). Further, we assume zero threshold and MTA. At the initial time, the delivery amount is $2.00$, which is the difference between the initial value of the portfolio and the default value at 1M. If this amount were settled immediately, then the collateral value would have been $10$ and hence the residual exposure would habe been zero at 1M. The delay of the delivery amount by MPoR implies a collateral value of $8.00$ until 1M and hence a residual exposure of $2$. 
%
\begin{table}[!ht]
    \centering
    \begin{tabular}{|p{1cm}|p{1.4cm}|p{1.5cm}|p{1.5cm}|p{1.6cm}|p{1.cm}|}
    \hline
        Time Grid & Default Value & Delivery Amount & Delivery Amount Delayed  & Collateral Value   & NPV  \\ \hline
         0 & 8.00 & 2.00 &   True &~ &  ~   \\ \hline
        1M & 10.00 & 5.00& True & 8.00&  10.00   \\ \hline
        2M & 15.00 & -3.00 & True & 10.00 & 15.00 \\ \hline
        3M & 12.00 & -3.00 & True & 15.00 & 12.00  \\ \hline
        4M & 9.00 & 5.00 & True & 12.00 & 9.00   \\ \hline
        5M & 14.00 & 6.00 & True & 9.00 & 14.00  \\ \hline
        6M & 20.00 &  ~  & ~ & 14.00 & 20.00  \\ \hline
    \end{tabular}
    \caption{Toy example for delayed margin calls.}\label{table:lagged}
\end{table}
%

Some remarks and observations:
\begin{itemize}
 \item {\em Lagged Approach}  has the disadvantage that we need to use equally-spaced time grids with time steps that match the MPoR. In the above example, let us assume that the MPoR is 2W. Then, delaying the first delivery amount by 2W would still imply a collateral value of $10.00$ at 1M and hence a zero residual exposure.
  \item In {\em Lagged Approach} approach, we support three calculation (settlement) types where the delay of the {\em Delivery Amount } depends on its sign. The above example corresponds to a ``symmetric'' calculation type where both positive and negative delivery amounts are settled with delay, see section \ref{sec:analytics} for other calculation types.
   \item In ORE, the {\em Close-out Approach} is the preferred method -and the {\em Lagged Approach} is the legacy method- to incorporate MPoR in the collateral model. 
\end{itemize}
\subsection{Exposure Allocation}\label{sec:app_allocation}

XVAs and exposures are typically computed at netting set level. For accounting purposes it is typically required to {\em
  allocate} XVAs from netting set to individual trade level such that the allocated XVAs add up to the netting set
XVA. This distribution is not trivial, since due to netting and imperfect correlation single trade (stand-alone) XVAs
hardly ever add up to the netting set XVA: XVA is sub-additive similar to VaR. ORE provides an allocation method
(labeled {\em marginal allocation } in the following) which slightly generalises the one proposed in
\cite{PykhtinRosen}. Allocation is done pathwise which first leads to allocated expected exposures and then to allocated
CVA/DVA by inserting these exposures into equations (\ref{CVA},\ref{DVA}). The allocation algorithm in ORE is as
follows:
\begin{itemize}
\item Consider the netting set's discounted $\NPV$ after taking collateral into account, on a given path at time $t$:
$$
E(t)=D(0,t)\,(\NPV(t)-C(t))
$$ 
\item On each path, compute contributions $A_i$ of the latter to trade $i$ as
$$
A_{i} (t) = \left\{ \begin{array}{ll} 
E(t) \times \NPV_{i}(t) / \NPV(t), & |\NPV(t)| > \epsilon \\
E(t) / n, & |\NPV(t)| \le \epsilon
\end{array}
\right. 
$$
with number of trades $n$ in the netting set and trade $i$'s value $\NPV_i(t)$.
\item The $\EPE$ fraction allocated to trade $i$ at time $t$ by averaging over paths:
$$
\EPE_i(t) = \E\left[ A_i^+(t) \right]
$$
\end{itemize}
By construction, $\sum_i A_i(t) = E(t)$ and hence $\sum_i \EPE_i(t) = \EPE(t)$.\\

We introduced the {\em cutoff } parameter $\epsilon>0$ above in order to handle the case where the netting set value
$\NPV(t)$ (almost) vanishes due to netting, while the netting set 'exposure' $E(t)$ does not. This is possible in a
model with nonzero MTA and MPoR. Since a single scenario with vanishing $\NPV(t)$ suffices to invalidate the expected
exposure at this time $t$, the cutoff is essential. Despite introducing this cutoff, it is obvious that the marginal
allocation method can lead to spikes in the allocated exposures. And generally, the marginal allocation leads to both
positive and negative $\EPE$ allocations.

\medskip As a an example for a simple alternative to the marginal allocation of $\EPE$ we provide allocation based on
today's single-trade CVAs
$$
w_i = \CVA_i / \sum_i \CVA_i.
$$
This yields allocated exposures proportional to the netting set exposure, avoids spikes and negative $\EPE$, but does
not distinguish the 'direction' of each trade's contribution to $\EPE$ and $\CVA$.

\subsection{Sensitivity Analysis}\label{sec:app_sensi}

ORE's sensitivity analysis framework uses ``bump and revalue'' to compute Interest Rate, FX, Inflation, Equity and Credit sensitivities to
\begin{itemize}
\item Discount curves  (in the zero rate domain)
\item Index curves (in the zero rate domain)
\item Yield curves including e.g. equity forecast yield curves (in the zero rate domain)
\item FX Spots
\item FX volatilities
\item Swaption volatilities, ATM matrix or cube 
\item Cap/Floor volatility matrices (in the caplet/floorlet domain)
\item Default probability curves (in the ``zero rate'' domain, expressing survival probabilities $S(t)$ in term of zero rates $z(t)$ via $S(t)=\exp(-z(t)\times t)$ with Actual/365 day counter)
\item Equity spot prices
\item Equity volatilities, ATM or including strike dimension 
\item Zero inflation curves
\item Year-on-Year inflation curves
\item CDS volatilities
\item Base correlation curves
\end{itemize}

Apart from first order sensitivities (deltas), ORE computes second order sensitivities (gammas and cross gammas) as well. Deltas are computed using up-shifts and base values as
$$
\delta = \frac{f(x+\Delta)-f(x)}{\Delta},
$$ 
where the shift $\Delta$ can be absolute or expressed as a relative move $\Delta_r$ from the current level, $\Delta=x\,\Delta_r$. Gammas are computed using up- and down-shifts
$$
\gamma = \frac{f(x+\Delta)+f(x-\Delta) - 2\,f(x)}{\Delta^2},
$$ 
cross gammas using up-shifts and base values as
$$
\gamma_{cross} = \frac{f(x+\Delta_x,y+\Delta_y)-f(x+\Delta_x,y) -f(x,y+\Delta_y) + f(x,y)}{\Delta_x\,\Delta_y}.
$$ 

From the above it is clear that this involves the application of 1-d shifts (e.g. to discount zero curves) and 2-d shifts (e.g. to Swaption volatility matrices). The structure of the shift curves/matrices does not have to match the structure of the underlying data to be shifted, in particular the shift ``curves/matrices'' can be less granular than the market to be shifted. 
Figure \ref{fig_shiftcurve} illustrates for the one-dimensional case how shifts are applied.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.6]{shiftcurve.pdf}
\end{center}
\caption{1-d shift curve (bottom) applied to a more granular underlying curve (top). }
\label{fig_shiftcurve}
\end{figure} 

Shifts at the left and right end of the shift curve are extrapolated flat, i.e. applied to all data of the original curve to the left and to the right of the shift curve ends. In between, all shifts are distributed linearly as indicated to the left and right up to the adjacent shift grid points. As a result, a parallel shift of the all points on the shift curve yields a parallel shift of all points on the underlying curve.   \\

The two-dimensional case is covered in an analogous way, applying flat extrapolation at the boundaries and ``pyramidal-shaped'' linear interpolation for the bulk of the points. 

The details of the computation of sensitivities to implied volatilities in strike direction can be summarised as
follows, see also table \ref{sensi_config_overview} for an overview of the admissible configurations and the results
that are obtained using them.

\medskip
For {\em Swaption Volatilities}, the initial market setup can be an ATM surface only or a full cube. The simulation
market can be set up to simulate ATM only or to simulate the full cube, but the latter choice is only possible if a full cube is set
up in the initial market. The sensitivity set up must match the simulation setup with regards to the strikes (i.e. it
is ATM only if and only if the simulation setup is ATM only, or it must contain exactly the same strike spreads relative
to ATM as the simulation setup). Finally, if the initial market setup is a full cube, and the simulation / sensitivity
setup is to simulate ATM only, then sensitivities are computed by shifting the ATM volatility w.r.t. the given shift size and type and
shifting the non-ATM volatilities by the same absolute amount as the ATM volatility.

\medskip
For {\em Cap/Floor Volatilities}, the initial market setup always contains a set of fixed strikes, i.e. there is no
distinction between ATM only and a full surface. The same holds for the simulation market setup. The sensitivity setup
may contain a different strike grid in this case than the simulation market. Sensitivity are computed per expiry and
per strike in every case.

\medskip
For {\em Equity Volatilities}, the initial market setup can be an ATM curve or a full surface. The simulation market can
be set up to simulate ATM only or to simulate the full surface, where a full surface is allowed even if the initial market setup in an
ATM curve only. If we have a full surface in the initial market and simulate the ATM curve only in the simulation market, sensitivities
are computed as in the case of Swaption Volatilities, i.e. the ATM volatility is shifted w.r.t. the specified shift size
and type and the non-ATM volatilities are shifted by the same absolute amount as the ATM volatility. If the simulation
market is set up to simulate the full surface, then all volatilities are shifted individually using the specified shift size and type. In
every case the sensitivities are aggregated on the ATM bucket in the sensitivity report.

\medskip
For {\em FX Volatilities}, the treatment is similar to Equity Volatilities, except for the case of a full surface
definition in the initial market and an ATM only curve in the simulation market. In this case, the pricing in the
simulation market is using the ATM curve only, i.e. the initial market's smile structure is lost.

\medskip
For {\em CDS Volatilities} only an ATM curve can be defined.

\medskip
In all cases the smile dynamics is ``sticky strike'', i.e. the implied vol used for pricing a deal does not change if
the underlying spot price changes.

\begin{table}[hbt]
  \scriptsize
  \begin{center}
    \begin{tabular}{l | l | l | l | l | l}
      \hline
      Type & Init Mkt. Config. & Sim. Mkt Config. & Sensitivity Config. & Pricing & Sensitivities w.r.t. \\
      \hline
      Swaption & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      Swaption & Cube & Simulate Cube & Shift Smile Strikes & Full Cube & Smile Strike Shifts\footnote{smile
                                                                          strike spreads must match simulation market configuration} \\
      Swaption & Cube & Simulate ATM only & Shift ATM only & Full Cube & ATM Shifts\footnote{smile is shifted in parallel\label{sensismileparallel}} \\
      \hline
      Cap/Floor & Surface & Simulate Surface & Shift Smile Strikes & Full Surface & Smile Strike Shifts \\
      \hline
      Equity & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      Equity & ATM & Simulate Surface & Shift ATM only & ATM Curve & Smile Strike Shifts\footnote{result sensitivities
                                                                     are aggregated on ATM\label{sensiaggatm}} \\
      Equity & Surface & Simulate ATM only & Shift ATM only & Full Surface & ATM Shifts\textsuperscript{\ref{sensismileparallel}} \\
      Equity & Surface & Simulate Surface & Shift ATM only & Full Surface & Smile Strike Shifts\textsuperscript{\ref{sensiaggatm}} \\
      \hline
      FX & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      FX & ATM & Simulate Surface & Shift ATM only & ATM Curve & Smile Strike Shifts\textsuperscript{\ref{sensiaggatm}} \\
      FX & Surface & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      FX & Surface & Simulate Surface & Shift ATM only & Full Surface & Smile Strike Shifts\textsuperscript{\ref{sensiaggatm}} \\
      \hline
      CDS & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
    \end{tabular}
    \caption{Admissible configurations for Sensitivity computation in ORE}
    \label{sensi_config_overview}
  \end{center}
  \end{table}

\subsection{Par Sensitivity Analysis}
\label{app:par_sensi}

The ``raw'' sensitivities in ORE are generated in a computationally convenient domain (such as zero rates, caplet/floorlet volatilities, integrated hazard rates, inflation zero rates). These raw sensitivities are typically further processed in risk analytics such as VaR measures. On the other hand, for hedging purposes one is rather interested in sensitivities with respect to fair rates of hedge instruments such as Forward Rate Agreements, Swaps, flat Caps/Floors, CDS, Zero Coupon Inflation Swaps. \\

It is possible to generate par sensitivities from raw sensitivities using the chain rule as follows, and this is the approach taken in ORE. Recall for example the fair swap rate $c$ for some maturity as a function of zero rates $z_i$ in a single curve setting:
$$
c = \frac{1 - e^{-z_n\,t_n}}{\sum_{i=1}^n \delta_i\,e^{-z_i\, t_i}}
$$
More realistically, a given fair swap rate might be a function of the zero rates spanning the discount and index curves in the chosen currency. In a multi currency curve setting, that swap rate might even be a function of the zero rates spanning a foreign (collateral) currency discount curve, foreign and domestic currency index curves. Generally, we can write any fair par rate $c_i$ as function of raw rates $z_j$,
$$
c_i \equiv c_i(z_1, z_2, ..., z_n)
$$
This function may not be available in closed form, but numerically we can evaluate the sensitivity of $c_i$ with respect to changes in all raw rates,
$$
\frac{\partial c_i}{\partial z_j}.
$$
These sensitivities form a {\em Jacobi} matrix of derivatives. Now let $V$ denote some trade's price. Its sensitivity with respect a raw rate change $\partial V/\partial z_k$ can then be expressed in terms of sensitivities w.r.t. par rates using the chain rule
$$
\frac{\partial V}{\partial z_j} = \sum_{i=1}^n \frac{\partial V}{\partial c_i}\,\frac{\partial c_i}{\partial z_j},
$$
or in vector/matrix form
$$
\nabla_z V = C \cdot \nabla_c V, \qquad C_{ji} = \frac{\partial c_i}{\partial z_j}.
$$
Given the raw sensitivity vector $\nabla_z V$, we need to invert the Jacobi matrix $C$ to obtain the par rate sensitivity vector
$$
\nabla_c V = C^{-1} \cdot \nabla_z V.
$$

We then compute the Jacobi matrix $C$ by
\begin{itemize}
\item setting up par instruments with links to all required term structures expressed in terms of raw rates
\item ``bumping'' all relevant raw rates and numerically computing the par instrument's fair rate shift for each bump
\item thus filling the Jacobi matrix with finite difference approximations of the partial derivatives $\partial c_i/\partial z_j$.
\end{itemize}

The par rate conversion supports the following par instruments:
\begin{itemize}
\item Deposits
\item Forward rate Agreements
\item Interest Rate Swaps (fixed vs. ibor)
\item Overnight Index Swaps
\item Tenor Basis Swaps (ibor vs. ibor)
\item Overnight Index Basis Swaps (ibor vs. OIS)
\item FX Forwards
\item Cross Currency Basis Swaps
\item Credit Default Swaps
\item Caps/Floors
\end{itemize}


\subsection{Economic P\&L}\label{economic_pnl}

The economic P\&L of a portfolio denotes the change in its economic value over  
a time period $t_1$ to $t_2$. The economic value evolution during the period is due to three components
 
\begin{itemize}
\item the change in present value from period start to end
\item incoming and outgoing cash flows
\item accumulated cost of funding required to set up the portfolio initially 
\end{itemize}

In the following, we consider a portfolio consisting of assets in various currencies. We decompose the portfolio into parts each denominated in a different currency and value each sub-portfolio in its currency. We denote the sub-portfolio values at time $t$ in the respective currency $P_1(t), P_2(t), \dots$. Instruments with cash flows in more than one currency are decomposed into single-currency instruments and assigned into the related sub-portfolio.
The total portfolio value expressed in base currency (e.g. EUR) is 
\begin{equation} 
	P(t) = \sum_c P_c(t)\:X_c(t) \label{initial_value_base}
\end{equation}
where $X_c$ is the exchange rate that converts an amount in currency $c$ into an amount in base currency by multiplication. All prices $P_c(t)$ denote {\em dirty} market values (or theoretical values where market values are not available) at time $t$. 

In the following we consider three points in time,
\begin{itemize}
\item $t_0$: the time just before the first actual cash flow has appeared in the portfolio under consideration, possibly years ago
\item $t_1$: the beginning of the period for which we want to determine P\&L
\item $t_2$: the end of the period for which we want to determine P\&L
\end{itemize} 

\subsubsection*{Original P\&L} \label{trade_start}

The original P\&L is the portfolio's P\&L from portfolio inception $t_0$.
In this case the portfolio value at $t_0$ is 
$$P(t_0) = 0,$$
and the P\&L up to time $t_2$ is given by the portfolio value at $t_2$ plus the balance of currency accounts that collect incoming and outgoing cash flows and are compounded up to time $t_2$:
\begin{equation}
\pi(0, t_2) = P(t_2) + \sum_c X_c(t_2)\:B_c(t_2)
\label{pnl_3}
\end{equation}
where 
\begin{equation}
B_c(t_2) = \sum_{j=0}^{I(t_2)-1} F_c(\tau_j)\:C_c(\tau_j, t_2), \quad C_c(\tau_j, t_2)=\prod_{k=I(\tau_j)}^{I(t_2)-1} (1+r_c(\tau_k)\delta_k),
\end{equation}
sums and products are taken over daily time steps $\tau_j$ and

\medskip
\begin{tabular}{lp{10cm}}
$I(t)$ & is the day's index associated with time $t$ \\
$F_c(\tau_{j})$ & is the net cash flow in currency $c$ on date/time $\tau_j$, possibly zero \\
$r_c(\tau_j)$ & is the Bank's overnight funding and investment rate in currency $c$ for interest period $[\tau_j, \,\tau_{j+1}]$ (overnight)\\
$\delta_j$ & is the related day count fraction for period $[\tau_j, \,\tau_{j+1}]$ 
\end{tabular}

\bigskip

The balances $B_c$ can also be constructed iteratively
\begin{eqnarray*}
B_c(\tau_{j+1}) &=& B_c(\tau_{j}) (1+r_c(\tau_j)\delta_j) + F_c(\tau_{j+1}) 
\label{recursion}\\
j &=& 0, 1, 2, \dots \nonumber\\
B_c(\tau_0) &=& 0. \nonumber
\end{eqnarray*}

The P\&L for a period of interest $[t_1;\,t_2]$ is then computed by taking the difference
\begin{eqnarray}
	\pi(t_1, t_2) &=& \pi(0,t_2) - \pi(0,t_1) \label{pnl_1} \\
		&=& P(t_2) - P(t_1) + \sum_c (X_c(t_2)\: B_c(t_2) - X_c(t_1)\: B_c(t_1))
		\nonumber
\end{eqnarray}

One can show that
\begin{equation} 
	B_c(t_2) = B_c(t_1) \:C_c(t_1, t_2) 
	+ \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:C_c(\tau_j, t_2) 
\label{bc}
\end{equation}
which separates the contribution to $B_c(t_2)$ from cash flows in period $[t_1;t_2]$ (right-most sum) and contributions from realized P\&L and cost of funding of previous periods accumulated in $B_c(t_1)$. We can now insert (\ref{bc}) into (\ref{pnl_1}) to eliminate $B_c(t_2)$ and obtain 
\begin{eqnarray}
	\pi(t_1, t_2) &=& P(t_2) - P(t_1)
	+ \sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:C_c(\tau_j, t_2)
	\nonumber\\
	&& + \sum_c B_c(t_1)\:\left\{X_c(t_2)\:C_c(t_1, t_2) - X_c(t_1)\right\} \label{pnl_1a}
\end{eqnarray}
 
\subsubsection*{Cost of Carry}

Separating actual cash flows and prices from compounding effects yields
$$
	\pi(t_1, t_2) = P(t_2) - P(t_1)
	+ \sum_c X_c(t_2)\sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)
	+ CC(t_1,t_2)
$$

where the cost of carry term is 
\begin{eqnarray}
CC(t_1,t_2) &=& 
	\sum_c X_c(t_2)\sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:
	\left(C_c(\tau_j, t_2) - 1\right) \nonumber\\
&& + \sum_c \:B_c(t_1)\:\left\{X_c(t_2)\:C_c(t_1, t_2) - X_c(t_1)\right\} 
\label{CC}
\end{eqnarray}

\subsubsection*{Period P\&L after Sell-Down}

At time $t_1$, we can write the original P\&L (equation \ref{pnl_3}) in respective currencies
$$
\pi_c(t_1) = P_c(t_1) + B_c(t_1), \qquad \pi(t_1) = \sum_c X_c(t_1)\:\pi_c(t_1).
$$
Inserting this into (\ref{CC}),
\begin{eqnarray*}
CC(t_1,t_2) &=& 
	\sum_c X_c(t_2)\sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:
	\left(C_c(\tau_j, t_2) - 1\right) \\
&& + \sum_c \:(\pi_c(t_1) - P_c(t_1))\:\left\{X_c(t_2)\:C_c(t_1, t_2) - X_c(t_1)\right\}, 
\end{eqnarray*}
shows that there is a contribution to $\pi(t_1,t_2)$, via the cost of carry, due to compounding and FX effects on previous periods' P\&L result.

\bigskip
We now take the view that the portfolio is liquidated at time $t_1$, so that the account balance equals the P\&L at $t_1$. We further assume that this balance is then removed ("sell down" of P\&L) and transfered into a separate portfolio, the Bank's equity\footnote{Equity is in turn managed and most likely invested into financial instruments other than a Bank account}. The same portfolio is thereafter set up again so that the currency account balance turns into a liability $B_c(t_1) = - P_c(t_1)$, and the total starting balance is $B(t_1)=-P(t_1)$. In contrast to the previous section, this changes the balance at time $t_1$ suddenly and without relation to an actual cash flow.

This raises the question how the artificial initial balance is funded subsequently, in currency for each sub-portfolio or in base currency only. This
choice may vary by portfolio, depend on the actual currencies in which the Bank can source funding, depend on the location/economy in which the portfolio is run, which currency is a reasonable benchmark, etc. 

\subsubsection*{Funding in Currency}\label{funding_ccy}

In this section we take the view that each sub-portfolio is funded in currency
so that we start with opening balances $B_c(t_1) = -P_c(t_1)$.

Inserting the artificial opening balances at $t_1$ into (\ref{pnl_1a}) yields
\begin{eqnarray}
	\pi_2(t_1, t_2) &=& P(t_2) - P(t_1)
	+ \sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:C_c(\tau_j, t_2)
	\nonumber\\
	&& - \sum_c P_c(t_1)\:\left\{X_c(t_2)\:C_c(t_1, t_2) - X_c(t_1)\right\} \nonumber \\
&=&	P(t_2)
	+ \sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:C_c(\tau_j, t_2)
	\nonumber\\
	&& - \sum_c P_c(t_1)\:X_c(t_2)\:C_c(t_1, t_2)
	\label{pnl_2}
\end{eqnarray}

Note that only exchange rates at $t_2$ enter into the expression.

\subsubsection*{Cost of Carry}

Separating actual cash flows and prices from compounding effects yields

$$
	\pi_2(t_1, t_2) = P(t_2) + \sum_c X_c(t_2)\:\left\{-P_c(t_1)
	+ \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\right\}
	+ CC_2(t_1,t_2)
$$

where the cost of carry term is 
\begin{eqnarray*}
CC_2(t_1,t_2) &=& \sum_c \:X_c(t_2)\:
\sum_{j=I(t_1)}^{I(t_2)-1} F_c(\tau_j) \: \left(C_c(\tau_j, t_2) - 1\right)\\
&& - \sum_c \:X_c(t_2)\:P_c(t_1)\:\left(C_c(t_1, t_2) - 1\right)
\end{eqnarray*}

\subsubsection*{Funding in Base Currency}\label{funding_base_ccy}

In this section we assume that the setup cost for the portfolio is converted into base currency at $t_1$ and funded subsequently in base currency.
This means we insert artificial initial balances $B_c(t_1)=0$ except for the base currency account $B(t_1) = -P(t_1)$.
Inserting this opening balance at $t_1$ into (\ref{pnl_1a}) now yields
\begin{eqnarray}
	\pi_3(t_1, t_2) &=& P(t_2) 
	+ \sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:C_c(\tau_j, t_2)
	\nonumber\\
	&& - P(t_1)\:C(t_1, t_2)
	\label{pnl_4}
\end{eqnarray}

where $C(t_1,t_2)$ is the compounding factor in base currency.

\subsubsection*{Cost of Carry}

Separating actual cash flows and prices from compounding effects yields now

$$
	\pi_3(t_1, t_2) = P(t_2) - P(t_1) 
	+ \sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)
	+ CC_3(t_1, t_2)
$$
where
\begin{eqnarray*}
CC_3(t_1, t_2) &=& 
\sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:(C_c(\tau_j, t_2)-1)\\
&&  - P(t_1)\:(C(t_1, t_2) - 1)
\end{eqnarray*}

\subsubsection*{FX Effect}\label{fx_effect}

The difference between (\ref{pnl_2}) and (\ref{pnl_3}) is 

$$ \pi_2(t_1, t_2) - \pi_3(t_1, t_2) = P(t_1)\:C(t_1, t_2) 
	- \sum_c P_c(t_1)\:X_c(t_2)\:C_c(t_1, t_2).
$$

The expected value of this difference at period start $t_1$ is zero, but the retrospectively realized difference at period end $t_2$ is nonzero in general.
 

\subsection{Risk Hypothetical P\&L}\label{sec:pl}

In the following we briefly describe approaches to generating P\&L
vectors that feed into several subsequent sections for the purpose of
computing risk measures such as Value at Risk or for backtesting a
market risk model.

These P\&L's are different from the economic P\&L introduced in
section \ref{economic_pnl} above, but rather {\em risk-hypothetical} 
due to the application  of some historical market moves to the
current market which gives rise to a valuation change.
  
%\subsection{Basics}\label{sec:pl_basics}

Consider a history of market risk factors $X_i(j)$ where $i\in\{1,\ldots,n\}$ identifies the risk factor and
$j\in\{1,\ldots,m\}$ corresponds to a time $t_j$ on which the risk factor was observed. The times are assumed to be
equally spaced, with $t_{j+1}-t_j$ corresponding to $1$ business day w.r.t. a given calendar. To generate an overlapping
$k$-day PL, define the $k$ day return at time $t_j$ to be

\begin{equation}\label{returns}
r_i(j) = R_{T_i}(X_i(j), X_{i}(j+k))
\end{equation}

for $j=1,\ldots,m-k$. Note that the case of non-overlapping $k$ day returns fits in with straightforward modifications
of the scheme described here. $R$ defines the return value for two observations of the same factor, which is one of the
following

\begin{eqnarray}
  R_A(x,y) &=& y-x \\
  R_R(x,y) &=& y/x - 1 \\
  R_L(x,y) &=& \log(y/x)
\end{eqnarray}

where the subscript stands for absolute (A), relative (R) and lognormal (L) returns, respectively. Note that the
relative and lognormal returns are not defined for $x=0$, and we consider a data point with $X_i(j)=0$ and for which we
compute relative or lognormal returns to be an error in the data that needs to be corrected or excluded from the
analysis. Also note that $R_R \approx R_L$ for small values of $y/x-1$, the difference $R_R-R_L$ approaching zero when
$y/x$ approaches $1$.

Now assume $t_m$ to be the reference date (e.g. for the value at risk calculation) and

\begin{equation}
  X(m) = \{ X_i(m) \}_{i=1,\ldots,n}
\end{equation}

the market factor values on the reference date.

\subsubsection*{Full Revaluation P\&L}\label{fullpl}

For a given portfolio denote its NPV at $t_m$ by $\nu(X(m))$. Then we can compute a {\em full revaluation PL} vector

\begin{equation}\label{fullrevalpl}
\pi_F = \{ \pi_F(j) \}_{j=1,\ldots,m-k}
\end{equation}

as

\begin{equation}
  \pi_F(j) = \nu( X'(m,j) ) - \nu ( X(m) )
\end{equation}

by pricing the portfolio under each perturbed market factor vector

\begin{equation}
  X'(m,j) = \{ X'_i(m,j) \}_{i=1,\ldots,n}
\end{equation}

which is defined by

\begin{equation}\label{histReturns}
  X'_i(m,j) = a_{T_i}( X_i(m),  R_{T_i}(X_i(j),X_i(j+k)) )
\end{equation}

with the return application function

\begin{eqnarray}
  a_A(x,r) &=& x+r \\
  a_R(x,r) &=& x(1+r) \\
  a_L(x,r) &=& x e^r
\end{eqnarray}

and return types $T_i \in \{ A, R, L \}$, dependent on the particular factor $X_i$. Table \ref{sensiReturnTypes} shows a
possible choice of return types for the different risk factors (in ORE notation). Note, that the factors Discount
Curve, Index Curve and Survival Probability are discount factors resp. survival probabilities that are converted to zero
rate resp. hazard rate shifts by taking the log. Also note, that the factors Recovery Rate and Basis Correlation are
bounded (a recovery rate must be in $[0,1]$ while the base correlation must be in $[-1,1]$), so that after a shift is
applied, the result has to be capped / floored appropriately to ensure valid scenario values.

\subsubsection*{Sensitivity based P\&L}\label{sensipl}

As an alternative to the full revaluation PL in \eqref{fullrevalpl} we can approximate this PL using a Taylor expansion
of $\nu(X'(m,j))$ viewed as a function of the returns $R_{T_i}$\footnote{i.e. we view $\nu$ as a function of the second
argument of $a_{T_i}$ in \ref{histReturns}} around the expansion point $(0,0,\ldots,0)$ generating a {\em sensitivity
  based PL},

\begin{equation}
\pi_S = \{ \pi_S(j) \}_{j=1,\ldots,m-k}
\end{equation}

with

\begin{equation}
\begin{aligned}\label{taylorPl}
  \pi_S(j) = & \sum_{i=1}^n D^i_{T_i}\nu(X(m)) R_{T_i}(X_i(m), X'_i(m,j)) + \\
           \frac{1}{2}& \sum_{i,l=1}^n D^{i,l}_{T_i,T_l}\nu(X(m)) R_{T_i}(X_i(m), X'_i(m,j)) R_{T_l}(X_l(m), X'_l(m,j)),
\end{aligned}
\end{equation}

where we use sensitivities up to second order. Here $D^i_{T_i}$ denotes a first or second order derivative operator, depending
on the market factor specific shift type $T_i \in \{ A,R,L \}$, i.e.

\begin{eqnarray}\label{derivs}
  D^i_A f(x) &=& \frac{\partial f(x)}{\partial x_i}, \\
  D^i_R f(x) = D^i_L f(x) &=& x_i\frac{\partial f(x)}{\partial x_i}
\end{eqnarray}

and using the short hand notation

\begin{equation}\label{derivs_short}
  D^{i,l}_{T_i,T_l} f(x) = D^i_{T_i} D^l_{T_l} f(x).
\end{equation}

These first and second order sensitivities may be computed analytically, or (more common) as finite difference
approximations (``bump and revalue'' approximations), see section \ref{sec:app_sensi}. To clarify the relationship of \eqref{derivs}
and a finite difference scheme for derivatives computation in a bit more detail we note that for a absolute shift $h>0$

\begin{equation}
\frac{f(x+h)-f(x)}{h} \rightarrow f'(x)
\end{equation}

for $h\rightarrow 0$ by definition of $f'$ while for a relative shift

\begin{equation}
  \frac{f(x(1+h))-f(x)}{h} = x \frac{f(x(1+h))-f(x)}{xh} \rightarrow xf'(x)
\end{equation}

for $h\rightarrow 0$ and for a log shift

\begin{equation}\label{logshift}
\frac{f(xe^h)-f(x)}{h} \rightarrow xf'(x)
\end{equation}

using e.g. L'Hospital's rule, so that

\begin{itemize}
\item both a relative and a log shift bump and revalue sensitivity approximate the same value $xf'(x)$ in the limit for
  $h\rightarrow 0$,
\item an absolute shift sensitivity can be transformed into a relative / log shift sensitivity (in the limit for
  $h\rightarrow 0$) by multiplying with the risk factor value $x$, and vice versa.
\end{itemize}

We also note that the usual way of bumping continuously compounded zero rates to compute a Discount Curve or Index Curve
sensitivity by $h^*$ is equivalent to \eqref{logshift} with $h=h^*t$, where $t$ is the maturity of the respective
rate. Therefore in practice a log return of discount factors can not directly be combined with a sensitivity expressed
in zero rate shifts, but has to be scaled by $1/t$ before doing so.

Since the number of second order deriviatives can be quite big in realistic setups with hundreds or even thousands of
market factors, in practice only part of the second order deriviatives might be fed into \eqref{taylorPl} assuming the
rest to be zero.

Note that the types $T_i$ used to generate the historical returns \eqref{histReturns} can be different from those used
in the Taylor expansion \eqref{taylorPl}. It is important though that the same types $T_i$ are used for the
derivatives operators $D^i_{T_i}$ and the returns $R_{T_i}$ in \eqref{taylorPl}. 

A number of configurations are hard-coded into ORE depending on whether raw sensitivities, backtesting sensitivities or CRIF sensitivities are being called. These configurations are displayed in \ref{sensiReturnTypes} - note that there is currently no distinction made in ORE between raw sensitivities and backtest sensitivities.

\begin{table}[ht]
  \begin{tabular}{|l|c|c|c|c|}
    \hline
    \multicolumn{1}{|c|}{\multirow{2}{*}{ORE Risk Factor}} & \multicolumn{2}{c|}{Backtest Sensitivities} & \multicolumn{2}{c|}{CRIF Sensitivities} \\ \cline{2-5} 
    \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Return Type} & \multicolumn{1}{c|}{Shift Size} & \multicolumn{1}{c|}{Return Type} & \multicolumn{1}{c|}{Shift Size} \\ \hline
    Discount Curve & A & 0.01\% & A & 0.01\% \\
    Index Curve & A & 0.01\% & A & 0.01\% \\
    Yield Curve & A & 0.01\% & A & 0.01\% \\
    Dividend Yield & A & 0.01\% & A & 0.01\% \\
    Equity Forecast Curve & A & 0.01\% & A & 0.01\% \\
    Swaption Volatility* & R & 1\% & A & 0.01\% \\
    Optionlet Volatility* & R & 1\% & A & 0.01\% \\
    FX Spot** & R & 1\% & R & 0.1\% \\
    FX Volatility & R & 1\% & A & 1\% \\
    Equity Spot & R & 1\% & R & 1\% \\
    Equity Volatility & R & 1\% & A & 1\% \\
    Yield Volatility & R & 1\% & R & 1\% \\
    Survival Probability & A & 0.01\% & A & 0.01\% \\
    CDS Volatility & R & 1\% & R & 1\% \\
    Correlation & R & 1\% & - & - \\
    Base Correlation & A & 1\% & A & 1\% \\
    Zero Inflation Curve & A & 0.01\% & A & 0.01\% \\
    YoY Inflation Curve & A & 0.01\% & A & 0.01\% \\
    Zero Inflation CF Vol & R & 1\% & R & 1\% \\
    YoY Inflation CF Vol & R & 1\% & R & 1\% \\
    Commodity Curve & R & 1\% & R & 1\% \\
    Commodity Volatility & R & 1\% & A & 1\% \\
    Security Spread & A & 0.01\% & - & - \\ \hline
  \end{tabular}
  \caption{Sensitivity return type configuration for raw, backtest and CRIF sensitivities}
  \label{sensiReturnTypes}
\end{table}

\newpage
*We predominantly use normal IR volatilities with an absolute shift of $0.01\%$. For lognormal IR volatilities an absolute shift of $1\%$ applies. Also, notice that ``Optionlet Volatility'' is the ORE name for ``Cap / Floor Volatility''.

**During a CRIF run, the FX spot delta is computed using the central difference approximation by default. A smaller shift size of $0.1\%$ is used because of this. Furthermore, for FX vanilla European and American options, there are analytic formulae available for FX deltas. These are implemented in ORE during a CRIF run.

Note - the values $A$ and $R$ here refer to the absolute and relative shifts defined in \eqref{derivs}. 

\subsection{Value at Risk}\label{sec:app_var}

\subsubsection*{Historical Simulation VaR}\label{histsimvar}

The historical simulation VaR is defined to be a $p$-quantile of the empirical distribution generated by the full
revaluation PL vector $\pi_F = \{ \pi_F(j) \}_{j}$. Here, with ``generated'' we mean that we weigh each $\pi_F(j)$ with
the same probability $1/J$, where $J$ denotes the number of elements in the vector.

\subsubsection*{Historical Simulation Taylor VaR}\label{histtaylorvar}

Similarly, the historical simulation Taylor VaR is defined to be the $p$-quantile of the empirical distribution
generated by the sensitivity based PL vector $\pi_S$ (call side), resp. $-\pi_S$ (post side).

\subsubsection*{Parametric VaR}\label{parametricvar}

For the computation of the parametric, or variance-covariance VaR, we rely on a second order sensitivity-based P\&L approximation

\begin{eqnarray}\label{taylorPl2}
  \pi_S & = & \sum_{i=1}^n D^i_{T_i}\,V\cdot Y_i 
        + \frac{1}{2} \sum_{i,j=1}^n D^{i,j}_{T_i,T_j}\,V\cdot Y_i\cdot Y_j
\end{eqnarray}

with 
\begin{itemize}
\item portfolio value $V$
\item random variables $Y_i$ representing risk factor returns; these are assumed to be multivariate normally distributed with zero mean
and covariance matrix matrix $C = \{ \rho_{i,k} \sigma_i \sigma_k \}_{i,k}$, where $\sigma_i$ denotes the standard
deviation of $Y_i$; covariance matrix $C$ may be estimated using the Pearson estimator on historical return data
$\{ r_i(j) \}_{i,j}$. Since the raw estimate might not be positive semidefinite, we apply a salvaging algorithm to
ensure this property, which basically replaces negative Eigenvalues by zero and renormalises the resulting matrix, see
\cite{corrSalv};
\item first or second order derivative operators $D$, depending
on the market factor specific shift type $T_i \in \{ A,R,L \}$ (absolute shifts, relative shifts, absolute log-shifts), i.e.
\begin{eqnarray*}\label{derivs2}
  D^i_A \,V(x) &=& \frac{\partial V(x)}{\partial x_i} \\
  D^i_R \,V(x) = D^i_L f(x) &=& x_i\frac{\partial V(x)}{\partial x_i}
\end{eqnarray*}
and using the short hand notation
\begin{equation*}
  D^{i,j}_{T_i,T_j} V(x) = D^i_{T_i} D^j_{T_j} V(x)
\end{equation*}
In ORE, these first and second order sensitivities are computed as finite difference
approximations (``bump and revalue'').
\end{itemize}

To approximate the $p$-quantile of $\pi_S$ in \eqref{taylorPl2} ORE offers the techniques outlined below.

\subsubsection*{Delta Gamma Normal Approximation}
 
The distribution of \eqref{taylorPl2} is non-normal due to the second order terms. 
The delta gamma normal approximation in ORE computes mean $m$ and variance $v$ of the portfolio value change $\pi_S$ (discarding moments higher than two) following \cite{alexander} and provides a simple VaR estimate 
$$
VaR = m + N^{-1}(q)\,\sqrt{v}
$$
for the desired quantile $q$ ($N$ is the cumulative standard normal distribution). Omitting the second order terms in \eqref{taylorPl2} yields the delta normal approximation.
 
\subsubsection*{Cornish-Fisher Expansion}

The first four moments of the distribution of $\pi_S$ in \eqref{taylorPl2} can be computed in closed form using the covariance matrix $C$ and the sensitivities of first and second order $D_i$
and $D_{i,k}$, see e.g. \cite{alexander}. Once these moments are known, an approximation to the true quantile of $\pi_S$ can be computed using the Cornish-Fisher expansion, see also [7], which in practice often gives a decent approximation of the true value, but may also show bigger differences in certain configurations.

\subsubsection*{Saddlepoint Approximation}

Another approximation of the true quantile of $\pi_S$ can be computed using the Saddlepoint approximation using results from \cite{Lugannani} and \cite{Daniels}. This method typically produces more accurate results than the Cornish-Fisher method, while still being fast to evaluate.

\subsubsection*{Monte Carlo Simulation}

By simulating a large number of realisations of the return vector $Y=\{ Y_i \}_i$ and computing the corresponding
realisations of $\pi_S$ in \eqref{taylorPl2} we can estimate the desired quantile as the quantile of the empirical
distribution generated by the Monte Carlo samples. Apart from the Monte Carlo Error no approximation is involved in this
method, so that albeit slow it is well suited to produce values against which any other approximate approaches can be tested. Numerically, the simulation is implemented using a Cholesky Decomposition
of the covariance matrix $C$ in conjunction with a pseudo random number generator (Mersenne Twister) and an
implementation of the inverse cumulative normal distribution to transform $U[0,1]$ variates to $N(0,1)$ variates.

\end{appendix}

%========================================================
%\section{References}
%========================================================

\begin{thebibliography}{*}

\bibitem{ORE} \url{http://www.opensourcerisk.org}

\bibitem{QL} \url{http://www.quantlib.org}
 
\bibitem{QRM} \url{http://www.quaternion.com}

\bibitem{acadia} \url{http://www.acadia.inc}

\bibitem{quantlib-install} \url{http://quantlib.org/install/vc10.shtml}

%\bibitem{confluence} https://confluence.atlassian.com/bitbucket/set-up-git-744723531.html

\bibitem{git-download} \url{https://git-scm.com/downloads}

\bibitem{boost-binaries} \url{https://sourceforge.net/projects/boost/files/boost-binaries}

\bibitem{boost} \url{http://www.boost.org}

\bibitem{jupyter} \url{http://jupyter.org}

\bibitem{Anaconda} \url{https://docs.continuum.io/anaconda}

\bibitem{LO} \url{http://www.libreoffice.org}

%\bibitem{xlwings} \url{http://www.xlwings.org}

\bibitem{bcbs128} Basel Committee on Banking Supervision, {\em International Convergence of Capital Measurement and
    Capital Standards, A Revised Framework}, \url{http://www.bis.org/publ/bcbs128.pdf}, June 2006

\bibitem{bcbs189} Basel Committee on Banking Supervision, {\em Basel III: A global regulatory framework for more
    resilient banks and banking systems}, \url{http://www.bis.org/publ/bcbs189.pdf}, June 2011

\bibitem{d325} Basel Committee on Banking Supervision, {\em Review of the Credit Valuation Adjustment Risk Framework}, \url{https://www.bis.org/bcbs/publ/d325.pdf}, 2015

\bibitem{d424} Basel Committee on Banking Supervision, {\em Basel III: Finalising post-crisis reforms}, \url{https://www.bis.org/bcbs/publ/d424.pdf}, 2017

\bibitem{BrigoMercurio} Damiano Brigo and Fabio Mercurio, {\em Interest Rate Models: Theory and Practice, 2nd Edition},
  Springer, 2006.

\bibitem{Pykhtin2010} Michael Pykhtin, {\em Collateralized Credit Exposure}, in Counterparty Credit Risk, (E. Canabarro,
  ed.), Risk Books, 2010

\bibitem{PykhtinRosen} Michael Pykhtin and Dan Rosen, {\em Pricing Counterparty Risk at the Trade Level and CVA
    Allocations}, Finance and Economics Discussion Series, Divisions of Research \& Statistics and Monetary Affairs,
  Federal Reserve Board, Washington, D.C., 2010

\bibitem{Gregory12} Jon Gregory, {\em Counterparty Credit Risk and Credit Value Adjustment, 2nd Ed.}, Wiley Finance,
  2013.

\bibitem{Gregory15} Jon Gregory, {\em The xVA Challenge, 3rd Ed.}, Wiley Finance, 2015.

\bibitem{Lichters} Roland Lichters, Roland Stamm, Donal Gallagher, {\em Modern Derivatives Pricing and Credit Exposure
    Analysis, Theory and Practice of CSA and XVA Pricing, Exposure Simulation and Backtesting}, Palgrave Macmillan,
  2015.

\bibitem{Anfuso2016} Fabrizio Anfuso, Daniel Aziz, Paul Giltinan, Klearchos Loukopoulos, {\em A Sound Modelling and
    Backtesting Framework for Forecasting Initial Margin Requirements},
  \url{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2716279}, 2016

\bibitem{Andersen2016} Leif B. G. Andersen, Michael Pykhtin, Alexander Sokol, {\em Rethinking Margin Period of Risk},
  http://papers.ssrn.com/sol3/papers.cfm?abstract\_id=2719964, 2016

\bibitem{SIMM2.5A} ISDA SIMM Methodology, version 2.5A, (based on v2.5a) \\
  \url{https://www.isda.org/a/FBLgE/ISDA-SIMM\_v2.5A.pdf}

  % \bibitem{SIMM}{SIMM Methodology\\ \tiny
  %   http://www2.isda.org/attachment/ODM1Mw==/ISDA\%20SIMM\%20Methodology\_7\%20April\%202016\_v3.15\%20(PUBLIC).pdf}

  % \bibitem{SIMM_Data_Standards}{SIMM Risk Data Standards\\ \tiny
  %   https://www2.isda.org/attachment/ODQzMg==/Risk\%20Data\%20Standards\_24\%20May\%202016\_v1.22\%20(PUBLIC).pdf}

  % \bibitem{OO} http://www.openoffice.org

\bibitem{Andersen_Piterbarg_2010} Andersen, L., and Piterbarg, V. (2010): Interest Rate Modeling, Volume I-III
  
\bibitem{LichtersEtAl} Peter Caspers, Paul Giltinan, Paul; Lichters, Roland; Nowaczyk , Nikolai. {\em Forecasting Initial Margin Requirements  A Model Evaluation}, Journal of Risk Management in Financial Institutions, Vol. 10 (2017), No. 4, \url{https://ssrn.com/abstract=2911167}

\bibitem{corrSalv} R. Rebonato and P. Jaeckel, The most general methodology to create a valid correlation matrix for
  risk management and option pricing purposes, The Journal of Risk, 2(2), Winter 1999/2000,
  \url{http://www.quarchome.org/correlationmatrix.pdf}

\bibitem{alexander} Carol Alexander, Market Risk Analysis, Volume IV, Value at Risk Models, Wiley 2009

\bibitem{Lugannani} Lugannani, R.and S.Rice (1980), Saddlepoint Approximations for the Distribution of the Sum of
  Independent Random Variables, Advances in Applied Probability, 12,475-490.

\bibitem{Daniels} Daniels, H. E. (1987), Tail Probability Approximations, International Statistical Review, 55, 37-48.

\bibitem{ScriptedTrade} ORE Scripted Trade Module, latest version at \url{https://github.com/OpenSourceRisk/Engine/tree/master/Docs/ScriptedTrade}
  
\end{thebibliography}

\newpage
\addcontentsline{toc}{section}{Todo}
\listoftodos[Todo]
%\todos

\end{document}
